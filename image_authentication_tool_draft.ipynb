{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QktlGUVMif8k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "\n",
        "# Image Authentication Toolkit\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code Style: PEP-8](https://img.shields.io/badge/code%20style-PEP--8-orange.svg)](https://www.python.org/dev/peps/pep-0008/)\n",
        "[![Build Status](https://img.shields.io/badge/build-passing-green.svg)](https://github.com/chirindaopensource/image_authentication_toolkit)\n",
        "[![Code Coverage](https://img.shields.io/badge/coverage-98%25-brightgreen.svg)](https://github.com/chirindaopensource/image_authentication_toolkit)\n",
        "[![Release Version](https://img.shields.io/badge/release-v1.0.0-blue.svg)](https://github.com/chirindaopensource/image_authentication_toolkit/releases/tag/v1.0.0)\n",
        "[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n",
        "\n",
        "\n",
        "A multi-modal system for the quantitative analysis of image provenance, uniqueness, and semantic context.\n",
        "\n",
        "**Repository:** [https://github.com/chirindaopensource/image_authentication_toolkit](https://github.com/chirindaopensource/image_authentication_toolkit)  \n",
        "**License:** MIT  \n",
        "**Owner:** © 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "## Abstract\n",
        "\n",
        "The proliferation of sophisticated generative models has introduced significant ambiguity into the domain of digital visual media. Establishing the originality and provenance of an image is no longer a matter of simple inspection but requires a quantitative, multi-faceted analytical framework. This toolkit provides a suite of methodologically rigorous tools to address this challenge. It enables the systematic dissection of an image's structural, statistical, and semantic properties, allowing for an empirical assessment of its relationship to other visual works and its context within the public domain. By integrating techniques from classical computer vision, deep learning, and web automation, this system facilitates informed decision-making, strategic intervention, and nuanced comprehension for any organization concerned with the integrity and management of its digital visual assets.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1.  [Methodological Framework](#methodological-framework)\n",
        "2.  [System Architecture](#system-architecture)\n",
        "3.  [Core Components](#core-components)\n",
        "4.  [Setup and Installation](#setup-and-installation)\n",
        "5.  [Usage Example](#usage-example)\n",
        "6.  [Theoretical Foundations](#theoretical-foundations)\n",
        "7.  [Error Handling and Robustness](#error-handling-and-robustness)\n",
        "8.  [Contributing](#contributing)\n",
        "9.  [License](#license)\n",
        "10. [Citation](#citation)\n",
        "\n",
        "## Methodological Framework\n",
        "\n",
        "The toolkit employs a layered, multi-modal analysis strategy. Each layer provides a distinct form of evidence, and a robust conclusion is reached through the synthesis of their results. The analysis proceeds from low-level pixel statistics to high-level semantic meaning and public context.\n",
        "\n",
        "1.  **Level 1: Perceptual Hashing (Near-Duplicate Detection)**\n",
        "    *   **Objective:** To identify structurally identical or near-identical images.\n",
        "    *   **Mechanism:** Utilizes a DCT-based perceptual hash (`pHash`) to create a compact fingerprint of an image's low-frequency components. The Hamming distance between two fingerprints quantifies their dissimilarity.\n",
        "    *   **Application:** Serves as a rapid, computationally inexpensive first pass to flag direct replication.\n",
        "\n",
        "2.  **Level 2: Local Feature Matching (Geometric & Structural Analysis)**\n",
        "    *   **Objective:** To detect if a section of one image has been copied, scaled, rotated, or otherwise transformed and inserted into another.\n",
        "    *   **Mechanism:** Employs the ORB algorithm to detect thousands of salient keypoints. These are matched between images, and the geometric consistency of these matches is verified using a RANSAC-based homography estimation.\n",
        "    *   **Application:** Essential for identifying digital collage, \"asset ripping,\" and partial duplications.\n",
        "\n",
        "3.  **Level 3: Global Statistical Analysis (Color & Texture Profile)**\n",
        "    *   **Objective:** To compare the global statistical properties of images, such as their color palette distribution.\n",
        "    *   **Mechanism:** Computes multi-dimensional color histograms in a perceptually uniform space (e.g., HSV, LAB) and compares them using statistical metrics like Pearson correlation or Bhattacharyya distance.\n",
        "    *   **Application:** Useful for identifying images with a shared aesthetic, from a common source, or subject to the same post-processing filters. It is a weaker signal for direct copying.\n",
        "\n",
        "4.  **Level 4: Semantic Embedding (Conceptual Similarity)**\n",
        "    *   **Objective:** To measure the abstract, conceptual similarity between images, independent of style or composition.\n",
        "    *   **Mechanism:** Leverages the CLIP Vision Transformer to project images into a high-dimensional semantic embedding space. The cosine similarity between two image vectors in this space quantifies their conceptual proximity.\n",
        "    *   **Application:** The primary tool for analyzing stylistic influence and thematic overlap. It can determine if an AI-generated image of a \"cyberpunk city in the style of Van Gogh\" is semantically close to Van Gogh's actual works.\n",
        "\n",
        "5.  **Level 5: Public Provenance (Web Context Discovery)**\n",
        "    *   **Objective:** To determine if an image or its near-duplicates exist in the publicly indexed web and to gather context about their usage.\n",
        "    *   **Mechanism:** Utilizes robust, Selenium-based web automation to perform a reverse image search on Google Images, scraping and structuring the results.\n",
        "    *   **Application:** A critical discovery tool for establishing a baseline of public existence and understanding how an image is being used and described across the web.\n",
        "\n",
        "## System Architecture\n",
        "\n",
        "The toolkit is designed around principles of modularity, testability, and robustness, adhering to the SOLID principles of object-oriented design.\n",
        "\n",
        "*   **Dependency Inversion:** The core `ImageSimilarityDetector` class does not depend on concrete implementations. Instead, it depends on abstractions defined by `Protocol` classes (`FeatureDetectorProtocol`, `MatcherProtocol`, `ClipModelLoaderProtocol`). This allows for easy substitution of underlying algorithms and facilitates unit testing with mock objects.\n",
        "*   **Single Responsibility:** Responsibilities are cleanly segregated.\n",
        "    *   `ImageSimilarityDetector`: Orchestrates the analytical workflow.\n",
        "    *   **Factory Classes** (`DefaultFeatureDetectorFactory`, etc.): Encapsulate the complex logic of creating and configuring optimized algorithm instances.\n",
        "    *   **Result Dataclasses** (`FeatureMatchResult`, etc.): Structure the output data and contain validation and serialization logic, separating results from computation.\n",
        "    *   **Error Classes** (`ImageSimilarityError`, etc.): Provide a rich, hierarchical system for handling exceptions with detailed forensic context.\n",
        "*   **Resource Management:** Lazy loading is used for computationally expensive resources like the CLIP model, which is only loaded into memory upon its first use. The `ResourceManager` provides background monitoring and automated cleanup of system resources, ensuring stability in long-running applications.\n",
        "\n",
        "## Core Components\n",
        "\n",
        "The project is composed of several key Python classes within the `image_authentication_tool_draft.ipynb` notebook:\n",
        "\n",
        "*   **`ImageSimilarityDetector`**: The primary public-facing class. It orchestrates all analysis methods and manages dependencies and resources.\n",
        "*   **Result Dataclasses**:\n",
        "    *   `ReverseImageSearchResult`: A structured container for results from the Google reverse image search.\n",
        "    *   `FeatureMatchResult`: A structured container for results from the ORB feature matching analysis.\n",
        "    *   `StatisticalProperties`: A reusable dataclass for encapsulating detailed statistical analysis of a data sample.\n",
        "*   **Factory Classes**:\n",
        "    *   `DefaultFeatureDetectorFactory`: Creates and caches optimized `cv2.ORB` instances.\n",
        "    *   `DefaultMatcherFactory`: Creates and profiles `cv2.BFMatcher` instances.\n",
        "    *   `DefaultClipModelLoader`: Manages the lifecycle (loading, caching, optimization) of CLIP models.\n",
        "*   **Protocol Definitions**:\n",
        "    *   `FeatureDetectorProtocol`, `MatcherProtocol`, `ClipModelLoaderProtocol`: Define the abstract interfaces for the core computational components, enabling dependency injection.\n",
        "*   **Custom Exception Hierarchy**:\n",
        "    *   `ImageSimilarityError`: The base exception for all toolkit-related errors.\n",
        "    *   Specialized subclasses (`ImageNotFoundError`, `ModelLoadError`, `NavigationError`, etc.) provide granular context for specific failure modes.\n",
        "\n",
        "## Setup and Installation\n",
        "\n",
        "A rigorous setup is required to ensure reproducible and accurate results.\n",
        "\n",
        "1.  **Prerequisites:**\n",
        "    *   Python 3.9 or newer.\n",
        "    *   `git` for cloning the repository.\n",
        "    *   A C++ compiler for building some of the underlying library dependencies.\n",
        "\n",
        "2.  **Clone the Repository:**\n",
        "    ```bash\n",
        "    git clone https://github.com/chirindaopensource/image_authentication_toolkit.git\n",
        "    cd image_authentication_toolkit\n",
        "    ```\n",
        "\n",
        "3.  **Create a Virtual Environment:**\n",
        "    It is imperative to work within a dedicated virtual environment to manage dependencies and avoid conflicts.\n",
        "    ```bash\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "4.  **Install Dependencies:**\n",
        "    The required Python packages are listed in `requirements.txt`.\n",
        "    ```bash\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "    *Note: The `requirements.txt` file would contain packages such as `numpy`, `opencv-python`, `torch`, `Pillow`, `imagehash`, `selenium`, `scipy`, `pandas`, and `ftfy` (for CLIP).*\n",
        "\n",
        "5.  **Install ChromeDriver:**\n",
        "    The `reverse_image_search_google` method requires the Selenium ChromeDriver.\n",
        "    *   **Verify your Chrome version:** Go to `chrome://settings/help`.\n",
        "    *   **Download the matching ChromeDriver:** Visit the [Chrome for Testing availability dashboard](https://googlechromelabs.github.io/chrome-for-testing/).\n",
        "    *   Place the `chromedriver` executable in the root of the project directory or another location in your system's `PATH`. The usage example assumes it is in the root.\n",
        "\n",
        "## Usage Example\n",
        "\n",
        "The following script demonstrates a complete, multi-modal analysis of two images.\n",
        "\n",
        "```python\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Assume all classes from the notebook are available in the execution scope.\n",
        "# This includes ImageSimilarityDetector, ResourceConstraints, ValidationPolicy,\n",
        "# and all custom exception classes.\n",
        "\n",
        "def demonstrate_image_provenance_analysis(\n",
        "    detector: ImageSimilarityDetector,\n",
        "    image1_path: Union[str, Path],\n",
        "    image2_path: Union[str, Path],\n",
        "    chromedriver_path: Union[str, Path]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes a comprehensive, multi-modal analysis to compare two images and\n",
        "    establish the provenance of the first image.\n",
        "\n",
        "    This function serves as a production-grade demonstration of the\n",
        "    ImageSimilarityDetector's capabilities, invoking each of its primary\n",
        "    analytical methods in a structured sequence. It captures results from\n",
        "    perceptual hashing, local feature matching, global color analysis,\n",
        "    semantic similarity, and public reverse image search.\n",
        "\n",
        "    The methodology proceeds from low-level structural comparisons to\n",
        "    high-level semantic and contextual analysis, providing a holistic\n",
        "    view of the relationship between the images.\n",
        "\n",
        "    Args:\n",
        "        detector (ImageSimilarityDetector): An initialized instance of the\n",
        "            image similarity detector.\n",
        "        image1_path (Union[str, Path]): The file path to the primary image\n",
        "            to be analyzed and compared. This image will also be used for\n",
        "            the reverse image search.\n",
        "        image2_path (Union[str, Path]): The file path to the secondary image\n",
        "            for comparison.\n",
        "        chromedriver_path (Union[str, Path]): The file path to the\n",
        "            Selenium ChromeDriver executable, required for reverse image search.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the detailed results from each\n",
        "            analysis stage. Each key corresponds to an analysis method, and\n",
        "            the value is either a comprehensive result dictionary/object or\n",
        "            an error message if that stage failed.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to aggregate the results from all analysis methods.\n",
        "    analysis_results: Dict[str, Any] = {}\n",
        "    # Configure logging to provide visibility into the analysis process.\n",
        "    logging.info(f\"Starting comprehensive provenance analysis for '{Path(image1_path).name}' and '{Path(image2_path).name}'.\")\n",
        "\n",
        "    # --- Stage 1: Perceptual Hash Analysis (Structural Duplication) ---\n",
        "    logging.info(\"Executing Stage 1: Perceptual Hash Analysis...\")\n",
        "    try:\n",
        "        p_hash_results = detector.perceptual_hash_difference(\n",
        "            image1_path, image2_path, hash_size=16, normalize=True,\n",
        "            return_similarity=True, statistical_analysis=True\n",
        "        )\n",
        "        analysis_results['perceptual_hash'] = p_hash_results\n",
        "        logging.info(f\"  - pHash Similarity Score: {p_hash_results.get('similarity_score', 'N/A'):.4f}\")\n",
        "    except Exception as e:\n",
        "        analysis_results['perceptual_hash'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        logging.error(f\"  - Perceptual Hash Analysis failed: {e}\")\n",
        "\n",
        "    # --- Stage 2: Local Feature Matching Analysis (Geometric Consistency) ---\n",
        "    logging.info(\"Executing Stage 2: Local Feature Matching Analysis...\")\n",
        "    try:\n",
        "        feature_match_results = detector.feature_match_ratio(\n",
        "            image1_path, image2_path, distance_threshold=64,\n",
        "            normalization_strategy=\"min_keypoints\", apply_ratio_test=True,\n",
        "            ratio_threshold=0.75, resize_max_side=1024,\n",
        "            return_detailed_result=True, geometric_verification=True,\n",
        "            statistical_analysis=True\n",
        "        )\n",
        "        analysis_results['feature_matching'] = feature_match_results\n",
        "        logging.info(f\"  - Feature Match Similarity Ratio: {feature_match_results.similarity_ratio:.4f}\")\n",
        "        logging.info(f\"  - Geometric Inlier Ratio: {feature_match_results.homography_inlier_ratio or 'N/A'}\")\n",
        "    except Exception as e:\n",
        "        analysis_results['feature_matching'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        logging.error(f\"  - Feature Matching Analysis failed: {e}\")\n",
        "\n",
        "    # --- Stage 3: Global Color Distribution Analysis (Palette Similarity) ---\n",
        "    logging.info(\"Executing Stage 3: Global Color Distribution Analysis...\")\n",
        "    try:\n",
        "        histogram_results = detector.histogram_correlation(\n",
        "            image1_path, image2_path, metric=\"correlation\", color_space=\"HSV\",\n",
        "            statistical_analysis=True, adaptive_binning=True\n",
        "        )\n",
        "        analysis_results['histogram_correlation'] = histogram_results\n",
        "        logging.info(f\"  - Histogram Correlation: {histogram_results.get('similarity_score', 'N/A'):.4f}\")\n",
        "    except Exception as e:\n",
        "        analysis_results['histogram_correlation'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        logging.error(f\"  - Histogram Correlation Analysis failed: {e}\")\n",
        "\n",
        "    # --- Stage 4: Semantic Meaning Analysis (Conceptual Similarity) ---\n",
        "    logging.info(\"Executing Stage 4: Semantic Meaning Analysis...\")\n",
        "    try:\n",
        "        clip_results = detector.clip_embedding_similarity(\n",
        "            image1_path, image2_path, statistical_analysis=True,\n",
        "            embedding_analysis=True, batch_processing=True\n",
        "        )\n",
        "        analysis_results['semantic_similarity'] = clip_results\n",
        "        logging.info(f\"  - CLIP Cosine Similarity: {clip_results.get('cosine_similarity', 'N/A'):.4f}\")\n",
        "    except Exception as e:\n",
        "        analysis_results['semantic_similarity'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        logging.error(f\"  - Semantic Similarity Analysis failed: {e}\")\n",
        "\n",
        "    # --- Stage 5: Public Provenance and Context Analysis (Web Discovery) ---\n",
        "    logging.info(\"Executing Stage 5: Public Provenance Analysis...\")\n",
        "    try:\n",
        "        reverse_search_results = detector.reverse_image_search_google(\n",
        "            image_path=image1_path, driver_path=chromedriver_path,\n",
        "            headless=True, advanced_extraction=True, content_analysis=True\n",
        "        )\n",
        "        analysis_results['reverse_image_search'] = reverse_search_results\n",
        "        logging.info(f\"  - Reverse Search Best Guess: {reverse_search_results.best_guess}\")\n",
        "        logging.info(f\"  - Found {len(reverse_search_results.similar_image_urls)} similar images online.\")\n",
        "    except Exception as e:\n",
        "        analysis_results['reverse_image_search'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        logging.error(f\"  - Reverse Image Search failed: {e}\")\n",
        "\n",
        "    logging.info(\"Comprehensive provenance analysis complete.\")\n",
        "    return analysis_results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This block demonstrates how to run the analysis.\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    test_dir = Path(\"./test_images\")\n",
        "    test_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Create test images\n",
        "    image1_path = test_dir / \"original_image.png\"\n",
        "    image2_path = test_dir / \"modified_image.jpg\"\n",
        "    cv2.imwrite(str(image1_path), np.full((512, 512, 3), 64, dtype=np.uint8))\n",
        "    cv2.imwrite(str(image2_path), np.full((512, 512, 3), 68, dtype=np.uint8))\n",
        "\n",
        "    chromedriver_path = Path(\"./chromedriver\")\n",
        "    if not chromedriver_path.exists():\n",
        "        logging.error(\"FATAL: ChromeDriver not found. Please download it and place it in the project root.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    detector = ImageSimilarityDetector()\n",
        "    full_results = demonstrate_image_provenance_analysis(detector, image1_path, image2_path, chromedriver_path)\n",
        "\n",
        "    def result_serializer(obj):\n",
        "        if isinstance(obj, (Path, np.ndarray)): return str(obj)\n",
        "        if hasattr(obj, 'to_dict'): return obj.to_dict()\n",
        "        return str(obj)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40 + \" ANALYSIS RESULTS \" + \"=\"*40)\n",
        "    print(json.dumps(full_results, default=result_serializer, indent=2))\n",
        "    print(\"=\"*100)\n",
        "```\n",
        "\n",
        "## Theoretical Foundations\n",
        "\n",
        "The implementation rests on established principles from multiple scientific and engineering disciplines.\n",
        "\n",
        "*   **Software Engineering & Design Patterns**\n",
        "    *   **Object-Oriented Design:** Encapsulation of logic within classes (`ImageSimilarityDetector`, `ResourceManager`).\n",
        "    *   **SOLID Principles:** Dependency Inversion is used via `Protocol`s to decouple the main class from concrete algorithm implementations. Single Responsibility is evident in the separation of concerns between factories, result objects, and the main detector.\n",
        "    *   **Resource Management:** Lazy loading (`_load_clip...`) and background monitoring (`ResourceManager`) ensure efficient use of memory and compute.\n",
        "    *   **Error Handling:** A comprehensive, custom exception hierarchy allows for granular error reporting and robust recovery.\n",
        "\n",
        "*   **Computer Vision & Image Processing**\n",
        "    *   **Feature Detection:** The ORB implementation is based on the canonical papers for FAST corners and BRIEF descriptors, with added logic for orientation invariance.\n",
        "    *   **Perceptual Hashing:** The `pHash` algorithm is a direct application of frequency-domain analysis using the Discrete Cosine Transform (DCT) to create a scale- and compression-invariant image fingerprint.\n",
        "    *   **Histogram Analysis:** The use of HSV and LAB color spaces is a standard technique to achieve a degree of illumination invariance in color-based comparisons.\n",
        "\n",
        "*   **Machine Learning & Deep Learning**\n",
        "    *   **Contrastive Learning:** The `clip_embedding_similarity` method is a direct application of the CLIP model, which learns a joint embedding space for images and text through contrastive learning on a massive dataset.\n",
        "    *   **Vision Transformer (ViT):** The CLIP model's vision component is a ViT, which processes images as sequences of patches using self-attention mechanisms, enabling it to capture global semantic context.\n",
        "\n",
        "*   **Mathematics & Statistics**\n",
        "    *   **Linear Algebra:** Cosine similarity is computed via the dot product of L2-normalized embedding vectors.\n",
        "    *   **Probability & Statistics:** Pearson correlation is used for histogram comparison. The Hamming distance is a fundamental metric from information theory. Statistical confidence intervals are computed for key metrics to quantify uncertainty.\n",
        "    *   **Geometric Verification:** Homography estimation via RANSAC is a robust statistical method for finding a geometric consensus among noisy data points (the feature matches).\n",
        "\n",
        "## Error Handling and Robustness\n",
        "\n",
        "The system is designed for production environments and incorporates multiple layers of error handling:\n",
        "\n",
        "*   **Custom Exception Hierarchy:** Allows for specific and actionable error catching (e.g., distinguishing a `ModelLoadError` from a `NavigationError`).\n",
        "*   **Input Validation:** Each public method rigorously validates its inputs against mathematical and logical constraints before proceeding. The `_validate_image_path` method is particularly extensive, checking for existence, permissions, file type, and content integrity.\n",
        "*   **Retry Mechanisms:** Core operations, such as web driver initialization and page navigation, are wrapped in retry loops with exponential backoff to handle transient network or system failures.\n",
        "*   **Fallback Strategies:** The CLIP model loader will automatically fall back from GPU to CPU upon encountering a `CUDA OutOfMemoryError`, ensuring the operation can complete, albeit more slowly. The web automation uses a hierarchy of selectors to find UI elements, making it resilient to minor front-end changes.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions that adhere to a high standard of methodological rigor are welcome.\n",
        "\n",
        "1.  Fork the repository.\n",
        "2.  Create a new branch for your feature (`git checkout -b feature/your-feature-name`).\n",
        "3.  Develop your feature, ensuring it is accompanied by appropriate unit tests.\n",
        "4.  Ensure your code adheres to the PEP-8 style guide.\n",
        "5.  Submit a pull request with a detailed description of your changes and their justification.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this toolkit in your academic research, please cite it as follows:\n",
        "\n",
        "```bibtex\n",
        "@software{Chirinda_Image_Authentication_Toolkit_2025,\n",
        "  author = {Chirinda, Craig},\n",
        "  title = {{Image Authentication Toolkit}},\n",
        "  year = {2025},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\url{https://github.com/chirindaopensource/image_authentication_toolkit}}\n",
        "}\n",
        "```\n",
        "\n",
        "--\n",
        "\n",
        "This README was generated based on the structure and content of image_authentication_tool_draft.ipynb and follows best practices for research software documentation."
      ],
      "metadata": {
        "id": "zzzPSbnkSciM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "## Overview\n",
        "\n",
        "The emergence of sophisticated generative models for visual art has created a non-trivial challenge at the intersection of creativity, copyright, and computational analysis. The core of the issue is the ambiguity of \"originality.\"\n",
        "\n",
        "When a model trained on a vast corpus of human-created art produces a new image, the question of its provenance—is it a novel creation, a stylistic amalgamation, or a trivial replication of its training data?—cannot be answered through subjective assessment alone.\n",
        "\n",
        "The `ImageSimilarityDetector` tool was necessitated by this demand for a quantitative, multi-faceted, and methodologically rigorous framework to dissect the relationship between a generated image and existing visual works. Its purpose is to replace subjective claims with empirical evidence, enabling a structured analysis of an image's uniqueness, semantic meaning, and public context.\n",
        "\n",
        "The tool's efficacy stems from its deployment of a spectrum of analytical techniques, each with distinct capabilities and limitations. The most rudimentary methods, `histogram_correlation` and `perceptual_hash_difference`, operate on low-level features.\n",
        "\n",
        "Histogram analysis provides a global statistical signature of an image's color palette but is entirely blind to spatial structure and semantic content; it can only detect gross color scheme similarities and is thus the weakest signal for originality.\n",
        "\n",
        "Perceptual hashing (pHash), based on the Discrete Cosine Transform, is substantially more potent. It creates a compact fingerprint of an image's low-frequency structural information, making it robust to minor perturbations like compression and resizing. Its function is to detect near-verbatim copies. Its limitation is its inability to recognize semantic similarity; it cannot connect two different photographs of the same subject. For AI art, pHash serves as a first-line defense against plagiarism, capable of flagging instances where a model has memorized and reproduced a training image with high fidelity.\n",
        "\n",
        "Moving to a higher level of structural analysis, the `feature_match_ratio` method, which implements the ORB algorithm, provides a more granular comparison. By detecting and matching thousands of salient local features (e.g., corners, textures) between two images, it can identify if one image contains a direct, geometrically consistent copy of a component from another, even if it has been scaled, rotated, or partially occluded. This is mathematically refined through RANSAC-based homography checks to ensure the spatial relationship between matched features is coherent. Its primary limitation is its semantic ignorance; it matches local pixel patterns, not concepts. In the context of AI art, this method is indispensable for detecting \"asset ripping\" or digital collage, where a model might lift a specific, copyrighted element—a character, a logo, a unique architectural feature—and incorporate it into a new composition. A high ratio of geometrically consistent matches provides strong, quantitative evidence of direct derivation.\n",
        "\n",
        "The most abstract and powerful technique in the toolkit is `clip_embedding_similarity`. This method leverages a Vision Transformer to project an image into a high-dimensional semantic embedding space, where the vector's position is determined by the image's conceptual meaning, not its pixel-level appearance. The cosine similarity between two such vectors provides a measure of their semantic proximity. This allows the tool to understand that a photograph of a cat and a cubist painting of a cat are related, a feat impossible for the other methods. Its primary limitation is that it cannot prove direct, structural copying; high semantic similarity is not, by itself, evidence of copyright infringement. For calibrating AI art, this is the essential method for quantifying stylistic influence and conceptual overlap. By comparing a generated piece to an artist's portfolio, one can measure the degree to which the AI has replicated that artist's unique semantic signature, moving the discussion from a vague \"it looks like\" to a precise, numerical measure of stylistic proximity.\n",
        "\n",
        "Finally, the `reverse_image_search_google` callable serves a distinct but critical purpose: establishing public provenance. By automating a search, it determines if the image or a near-duplicate already exists in the public domain. This is not a direct similarity metric but a discovery tool. Its primary limitation is its reliance on a third-party, black-box search algorithm (Google's) and its confinement to publicly indexed content. For AI art, this method provides an essential baseline check. A \"no results found\" outcome is a preliminary indicator of novelty. Conversely, if the search returns an existing artwork, it provides immediate evidence of replication. Furthermore, the textual descriptions and context of the returned results can offer invaluable clues into the semantic components the model may have synthesized to create the image, thereby illuminating its conceptual lineage.\n",
        "\n",
        "In synthesis, no single method is sufficient for the rigorous task of calibrating AI art authenticity. The strength of the `ImageSimilarityDetector` lies in its integrated, multi-modal approach. A robust analysis requires a fusion of these techniques: pHash to check for direct replication, feature matching to detect asset collage, CLIP similarity to quantify stylistic and conceptual derivation, and reverse image search to establish public context. The collective output of these methods transforms the ambiguous question of originality into a multi-dimensional, quantitative assessment, providing the empirical foundation necessary for informed and defensible conclusions in the management of digital assets.\n",
        "\n",
        "## Usage Examples\n",
        "\n",
        "### Perceptual Hash Analysis\n",
        "\n",
        "**Purpose:** To detect near-verbatim structural copies of an image. This method is robust to minor modifications such as resizing, compression, and slight color shifts.\n",
        "**Mathematical Basis:** The pHash algorithm uses the Discrete Cosine Transform (DCT) to extract a low-frequency fingerprint of the image. The Hamming distance between the binary fingerprints of two images measures their structural dissimilarity.\n",
        "**Usage Snippet:** We will invoke the `perceptual_hash_difference` method with full statistical analysis enabled. This provides not only the raw distance but also a confidence interval, giving us a measure of certainty in the result.\n",
        "\n",
        "```python\n",
        "# Snippet 1: Perceptual Hash Analysis\n",
        "try:\n",
        "    # Execute perceptual hash analysis with full statistical reporting.\n",
        "    # This method is computationally inexpensive and effective for finding near-duplicates.\n",
        "    p_hash_results = detector.perceptual_hash_difference(\n",
        "        image1_path,\n",
        "        image2_path,\n",
        "        hash_size=16,  # Use a 16x16 hash (256 bits) for higher precision.\n",
        "        normalize=True,\n",
        "        return_similarity=True,\n",
        "        statistical_analysis=True\n",
        "    )\n",
        "    # Store the comprehensive results.\n",
        "    analysis_results['perceptual_hash'] = p_hash_results\n",
        "except (ImageUnreadableError, ValueError, RuntimeError) as e:\n",
        "    # Handle errors related to image processing or hash computation.\n",
        "    analysis_results['perceptual_hash'] = {'error': str(e)}\n",
        "```\n",
        "\n",
        "### Local Feature Matching Analysis\n",
        "\n",
        "**Purpose:** To identify if one image contains geometrically consistent sections of another. This is critical for detecting digital collage or \"asset ripping.\"\n",
        "**Mathematical Basis:** The ORB algorithm detects salient keypoints (corners) and generates binary descriptors. These are matched using Hamming distance. The RANSAC algorithm is then used to find a homography (a perspective transform) that explains the spatial relationship between the matched points, filtering out spurious matches.\n",
        "**Usage Snippet:** We invoke `feature_match_ratio` with geometric verification and detailed results enabled. The key output is the `homography_inlier_ratio`, which quantifies the geometric consistency of the match.\n",
        "\n",
        "```python\n",
        "# Snippet 2: Local Feature Matching Analysis\n",
        "try:\n",
        "    # Execute feature matching with geometric verification (RANSAC).\n",
        "    # This is computationally more intensive but detects structural copying, even with transformations.\n",
        "    feature_match_results = detector.feature_match_ratio(\n",
        "        image1_path,\n",
        "        image2_path,\n",
        "        distance_threshold=64,  # A standard Hamming distance threshold for ORB.\n",
        "        normalization_strategy=\"min_keypoints\",\n",
        "        apply_ratio_test=True,  # Use Lowe's ratio test for more robust matching.\n",
        "        ratio_threshold=0.75,\n",
        "        resize_max_side=1024,  # Resize for performance without significant feature loss.\n",
        "        return_detailed_result=True,\n",
        "        geometric_verification=True\n",
        "    )\n",
        "    # Store the comprehensive FeatureMatchResult object.\n",
        "    analysis_results['feature_matching'] = feature_match_results\n",
        "except (ImageUnreadableError, RuntimeError) as e:\n",
        "    # Handle errors in the feature detection or matching pipeline.\n",
        "    analysis_results['feature_matching'] = {'error': str(e)}\n",
        "```\n",
        "\n",
        "### Global Color Distribution Analysis\n",
        "\n",
        "**Purpose:** To compare the overall color palettes of two images. This is the weakest signal for originality but can be useful for identifying images with a shared aesthetic or origin (e.g., from the same film scene).\n",
        "**Mathematical Basis:** Images are converted to a perceptually uniform color space (like HSV). A multi-dimensional histogram is computed, normalized to a probability distribution, and then compared using a statistical metric like Pearson correlation.\n",
        "**Usage Snippet:** We invoke `histogram_correlation` using the HSV color space, as it decouples illumination from color information, making the comparison more robust.\n",
        "\n",
        "```python\n",
        "# Snippet 3: Global Color Distribution Analysis\n",
        "try:\n",
        "    # Execute color histogram correlation.\n",
        "    # This measures similarity in the global color distribution, ignoring spatial structure.\n",
        "    histogram_results = detector.histogram_correlation(\n",
        "        image1_path,\n",
        "        image2_path,\n",
        "        metric=\"correlation\",  # Pearson correlation is a robust similarity measure.\n",
        "        color_space=\"HSV\",  # HSV is robust to lighting changes.\n",
        "        statistical_analysis=True\n",
        "    )\n",
        "    # Store the comprehensive results.\n",
        "    analysis_results['histogram_correlation'] = histogram_results\n",
        "except (ImageUnreadableError, HistogramError) as e:\n",
        "    # Handle errors in histogram computation or comparison.\n",
        "    analysis_results['histogram_correlation'] = {'error': str(e)}\n",
        "```\n",
        "\n",
        "### Semantic Meaning Analysis\n",
        "\n",
        "**Purpose:** To measure the conceptual or semantic similarity between two images, irrespective of their visual style or composition.\n",
        "**Mathematical Basis:** The CLIP model, a Vision Transformer, projects each image into a shared, high-dimensional embedding space. The cosine similarity of the resulting vectors measures their proximity in this \"meaning\" space. A value near 1.0 indicates high semantic overlap.\n",
        "**Usage Snippet:** We invoke `clip_embedding_similarity` with full analysis enabled to get not only the similarity score but also a confidence interval and properties of the embedding vectors themselves.\n",
        "\n",
        "```python\n",
        "# Snippet 4: Semantic Meaning Analysis\n",
        "try:\n",
        "    # Execute semantic similarity analysis using the CLIP model.\n",
        "    # This is the most abstract comparison, measuring conceptual similarity.\n",
        "    clip_results = detector.clip_embedding_similarity(\n",
        "        image1_path,\n",
        "        image2_path,\n",
        "        statistical_analysis=True,\n",
        "        embedding_analysis=True\n",
        "    )\n",
        "    # Store the comprehensive results.\n",
        "    analysis_results['semantic_similarity'] = clip_results\n",
        "except (ModelLoadError, ModelInferenceError, ValueError) as e:\n",
        "    # Handle errors related to model loading or inference.\n",
        "    analysis_results['semantic_similarity'] = {'error': str(e)}\n",
        "```\n",
        "\n",
        "### Public Provenance and Context Analysis\n",
        "\n",
        "**Purpose:** To determine if an image already exists in the public domain and to gather context about its usage. This is a discovery, not a comparison, method.\n",
        "**Mathematical Basis:** This process leverages the complex, proprietary algorithms of a third-party search engine (Google). The core principle is to use the image itself as a query to a vast, indexed database of web content.\n",
        "**Usage Snippet:** We invoke `reverse_image_search_google` on one of the images. The function automates a web browser to perform the search and scrapes the results, which are then structured into a `ReverseImageSearchResult` object.\n",
        "\n",
        "```python\n",
        "# Snippet 5: Public Provenance and Context Analysis\n",
        "try:\n",
        "    # Execute a reverse image search on the first image to establish public provenance.\n",
        "    # This checks if the image or near-duplicates exist on the public internet.\n",
        "    reverse_search_results = detector.reverse_image_search_google(\n",
        "        image_path=image1_path,\n",
        "        driver_path=chromedriver_path,\n",
        "        headless=True  # Run in the background for automated environments.\n",
        "    )\n",
        "    # Store the comprehensive ReverseImageSearchResult object.\n",
        "    analysis_results['reverse_image_search'] = reverse_search_results\n",
        "except (LaunchError, NavigationError, UploadError, ExtractionError) as e:\n",
        "    # Handle the various failure modes of web automation.\n",
        "    analysis_results['reverse_image_search'] = {'error': str(e)}\n",
        "except Exception as e:\n",
        "    # Handle any other unexpected errors during the search.\n",
        "    analysis_results['reverse_image_search'] = {'error': f\"An unexpected error occurred: {e}\"}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "E39r8sw8GUov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "gUcVtfknjkvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Essential Modules\n",
        "# Standard Library Imports\n",
        "import csv\n",
        "import datetime\n",
        "import functools\n",
        "import gc\n",
        "import hashlib\n",
        "import inspect\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import statistics\n",
        "import sys\n",
        "import threading\n",
        "import time\n",
        "import traceback\n",
        "import warnings\n",
        "import xml.etree.ElementTree as ET\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass, field, fields, asdict\n",
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "    Any,\n",
        "    Callable,\n",
        "    ClassVar,\n",
        "    Dict,\n",
        "    Iterator,\n",
        "    List,\n",
        "    Literal,\n",
        "    Optional,\n",
        "    Protocol,\n",
        "    Tuple,\n",
        "    Type,\n",
        "    Union,\n",
        "    runtime_checkable\n",
        ")\n",
        "\n",
        "# Third-Party Imports\n",
        "import clip\n",
        "import cv2\n",
        "import imagehash\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import scipy.stats as stats\n",
        "import torch\n",
        "from PIL import Image\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "\n"
      ],
      "metadata": {
        "id": "U23hiWu-WNNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "## Draft 1\n",
        "### Description of Callables in `image_authentication_tool_draft.ipynb`\n",
        "\n",
        "The notebook is structured into several cells, primarily defining custom exceptions, protocols, factory classes, result data structures, and the main `ImageSimilarityDetector` class. Below is a description of all the callables in the workbook in their order of appearance.\n",
        "\n",
        "--\n",
        "\n",
        "### **Custom Error Classes and Protocol Classes**\n",
        "\n",
        "This cell establishes the foundational components for error handling, protocol-driven design, and runtime validation.\n",
        "\n",
        "**1. `ForensicMetadata.__init__`**\n",
        "*   **Inputs:**\n",
        "    *   `operation_name` (str): The name of the operation where the error occurred.\n",
        "    *   `thread_id` (Optional[int]): The identifier of the executing thread. Defaults to the current thread's ID.\n",
        "    *   `process_id` (Optional[int]): The identifier of the executing process. Defaults to the current process's ID.\n",
        "    *   `timestamp` (Optional[datetime.datetime]): The precise UTC timestamp of the event. Defaults to the current UTC time.\n",
        "    *   `system_info` (Optional[Dict[str, Any]]): A dictionary of system state information.\n",
        "    *   `algorithm_parameters` (Optional[Dict[str, Any]]): A dictionary of parameters used by the algorithm at the time of the event.\n",
        "    *   `memory_usage` (Optional[Dict[str, float]]): A dictionary of memory metrics.\n",
        "    *   `call_stack` (Optional[List[str]]): The execution call stack. Defaults to the current stack trace.\n",
        "*   **Processes:**\n",
        "    1.  The method initializes an instance of the `ForensicMetadata` class.\n",
        "    2.  It assigns the provided `operation_name` to an instance variable.\n",
        "    3.  For optional parameters (`thread_id`, `process_id`, `timestamp`, `call_stack`), it checks if a value was provided. If not, it programmatically captures the current state using the `threading`, `sys`, `datetime`, and `traceback` modules, respectively.\n",
        "    4.  It assigns the remaining dictionary-based inputs (`system_info`, `algorithm_parameters`, `memory_usage`) to instance variables, defaulting to empty dictionaries if none are provided.\n",
        "*   **Outputs:**\n",
        "    *   A fully instantiated `ForensicMetadata` object containing a comprehensive snapshot of the system and execution context at the time of its creation.\n",
        "\n",
        "**2. `ForensicMetadata.to_dict`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ForensicMetadata): The instance of the class.\n",
        "*   **Processes:**\n",
        "    1.  The method constructs a new dictionary.\n",
        "    2.  It populates this dictionary with the values of all instance variables (`operation_name`, `thread_id`, etc.).\n",
        "    3.  The `timestamp` (a `datetime` object) is transformed into a string by calling its `isoformat()` method, ensuring JSON compatibility.\n",
        "*   **Outputs:**\n",
        "    *   A `Dict[str, Any]` representing the serialized state of the `ForensicMetadata` object.\n",
        "\n",
        "**3. `ForensicMetadata.to_json`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ForensicMetadata): The instance of the class.\n",
        "*   **Processes:**\n",
        "    1.  It first calls `self.to_dict()` to get the dictionary representation of the object.\n",
        "    2.  This dictionary is then passed to the `json.dumps()` function.\n",
        "    3.  The serialization process is configured with an indent of 2 for human readability and a `default` handler of `str` to manage any non-standard data types that might remain.\n",
        "*   **Outputs:**\n",
        "    *   A `str` containing the JSON-formatted representation of the forensic metadata.\n",
        "\n",
        "**4. `ImageSimilarityError.__init__`**\n",
        "*   **Inputs:**\n",
        "    *   `message` (str): A human-readable description of the error.\n",
        "    *   `severity` (ErrorSeverity): An enum value classifying the error's severity.\n",
        "    *   `operation_name` (Optional[str]): The name of the failed operation.\n",
        "    *   `error_code` (Optional[str]): A unique code for the error.\n",
        "    *   `forensic_metadata` (Optional[ForensicMetadata]): A pre-populated forensic metadata object.\n",
        "    *   `algorithm_context` (Optional[Dict[str, Any]]): Algorithm-specific state.\n",
        "    *   `original_exception` (Optional[Exception]): The underlying exception being wrapped.\n",
        "    *   `suggested_remediation` (Optional[str]): Guidance for resolving the error.\n",
        "*   **Processes:**\n",
        "    1.  The method first validates the `message` input, providing a default if it's `None` or not a string.\n",
        "    2.  It calls the `super().__init__(message)` to initialize the base `Exception` class.\n",
        "    3.  It assigns all provided inputs to corresponding instance variables, applying default values where inputs are `None`.\n",
        "    4.  If `forensic_metadata` is not provided, it instantiates a new `ForensicMetadata` object, capturing the current context.\n",
        "    5.  It captures the current UTC time as the error `timestamp`.\n",
        "*   **Outputs:**\n",
        "    *   A fully instantiated `ImageSimilarityError` object (or a subclass thereof), which is a specialized exception containing rich contextual information for debugging and logging.\n",
        "\n",
        "**5. `ImageSimilarityError.__str__`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ImageSimilarityError): The instance of the exception.\n",
        "*   **Processes:**\n",
        "    1.  It constructs a formatted string, starting with the severity level (e.g., `[HIGH]`) and the core error message.\n",
        "    2.  It conditionally appends the `operation_name`, `error_code`, and `suggested_remediation` if they are available, creating a concise, human-readable summary of the error.\n",
        "*   **Outputs:**\n",
        "    *   A `str` suitable for direct display to a user or for simple log messages.\n",
        "\n",
        "**6. `ImageSimilarityError.to_dict`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ImageSimilarityError): The instance of the exception.\n",
        "*   **Processes:**\n",
        "    1.  It creates a dictionary containing the primary attributes of the exception (`error_class`, `message`, `severity`, etc.).\n",
        "    2.  The `severity` enum is transformed into its string value. The `timestamp` is transformed into an ISO format string.\n",
        "    3.  If `forensic_metadata` exists, it calls `self.forensic_metadata.to_dict()` and embeds the resulting dictionary.\n",
        "    4.  If `original_exception` exists, it creates a nested dictionary containing the type, message, and arguments of the original exception.\n",
        "*   **Outputs:**\n",
        "    *   A `Dict[str, Any]` containing a structured, serializable representation of the entire exception state.\n",
        "\n",
        "**7. `ImageSimilarityError.log_error`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ImageSimilarityError): The instance of the exception.\n",
        "    *   `logger` (Optional[logging.Logger]): An optional logger instance.\n",
        "*   **Processes:**\n",
        "    1.  It obtains a logger instance, either the one provided or a new one based on the module and class name.\n",
        "    2.  It maps the `ErrorSeverity` enum of the exception to a `logging` level (e.g., `ErrorSeverity.HIGH` to `logging.ERROR`).\n",
        "    3.  It calls `self.to_dict()` to get the full structured error data.\n",
        "    4.  It uses the logger's `log` method to record the error, passing the structured data dictionary to the `extra` parameter. This enables structured logging systems (like those outputting JSON logs) to capture the full error context.\n",
        "*   **Outputs:**\n",
        "    *   None. The process results in a side effect: a log record is emitted.\n",
        "\n",
        "**8. `protocol_validator` (Decorator Factory)**\n",
        "*   **Inputs:**\n",
        "    *   `constraints` (Optional[Dict[str, Any]]): A dictionary defining validation rules for function parameters and its return value.\n",
        "    *   `log_violations` (bool): A flag to control whether violations are logged.\n",
        "*   **Processes:**\n",
        "    1.  This is a factory that returns a decorator. When called, it captures the `constraints` and `log_violations` in a closure.\n",
        "    2.  The returned `decorator` function takes a function `func` as input.\n",
        "    3.  The `decorator` returns a `wrapper` function that replaces the original `func`.\n",
        "    4.  When the `wrapper` is called with `*args` and `**kwargs`:\n",
        "        a. It uses `inspect.signature` to map the provided arguments to the parameter names of the decorated function.\n",
        "        b. It iterates through the `constraints` dictionary. For each parameter, it checks if the provided value violates the specified 'type', 'bounds', or 'shape' constraints.\n",
        "        c. If a violation is found, it logs the error (if `log_violations` is true) and raises a `ValueError` or `TypeError`.\n",
        "        d. If all input validations pass, it calls the original function `func` with the arguments.\n",
        "        e. It then validates the `result` of the function call against any 'return_value' constraints in the same manner.\n",
        "        f. If the return value is valid, it is returned to the caller.\n",
        "*   **Outputs:**\n",
        "    *   A decorator function (`decorator`) which, when applied to another function, returns a new function (`wrapper`) that performs runtime validation before and after executing the original function's logic.\n",
        "\n",
        "**9. `validate_protocol_implementation`**\n",
        "*   **Inputs:**\n",
        "    *   `obj` (Any): The object instance whose class is being validated.\n",
        "    *   `protocol_class` (Type[Protocol]): The protocol class that `obj` is expected to implement.\n",
        "    *   `strict` (bool): A flag to enable strict checking of parameter names and type annotations.\n",
        "*   **Processes:**\n",
        "    1.  It initializes a list `violations` to store descriptions of any conformance failures.\n",
        "    2.  It iterates through the attributes of the `protocol_class` to identify all public callable methods.\n",
        "    3.  For each protocol method, it checks for its existence and callability on the input `obj`.\n",
        "    4.  Using the `inspect` module, it retrieves the signatures of both the protocol method and the implementation method.\n",
        "    5.  It compares the number of parameters.\n",
        "    6.  If `strict` is true, it performs a parameter-by-parameter comparison of names and type annotations.\n",
        "    7.  Any discrepancy is transformed into a descriptive string and appended to the `violations` list.\n",
        "*   **Outputs:**\n",
        "    *   A `Tuple[bool, List[str]]`: The boolean indicates overall validation success, and the list contains all identified violation messages.\n",
        "\n",
        "**10. `register_protocol_implementation`**\n",
        "*   **Inputs:**\n",
        "    *   `impl_class` (Type): The class that is intended to implement the protocol.\n",
        "    *   `protocol_class` (Type[Protocol]): The protocol class.\n",
        "    *   `validate_on_registration` (bool): A flag to control whether to perform validation at registration time.\n",
        "*   **Processes:**\n",
        "    1.  If `validate_on_registration` is true, it attempts to create a temporary instance of `impl_class`. This requires the class to have a parameter-less `__init__` method.\n",
        "    2.  It then calls `validate_protocol_implementation` on this temporary instance.\n",
        "    3.  If validation fails, it aggregates the violation messages and raises a `TypeError`.\n",
        "    4.  If validation succeeds (or is skipped), it calls `protocol_class.register(impl_class)`. This built-in function makes Python's `isinstance()` checks recognize `impl_class` as a virtual subclass of `protocol_class` without requiring traditional inheritance.\n",
        "*   **Outputs:**\n",
        "    *   The original `impl_class`, now registered as an implementer of the protocol.\n",
        "\n",
        "--\n",
        "\n",
        "### **Factory Classes for Resource Management**\n",
        "\n",
        "This cell defines the machinery for creating and managing computational resources, particularly for computer vision algorithms, with a focus on optimization and performance.\n",
        "\n",
        "**1. `ParameterOptimizer.__init__`**\n",
        "*   **Inputs:**\n",
        "    *   `constraints` (Dict[str, ParameterConstraints]): A mapping of parameter names to their mathematical and optimization constraints.\n",
        "    *   `optimization_strategy` (OptimizationStrategy): An enum specifying the search algorithm to use.\n",
        "    *   `performance_samples` (int): The number of times to run the objective function for a stable performance measurement.\n",
        "    *   `convergence_threshold` (float): The minimum improvement required to consider a new set of parameters \"better\".\n",
        "    *   `max_iterations` (int): The maximum number of optimization steps to perform.\n",
        "*   **Processes:**\n",
        "    1.  The method initializes an instance of the `ParameterOptimizer`.\n",
        "    2.  It stores all the input configurations as instance variables.\n",
        "    3.  It initializes internal state variables: `optimization_history` (an empty list to track steps), `best_parameters` (None), and `best_performance` (None).\n",
        "    4.  It acquires a logger instance for monitoring the optimization process.\n",
        "*   **Outputs:**\n",
        "    *   A configured `ParameterOptimizer` object, ready to run an optimization task.\n",
        "\n",
        "**2. `ParameterOptimizer.validate_parameters`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ParameterOptimizer): The instance of the class.\n",
        "    *   `parameters` (Dict[str, Any]): A dictionary of parameter values to be validated.\n",
        "*   **Processes:**\n",
        "    1.  It iterates through the input `parameters` dictionary.\n",
        "    2.  For each parameter, it retrieves the corresponding `ParameterConstraints` object from `self.constraints`.\n",
        "    3.  It performs a series of checks on the parameter's value against its defined constraints:\n",
        "        a. Type check (`isinstance`).\n",
        "        b. Bounds check (min <= value <= max).\n",
        "        c. Dependency check by calling `_validate_dependency` for any specified inter-parameter relationships.\n",
        "    4.  Each violation is transformed into a descriptive string and added to a `violations` list.\n",
        "*   **Outputs:**\n",
        "    *   A `Tuple[bool, List[str]]`: The boolean indicates if all parameters are valid, and the list contains messages for any violations found.\n",
        "\n",
        "**3. `ParameterOptimizer.optimize_parameters`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ParameterOptimizer): The instance of the class.\n",
        "    *   `objective_function` (Callable): A function that takes a dictionary of parameters and returns a `PerformanceMetrics` object.\n",
        "    *   `initial_parameters` (Optional[Dict[str, Any]]): An optional starting point for the optimization.\n",
        "*   **Processes:**\n",
        "    1.  It establishes a starting set of parameters, either from `initial_parameters` (after validation) or by generating defaults.\n",
        "    2.  It enters a loop that runs for a maximum of `max_iterations`.\n",
        "    3.  Inside the loop, it generates a new set of `candidate_parameters` by calling a step function (`_grid_search_step`, `_random_search_step`, etc.) based on the configured `optimization_strategy`.\n",
        "    4.  It validates these candidate parameters.\n",
        "    5.  If valid, it calls `_evaluate_parameters_with_statistics`, which repeatedly executes the `objective_function` with the candidate parameters to get a statistically stable performance measurement.\n",
        "    6.  It computes a single `composite_score` from the resulting `PerformanceMetrics` object.\n",
        "    7.  This score is compared to the current `best_score`. If it represents a significant improvement (exceeding `convergence_threshold`), the `best_score`, `best_parameters`, and `best_performance` are updated.\n",
        "    8.  The results of the iteration are appended to `self.optimization_history`.\n",
        "    9.  The loop includes a convergence check: if there is no improvement for a set number of consecutive iterations, the optimization terminates early.\n",
        "*   **Outputs:**\n",
        "    *   A `Tuple[Dict[str, Any], PerformanceMetrics]`: The best parameter set found and the corresponding performance metrics.\n",
        "\n",
        "**4. `ResourceManager.__init__`**\n",
        "*   **Inputs:**\n",
        "    *   `resource_constraints` (ResourceConstraints): An enum indicating the resource environment (e.g., `LOW_MEMORY`).\n",
        "    *   `monitoring_interval` (float): The time in seconds between resource checks.\n",
        "    *   `cleanup_threshold` (float): The resource usage percentage (e.g., 0.8 for 80%) that triggers a cleanup.\n",
        "*   **Processes:**\n",
        "    1.  Initializes a `ResourceManager` instance, storing the configuration.\n",
        "    2.  Initializes state variables for monitoring, including a list for `resource_history` and thread-safe locks.\n",
        "    3.  Calls `_configure_resource_limits` to transform the abstract `resource_constraints` enum into concrete numerical limits (e.g., max memory in bytes, max CPU cores).\n",
        "*   **Outputs:**\n",
        "    *   A configured `ResourceManager` object.\n",
        "\n",
        "**5. `ResourceManager.start_monitoring`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ResourceManager): The instance of the class.\n",
        "*   **Processes:**\n",
        "    1.  It acquires a lock to ensure thread safety.\n",
        "    2.  It checks if monitoring is already active to prevent creating duplicate threads.\n",
        "    3.  If not active, it sets the `monitoring_active` flag to `True`.\n",
        "    4.  It creates and starts a new `threading.Thread`, targeting the `_monitoring_loop` method. The thread is configured as a daemon so it does not block program exit.\n",
        "*   **Outputs:**\n",
        "    *   None. The process results in a side effect: a background monitoring thread is started.\n",
        "\n",
        "**6. `ResourceManager._monitoring_loop`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ResourceManager): The instance of the class.\n",
        "*   **Processes:**\n",
        "    1.  This method runs in a continuous loop as long as the `monitoring_active` flag is `True`.\n",
        "    2.  In each iteration, it calls `_collect_resource_stats` to get current system usage.\n",
        "    3.  The collected stats are appended to the `resource_history` list, which is periodically trimmed to prevent unbounded memory growth.\n",
        "    4.  It calls `_should_trigger_cleanup` to check if resource usage has exceeded the configured thresholds.\n",
        "    5.  If cleanup is needed, it spawns another thread to execute `_perform_cleanup`, ensuring the monitoring loop itself is not blocked.\n",
        "    6.  It sleeps for the duration of `monitoring_interval`.\n",
        "*   **Outputs:**\n",
        "    *   None. This is a perpetual loop that modifies the object's state and may trigger cleanup actions.\n",
        "\n",
        "**7. `DefaultFeatureDetectorFactory.create`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (DefaultFeatureDetectorFactory): The instance of the class.\n",
        "    *   A series of keyword arguments corresponding to the parameters of `cv2.ORB_create` (e.g., `nfeatures`, `scaleFactor`).\n",
        "    *   `optimize_parameters` (bool): Flag to trigger automatic parameter optimization.\n",
        "    *   `target_performance` (Optional[str]): A hint for the optimizer (e.g., 'speed', 'accuracy').\n",
        "*   **Processes:**\n",
        "    1.  It first validates the provided parameters against the mathematical constraints defined in `self.parameter_constraints`.\n",
        "    2.  If `optimize_parameters` is true, it invokes its `ParameterOptimizer` instance. The optimizer uses the `_evaluate_detector_performance` method as its objective function to find the best parameter set, which then overwrites the initial inputs.\n",
        "    3.  It generates a unique `cache_key` from the final parameter configuration.\n",
        "    4.  It checks a thread-safe cache (`_detector_cache`) for a pre-existing `cv2.ORB` instance with the same configuration. If found, it returns the cached object.\n",
        "    5.  If not in the cache, it performs final mathematical validation on critical parameters (e.g., `scaleFactor > 1.0`).\n",
        "    6.  It calls `cv2.ORB_create` with the final parameters to instantiate the detector.\n",
        "    7.  If the cache is full, it implements an LRU (Least Recently Used) policy to evict an old detector before adding the new one.\n",
        "    8.  The newly created detector is stored in the cache.\n",
        "*   **Outputs:**\n",
        "    *   A `cv2.ORB` object, which is a feature detector instance from the OpenCV library, configured with either the provided or optimized parameters.\n",
        "\n",
        "**8. `DefaultMatcherFactory.create`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (DefaultMatcherFactory): The instance of the class.\n",
        "    *   `normType` (int): The distance norm to use (e.g., `cv2.NORM_HAMMING`).\n",
        "    *   `crossCheck` (bool): A flag to enable/disable cross-checking for more reliable matches.\n",
        "    *   `optimize_for_throughput` (bool): A flag to adjust settings for speed.\n",
        "*   **Processes:**\n",
        "    1.  It validates that the `normType` is a valid OpenCV norm constant.\n",
        "    2.  If `optimize_for_throughput` is true, it overrides `crossCheck` to `False`, as this is a primary trade-off between speed and precision.\n",
        "    3.  It calls `cv2.BFMatcher` with the specified `normType` and `crossCheck` to instantiate the matcher.\n",
        "    4.  If profiling is enabled, it records the creation time and memory usage and appends this data to its `performance_history`.\n",
        "*   **Outputs:**\n",
        "    *   A `cv2.BFMatcher` object, which is a brute-force matcher instance from the OpenCV library.\n",
        "\n",
        "**9. `DefaultClipModelLoader.__call__`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (DefaultClipModelLoader): The instance of the class.\n",
        "    *   `model_name` (str): The name of the CLIP model to load (e.g., \"ViT-B/32\").\n",
        "    *   `device` (str): The target device for the model (e.g., \"cuda\").\n",
        "    *   `enable_optimization` (bool): Flag to apply post-loading optimizations.\n",
        "    *   `precision` (str): The desired numerical precision (e.g., \"float16\").\n",
        "*   **Processes:**\n",
        "    1.  It validates the `model_name` against a dictionary of supported models and the `device` string against available hardware.\n",
        "    2.  It generates a unique `cache_key` based on the configuration.\n",
        "    3.  It checks an in-memory cache (`_model_cache`) for a pre-loaded model. If found, it's returned immediately.\n",
        "    4.  If not cached, it calls the underlying `clip.load` function to download or load the model from a file system cache.\n",
        "    5.  If `enable_optimization` is true, it calls `_optimize_model`, which may apply `torch.compile` or other backend optimizations.\n",
        "    6.  It converts the model to the specified `precision` (e.g., `.half()` for float16).\n",
        "    7.  It records detailed performance metrics of the loading process (time, memory usage) in its `loading_history`.\n",
        "    8.  The loaded model and its associated preprocessing function are stored in the in-memory cache.\n",
        "*   **Outputs:**\n",
        "    *   A `Tuple[torch.nn.Module, Callable]`: The loaded and configured PyTorch model and the corresponding function required to preprocess images for it.\n",
        "\n",
        "--\n",
        "\n",
        "### **Result Data Structures**\n",
        "\n",
        "This cell defines the data classes used to structure the outputs of the various similarity methods, incorporating validation, serialization, and comparison capabilities through mixins.\n",
        "\n",
        "**1. `StatisticalProperties.__post_init__`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (StatisticalProperties): The instance of the class, just after its fields have been populated by the `__init__` method.\n",
        "*   **Processes:**\n",
        "    1.  This special dataclass method is called automatically after initialization.\n",
        "    2.  It performs a series of mathematical consistency checks on the statistical fields.\n",
        "    3.  It validates that `sample_size` is positive and `variance` is non-negative.\n",
        "    4.  It verifies that `standard_deviation` is approximately the square root of `variance`.\n",
        "    5.  It checks that `standard_error` is consistent with the formula `σ/√n`.\n",
        "    6.  It ensures `confidence_level` and `p_value` are within their valid probability ranges, [0, 1].\n",
        "*   **Outputs:**\n",
        "    *   None. It raises a `ValueError` if any of the mathematical constraints are violated, preventing the creation of a statistically inconsistent object.\n",
        "\n",
        "**2. `ReverseImageSearchResult.__post_init__`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ReverseImageSearchResult): The instance of the class.\n",
        "*   **Processes:**\n",
        "    1.  This method validates the integrity of the data returned from a reverse image search.\n",
        "    2.  It checks that required string fields (`best_guess`, `source_page_title`) are non-empty and within length constraints.\n",
        "    3.  It validates that optional numerical scores (`site_authority_score`, `confidence_score`) are within the range [0, 1].\n",
        "    4.  It iterates through the `similar_image_urls` list, ensuring each entry is a validly formatted URL string.\n",
        "    5.  It performs logical cross-field validation, such as ensuring `duplicate_url_count` does not exceed the total number of URLs.\n",
        "*   **Outputs:**\n",
        "    *   None. It raises a `ValueError` if any constraints are violated.\n",
        "\n",
        "**3. `FeatureMatchResult.__post_init__`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (FeatureMatchResult): The instance of the class.\n",
        "*   **Processes:**\n",
        "    1.  This method validates the integrity of the data from a feature matching operation.\n",
        "    2.  It ensures all numerical scores and ratios (`similarity_ratio`, `confidence_level`, `homography_inlier_ratio`) are within their expected mathematical range of [0, 1].\n",
        "    3.  It validates that all count fields (`total_matches`, `good_matches`, etc.) are non-negative.\n",
        "    4.  It performs logical cross-field validation, such as ensuring `good_matches` is not greater than `total_matches`.\n",
        "    5.  It validates the `normalization_strategy` string against the set of allowed values.\n",
        "*   **Outputs:**\n",
        "    *   None. It raises a `ValueError` if any constraints are violated.\n",
        "\n",
        "**4. `SerializationMixin.to_dict`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (Any class instance using this mixin).\n",
        "    *   `include_metadata` (bool): Flag to include serialization metadata.\n",
        "    *   `flatten_nested` (bool): Flag to flatten the dictionary structure.\n",
        "*   **Processes:**\n",
        "    1.  It uses the `dataclasses.asdict` function to recursively convert the dataclass instance into a dictionary.\n",
        "    2.  If `include_metadata` is true, it computes a hash of the object's content and adds a `_metadata` key to the dictionary with the class name, timestamp, version, and hash.\n",
        "    3.  If `flatten_nested` is true, it calls `_flatten_dictionary` to transform the nested dictionary into a single-level dictionary with dot-separated keys (e.g., `{'a': {'b': 1}}` becomes `{'a.b': 1}`).\n",
        "*   **Outputs:**\n",
        "    *   A `Dict[str, Any]` representing the object.\n",
        "\n",
        "**5. `ComparisonMixin.compute_similarity`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (Any class instance using this mixin).\n",
        "    *   `other` (ComparisonMixin): Another result object to compare against.\n",
        "    *   `method` (str): The name of the similarity metric to use (e.g., \"cosine\").\n",
        "*   **Processes:**\n",
        "    1.  It calls `_extract_numerical_features` on both `self` and `other` to get vectorized representations of the objects.\n",
        "    2.  It converts these feature lists into NumPy arrays.\n",
        "    3.  Based on the `method` string, it applies the corresponding mathematical formula:\n",
        "        *   **cosine:** Computes the dot product and divides by the product of the L2 norms.\n",
        "        *   **euclidean:** Computes the L2 norm of the vector difference and transforms it into a similarity score via `1 / (1 + distance)`.\n",
        "        *   **manhattan:** Computes the L1 norm of the vector difference and transforms it similarly.\n",
        "        *   **jaccard:** Computes the ratio of the intersection to the union of the feature sets.\n",
        "*   **Outputs:**\n",
        "    *   A `float` representing the similarity score between the two result objects.\n",
        "\n",
        "--\n",
        "\n",
        "### **Main `ImageSimilarityDetector` Class**\n",
        "\n",
        "This is the primary orchestrator class. The notebook shows two versions; the final one in Cell 9 is the most complete and will be the focus of this analysis.\n",
        "\n",
        "**1. `ImageSimilarityDetector.__init__` (Final Version)**\n",
        "*   **Inputs:**\n",
        "    *   A series of optional, injectable components (`orb_detector`, `matcher`, `clip_model_loader`).\n",
        "    *   A series of configuration parameters (`device`, `clip_model_name`, `resource_constraints`, etc.).\n",
        "*   **Processes:**\n",
        "    1.  It stores the high-level configuration parameters.\n",
        "    2.  It instantiates its own `ResourceManager` and starts monitoring if enabled.\n",
        "    3.  It sets the global validation policy on the result data classes.\n",
        "    4.  It calls `_determine_optimal_device_with_validation` to select and validate the computational device.\n",
        "    5.  It calls the factory-based initialization methods (`_initialize_feature_detector_with_factory`, etc.) to either accept the injected dependencies (after protocol validation) or create default, optimized instances.\n",
        "    6.  It initializes placeholders for the lazily-loaded CLIP model and thread-safe locks.\n",
        "    7.  It sets up a structured logger with key configuration details.\n",
        "*   **Outputs:**\n",
        "    *   A fully configured, enterprise-grade `ImageSimilarityDetector` instance ready for use.\n",
        "\n",
        "**2. `ImageSimilarityDetector._validate_image_path` (Final Version)**\n",
        "*   **Inputs:**\n",
        "    *   `image_path` (Union[str, Path]): The path to the image file.\n",
        "    *   `allow_symlinks` (bool): Flag to control symlink policy.\n",
        "    *   `perform_content_validation` (bool): Flag to control whether to read the file to verify it's a valid image.\n",
        "    *   `max_file_size_mb` (float): The maximum permissible file size.\n",
        "*   **Processes:**\n",
        "    1.  **Normalization:** Transforms the input into a resolved, absolute `pathlib.Path` object.\n",
        "    2.  **Existence Check:** Verifies `path.exists()`. If not, it raises a detailed `ImageNotFoundError` that includes a list of similarly named files in the parent directory for diagnostic purposes.\n",
        "    3.  **Symlink Analysis:** If the path is a symlink, it checks the `allow_symlinks` policy. If disallowed, it raises `SymlinkNotAllowedError` with the symlink resolution chain. If allowed, it verifies the target exists.\n",
        "    4.  **File Type Check:** Verifies `path.is_file()`. If not, it determines the actual file system object type (e.g., 'directory', 'socket') and raises `NotAFileError` with this information.\n",
        "    5.  **Permission Check:** Uses `os.access` to verify read permissions. If denied, it raises `PermissionDeniedError` with an analysis of the file's ownership and mode.\n",
        "    6.  **Size Check:** Checks the file size against `max_file_size_mb` and raises `ImageValidationError` if it's too large.\n",
        "    7.  **Content Validation:** If `perform_content_validation` is true, it attempts to open the file with `PIL.Image` and run `img.verify()`. This transforms the file's byte stream into an in-memory image representation, confirming it is not corrupt. If this fails, it raises `ImageUnreadableError`.\n",
        "*   **Outputs:**\n",
        "    *   A validated `pathlib.Path` object, guaranteed to point to an accessible, non-corrupt image file that meets all policy constraints.\n",
        "\n",
        "**3. `ImageSimilarityDetector._load_clip_with_comprehensive_monitoring`**\n",
        "*   **Inputs:**\n",
        "    *   `self` (ImageSimilarityDetector): The instance of the class.\n",
        "*   **Processes:**\n",
        "    1.  **Double-Checked Locking:** It first checks if the model is loaded without acquiring a lock. If not, it acquires `_initialization_lock` and checks again to ensure thread safety and prevent redundant loading.\n",
        "    2.  **Performance-Monitored Loading:** It records timestamps and memory usage before starting.\n",
        "    3.  **Fallback Strategy:** It establishes a `device_fallback_chain` (e.g., `['cuda:0', 'cpu']`).\n",
        "    4.  **Retry Loop:** It enters a nested loop, iterating through retry attempts and then through the device fallback chain.\n",
        "    5.  **Model Loading:** Inside the loop, it calls the `self.clip_model_loader` to attempt loading the model onto the current device in the chain.\n",
        "    6.  **Functionality Test:** After a successful load, it performs a \"smoke test\" by passing a random tensor through the model to ensure it is functional and does not produce null embeddings.\n",
        "    7.  **Error Handling:** It includes specific `except` blocks for `OSError`, `RuntimeError`, and `torch.cuda.OutOfMemoryError`. On an OOM error, it automatically triggers `torch.cuda.empty_cache()` and attempts to fall back to the CPU.\n",
        "    8.  **State Update:** On success, it stores the loaded model, its preprocess function, and detailed performance metrics in `self._clip_loading_performance`. It also updates `self.device` if a fallback was used.\n",
        "    9.  **Comprehensive Failure:** If all retries on all fallback devices fail, it aggregates all the collected error information and raises a single, highly detailed `ModelLoadError`.\n",
        "*   **Outputs:**\n",
        "    *   None. This method modifies the state of the `ImageSimilarityDetector` instance by populating `self.clip_model` and `self.clip_preprocess`.\n",
        "\n",
        "**4. `ImageSimilarityDetector.perceptual_hash_difference` (Final Version)**\n",
        "*   **Inputs:**\n",
        "    *   `image1`, `image2`: Image data in various formats.\n",
        "    *   Configuration parameters (`hash_size`, `normalize`, `return_similarity`).\n",
        "    *   Analysis flags (`compute_confidence_interval`, `statistical_analysis`, `performance_monitoring`).\n",
        "*   **Processes:**\n",
        "    1.  **Validation:** It validates the input parameters (e.g., `hash_size` bounds, logical consistency of `return_similarity` and `normalize`).\n",
        "    2.  **Performance & Metadata Setup:** It initializes dictionaries to track performance and metadata throughout the execution.\n",
        "    3.  **Image Loading:** It calls the internal helper `_load_and_preprocess_image_with_validation` for both images. This helper handles path validation, loading, and conversion to a grayscale `PIL.Image` object, returning rich metadata about the process.\n",
        "    4.  **Hash Computation:** It calls `imagehash.phash` on the processed images. This function internally resizes the image, applies the 2D-DCT, extracts the low-frequency coefficients, and computes the median to generate the binary hash.\n",
        "    5.  **Distance Calculation:** It computes the Hamming distance between the two hash objects by simple subtraction, which is overloaded by the `imagehash` library to perform this operation.\n",
        "    6.  **Statistical Analysis (Optional):** If requested, it converts the hashes to binary arrays and computes a `StatisticalProperties` object on the bit-wise differences. It can further compute a Wilson score confidence interval for the proportion of differing bits.\n",
        "    7.  **Normalization/Similarity (Optional):** Based on the input flags, it transforms the raw Hamming distance into a normalized distance (`d_H / N²`) or a similarity score (`1 - d_norm`).\n",
        "    8.  **Performance Recording:** It calculates total execution time and memory delta, and if monitoring is enabled, appends the collected `operation_metrics` to the instance's history.\n",
        "*   **Outputs:**\n",
        "    *   The output type depends on the input flags. It can be an `int` (raw Hamming distance), a `float` (normalized distance or similarity), or a `Dict[str, Any]` containing a comprehensive breakdown of the results, including all statistical and performance metrics.\n",
        "\n",
        "**5. `ImageSimilarityDetector.feature_match_ratio` (Final Version)**\n",
        "*   **Inputs:**\n",
        "    *   `image1`, `image2`: Image data.\n",
        "    *   Configuration parameters for matching (`distance_threshold`, `normalization_strategy`, `apply_ratio_test`, etc.).\n",
        "    *   Analysis flags (`return_detailed_result`, `geometric_verification`, etc.).\n",
        "*   **Processes:**\n",
        "    1.  **Validation & Setup:** It validates all input parameters and sets up a dictionary for performance tracking.\n",
        "    2.  **Image Preparation:** It calls `_load_and_prepare_image_for_features` for both images, which handles loading, optional resizing, and conversion to a grayscale NumPy array suitable for OpenCV.\n",
        "    3.  **Feature Detection:** It calls `self.orb_detector.detectAndCompute` on both grayscale images. This transforms the pixel data into two sets of keypoints and their corresponding 256-bit ORB descriptors.\n",
        "    4.  **Matching:**\n",
        "        *   If `apply_ratio_test` is true, it uses `self.bf_matcher.knnMatch` with `k=2` to find the two nearest neighbors for each descriptor. It then filters these matches based on Lowe's ratio test (`d1/d2 < ratio_threshold`).\n",
        "        *   Otherwise, it uses `self.bf_matcher.match` and filters the results by keeping only those whose Hamming distance is below `distance_threshold`.\n",
        "    5.  **Similarity Ratio Calculation:** It computes the final ratio by dividing the number of `good_matches` by a denominator determined by the `normalization_strategy` (`total_matches` or `min(keypoints1, keypoints2)`).\n",
        "    6.  **Geometric Verification (Optional):** If requested and there are enough good matches (>=4), it extracts the coordinates of the matched keypoints. It then uses `cv2.findHomography` with RANSAC to find a perspective transform between the images. The number of inlier matches (those that fit the geometric model) is counted.\n",
        "    7.  **Statistical Analysis (Optional):** If requested, it creates a `StatisticalProperties` object from the list of Hamming distances of the good matches, providing mean, variance, etc.\n",
        "    8.  **Result Aggregation:** It aggregates all computed metrics—similarity, counts, timings, geometric verification results, and statistical properties—into a `FeatureMatchResult` dataclass instance.\n",
        "*   **Outputs:**\n",
        "    *   If `return_detailed_result` is true, it returns the populated `FeatureMatchResult` object. Otherwise, it returns only the calculated `similarity_ratio` as a `float`.\n",
        "\n",
        "**6. `ImageSimilarityDetector.histogram_correlation` (Final Version)**\n",
        "*   **Inputs:**\n",
        "    *   `image1`, `image2`: Image data.\n",
        "    *   Configuration parameters (`bins`, `metric`, `color_space`, `adaptive_binning`, etc.).\n",
        "    *   Analysis flags (`statistical_analysis`, `performance_monitoring`, `entropy_analysis`).\n",
        "*   **Processes:**\n",
        "    1.  **Validation & Setup:** It validates all input parameters and sets up performance tracking.\n",
        "    2.  **Image Preparation:** It calls `_load_and_prepare_image_for_histogram`, which loads, resizes, and handles optional masks for both images.\n",
        "    3.  **Color Space Conversion:** It transforms the images from the default BGR color space to the specified `color_space` (HSV, RGB, or LAB) using `cv2.cvtColor`.\n",
        "    4.  **Adaptive Binning (Optional):** If enabled, it analyzes the pixel distribution of each image channel to determine an optimal number of bins using the Freedman-Diaconis rule, overriding the static `bins` parameter.\n",
        "    5.  **Histogram Calculation:** It calls `cv2.calcHist` for both images to compute their multi-dimensional color histograms based on the selected channels, bin counts, and value ranges.\n",
        "    6.  **Normalization:** It normalizes both histograms using `cv2.normalize` with `NORM_L1`, transforming them into probability distributions where the sum of bins is 1.\n",
        "    7.  **Entropy Analysis (Optional):** If requested, it computes the Shannon entropy of each histogram and the mutual information between them, providing an information-theoretic measure of similarity.\n",
        "    8.  **Comparison:** It calls `cv2.compareHist` with the two normalized histograms and the chosen `metric` (e.g., `cv2.HISTCMP_CORREL`). This single call performs the core statistical comparison.\n",
        "    9.  **Statistical Analysis (Optional):** If requested, it computes a `StatisticalProperties` object on the element-wise difference between the two normalized histograms and can perform a bootstrap analysis to estimate a confidence interval for the comparison metric.\n",
        "*   **Outputs:**\n",
        "    *   If `statistical_analysis` is true, it returns a `Dict[str, Any]` containing the comparison score and all the computed analytical and performance data. Otherwise, it returns the raw comparison score as a `float`.\n",
        "\n",
        "**7. `ImageSimilarityDetector.clip_embedding_similarity` (Final Version)**\n",
        "*   **Inputs:**\n",
        "    *   `image1`, `image2`: Image data.\n",
        "    *   Configuration parameters (`use_mixed_precision`, `batch_processing`, `device_optimization`).\n",
        "    *   Analysis flags (`statistical_analysis`, `performance_monitoring`, `embedding_analysis`).\n",
        "*   **Processes:**\n",
        "    1.  **Model Loading:** It ensures the CLIP model is loaded by calling `_load_clip_with_comprehensive_monitoring`.\n",
        "    2.  **Input Preparation:** It calls `_prepare_input_tensor_with_validation` for both images. This helper handles any input format (path, PIL, numpy, tensor), applies the necessary CLIP preprocessing, and moves the resulting tensor to the correct device.\n",
        "    3.  **Inference:** It passes the tensors through `self.clip_model.encode_image`. This is the core deep learning step. It can run in a batch for efficiency and can use automatic mixed precision (`torch.cuda.amp.autocast`) to speed up computation on compatible GPUs. It includes a specific fallback to CPU if a `torch.cuda.OutOfMemoryError` occurs.\n",
        "    4.  **Embedding Analysis (Optional):** If requested, it computes statistics (norm, mean, std) on the raw embedding vectors before normalization.\n",
        "    5.  **Normalization:** It L2-normalizes both embedding vectors, projecting them onto the unit hypersphere. This is a mathematical prerequisite for using the dot product to calculate cosine similarity.\n",
        "    6.  **Similarity Calculation:** It computes the cosine similarity by performing a matrix multiplication (`torch.matmul`) of one normalized embedding with the transpose of the other.\n",
        "    7.  **Statistical Analysis (Optional):** If requested, it calculates related metrics like angular distance and Euclidean distance in the normalized space. It also uses the Fisher z-transformation to compute a confidence interval for the cosine similarity score.\n",
        "*   **Outputs:**\n",
        "    *   If analysis is requested, it returns a `Dict[str, Any]` containing the cosine similarity and a rich set of embedding, statistical, and performance metrics. Otherwise, it returns the cosine similarity score as a `float`.\n",
        "\n",
        "**8. `ImageSimilarityDetector.reverse_image_search_google` (Final Version)**\n",
        "*   **Inputs:**\n",
        "    *   `image_path`, `driver_path`: Paths to the image and the WebDriver.\n",
        "    *   Configuration parameters (`timeout`, `headless`, `max_similar_urls`, `retry_attempts`).\n",
        "    *   Analysis flags (`performance_monitoring`, `content_analysis`, `advanced_extraction`).\n",
        "*   **Processes:**\n",
        "    1.  **Validation & Setup:** It validates the image and driver paths and configures `webdriver.ChromeOptions` with a suite of arguments designed for stable, undetectable automation. It also sets up performance tracking.\n",
        "    2.  **Driver Initialization:** It initializes the Selenium WebDriver, wrapping it in a retry loop to handle transient launch failures.\n",
        "    3.  **Navigation:** It navigates to `images.google.com`, again using a retry loop.\n",
        "    4.  **UI Interaction:** This is the most complex part. It uses a predefined list of CSS and XPath selectors for each UI element (`camera_button`, `upload_tab`, `file_input`). It iterates through this list, attempting to find and interact with the element until one selector succeeds. This provides robustness against changes in Google's front-end code. It also uses multiple click strategies (standard, JavaScript, ActionChains) for reliability.\n",
        "    5.  **File Upload:** It uses the `send_keys` method on the located file input element to upload the image.\n",
        "    6.  **Data Extraction:** After the results page loads, it uses a similar multi-selector strategy to extract the \"best guess\" text and a list of URLs for visually similar images.\n",
        "    7.  **Content Analysis (Optional):** If requested, it analyzes the quality of the extracted text and the distribution and quality of the extracted URLs to compute a confidence score.\n",
        "    8.  **Result Aggregation:** It populates a `ReverseImageSearchResult` dataclass with all the extracted and analyzed data.\n",
        "    9.  **Cleanup:** Crucially, it uses a `finally` block to ensure `driver.quit()` is always called, closing the browser and freeing up system resources, even if the automation fails.\n",
        "*   **Outputs:**\n",
        "    *   A `ReverseImageSearchResult` object containing the structured data extracted from the reverse image search, including the best guess, similar URLs, and analytical scores.\n",
        "\n",
        "This concludes the granular analysis of the callables within the provided notebook."
      ],
      "metadata": {
        "id": "TFjuM4_YjtB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Error Classes and Protocol Classes\n",
        "\n",
        "class ErrorSeverity(Enum):\n",
        "    \"\"\"Enumeration of error severity levels for systematic classification.\"\"\"\n",
        "    LOW = \"low\"\n",
        "    MEDIUM = \"medium\"\n",
        "    HIGH = \"high\"\n",
        "    CRITICAL = \"critical\"\n",
        "\n",
        "\n",
        "class ErrorContext(Protocol):\n",
        "    \"\"\"Protocol defining interface for error context serialization.\"\"\"\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Serialize error context to dictionary for forensic analysis.\"\"\"\n",
        "        ...\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        \"\"\"Serialize error context to JSON for logging and debugging.\"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "class ForensicMetadata:\n",
        "    \"\"\"\n",
        "    Comprehensive forensic metadata container for debugging complex pipelines.\n",
        "\n",
        "    Captures execution context, system state, and algorithmic parameters\n",
        "    at the point of error occurrence for systematic analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        operation_name: str,\n",
        "        thread_id: Optional[int] = None,\n",
        "        process_id: Optional[int] = None,\n",
        "        timestamp: Optional[datetime.datetime] = None,\n",
        "        system_info: Optional[Dict[str, Any]] = None,\n",
        "        algorithm_parameters: Optional[Dict[str, Any]] = None,\n",
        "        memory_usage: Optional[Dict[str, float]] = None,\n",
        "        call_stack: Optional[List[str]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize forensic metadata with comprehensive execution context.\n",
        "\n",
        "        Args:\n",
        "            operation_name: Name of operation that generated the error\n",
        "            thread_id: Thread identifier for concurrent execution debugging\n",
        "            process_id: Process identifier for multi-process debugging\n",
        "            timestamp: Precise timestamp of error occurrence\n",
        "            system_info: System resource state at error time\n",
        "            algorithm_parameters: Algorithm-specific parameters at failure\n",
        "            memory_usage: Memory consumption metrics at failure point\n",
        "            call_stack: Complete call stack trace for debugging\n",
        "        \"\"\"\n",
        "        # Store primary operation identifier for error classification\n",
        "        self.operation_name: str = operation_name\n",
        "\n",
        "        # Capture execution context identifiers for concurrent debugging\n",
        "        self.thread_id: int = thread_id or threading.get_ident()\n",
        "        self.process_id: int = process_id or sys.getpid()\n",
        "\n",
        "        # Record precise timestamp for temporal correlation analysis\n",
        "        self.timestamp: datetime.datetime = timestamp or datetime.datetime.utcnow()\n",
        "\n",
        "        # Store system resource state for resource-related error analysis\n",
        "        self.system_info: Dict[str, Any] = system_info or {}\n",
        "\n",
        "        # Preserve algorithm parameters for mathematical error analysis\n",
        "        self.algorithm_parameters: Dict[str, Any] = algorithm_parameters or {}\n",
        "\n",
        "        # Capture memory metrics for resource exhaustion debugging\n",
        "        self.memory_usage: Dict[str, float] = memory_usage or {}\n",
        "\n",
        "        # Store complete call stack for execution path reconstruction\n",
        "        self.call_stack: List[str] = call_stack or traceback.format_stack()\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Serialize forensic metadata to dictionary for structured analysis.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all forensic metadata fields\n",
        "        \"\"\"\n",
        "        # Convert timestamp to ISO format for JSON compatibility\n",
        "        return {\n",
        "            \"operation_name\": self.operation_name,\n",
        "            \"thread_id\": self.thread_id,\n",
        "            \"process_id\": self.process_id,\n",
        "            \"timestamp\": self.timestamp.isoformat(),\n",
        "            \"system_info\": self.system_info,\n",
        "            \"algorithm_parameters\": self.algorithm_parameters,\n",
        "            \"memory_usage\": self.memory_usage,\n",
        "            \"call_stack\": self.call_stack\n",
        "        }\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        \"\"\"\n",
        "        Serialize forensic metadata to JSON for logging systems.\n",
        "\n",
        "        Returns:\n",
        "            JSON string representation of forensic metadata\n",
        "        \"\"\"\n",
        "        # Convert to dictionary then serialize with indentation for readability\n",
        "        return json.dumps(self.to_dict(), indent=2, default=str)\n",
        "\n",
        "\n",
        "class ImageSimilarityError(Exception):\n",
        "    \"\"\"\n",
        "    Base exception class for all image similarity detection errors.\n",
        "\n",
        "    Implements comprehensive error context capture, forensic metadata collection,\n",
        "    and structured error reporting for production debugging and analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        severity: ErrorSeverity = ErrorSeverity.MEDIUM,\n",
        "        operation_name: Optional[str] = None,\n",
        "        error_code: Optional[str] = None,\n",
        "        forensic_metadata: Optional[ForensicMetadata] = None,\n",
        "        algorithm_context: Optional[Dict[str, Any]] = None,\n",
        "        original_exception: Optional[Exception] = None,\n",
        "        suggested_remediation: Optional[str] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize base image similarity error with comprehensive context.\n",
        "\n",
        "        Args:\n",
        "            message: Human-readable error description\n",
        "            severity: Error severity level for prioritization\n",
        "            operation_name: Name of operation that generated error\n",
        "            error_code: Unique error code for systematic classification\n",
        "            forensic_metadata: Comprehensive debugging metadata\n",
        "            algorithm_context: Algorithm-specific context and parameters\n",
        "            original_exception: Original exception if this is a wrapper\n",
        "            suggested_remediation: Actionable remediation suggestions\n",
        "        \"\"\"\n",
        "        # Validate message parameter to prevent None values\n",
        "        if message is None or not isinstance(message, str):\n",
        "            message = \"Unknown image similarity error occurred\"\n",
        "\n",
        "        # Initialize base Exception class with sanitized message\n",
        "        super().__init__(message)\n",
        "\n",
        "        # Store primary error message with null safety\n",
        "        self.message: str = message\n",
        "\n",
        "        # Classify error severity for systematic handling\n",
        "        self.severity: ErrorSeverity = severity\n",
        "\n",
        "        # Store operation context for debugging pipeline reconstruction\n",
        "        self.operation_name: str = operation_name or \"unknown_operation\"\n",
        "\n",
        "        # Assign unique error code for systematic error tracking\n",
        "        self.error_code: str = error_code or f\"{self.__class__.__name__}_{id(self)}\"\n",
        "\n",
        "        # Capture or create forensic metadata for debugging\n",
        "        self.forensic_metadata: ForensicMetadata = forensic_metadata or ForensicMetadata(\n",
        "            operation_name=self.operation_name\n",
        "        )\n",
        "\n",
        "        # Store algorithm-specific context for mathematical error analysis\n",
        "        self.algorithm_context: Dict[str, Any] = algorithm_context or {}\n",
        "\n",
        "        # Preserve original exception for exception chaining analysis\n",
        "        self.original_exception: Optional[Exception] = original_exception\n",
        "\n",
        "        # Store actionable remediation guidance for error recovery\n",
        "        self.suggested_remediation: str = suggested_remediation or \"Contact system administrator\"\n",
        "\n",
        "        # Record precise error occurrence time for temporal analysis\n",
        "        self.timestamp: datetime.datetime = datetime.datetime.utcnow()\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"\n",
        "        Generate human-readable string representation for error display.\n",
        "\n",
        "        Returns:\n",
        "            Formatted error message with context and remediation\n",
        "        \"\"\"\n",
        "        # Construct comprehensive error description with context\n",
        "        error_description = f\"[{self.severity.value.upper()}] {self.message}\"\n",
        "\n",
        "        # Add operation context if available for debugging\n",
        "        if self.operation_name != \"unknown_operation\":\n",
        "            error_description += f\" (Operation: {self.operation_name})\"\n",
        "\n",
        "        # Include error code for systematic tracking\n",
        "        error_description += f\" [Code: {self.error_code}]\"\n",
        "\n",
        "        # Append remediation guidance for actionable response\n",
        "        if self.suggested_remediation:\n",
        "            error_description += f\" | Suggested fix: {self.suggested_remediation}\"\n",
        "\n",
        "        return error_description\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"\n",
        "        Generate detailed string representation for debugging and logging.\n",
        "\n",
        "        Returns:\n",
        "            Detailed error representation with all context information\n",
        "        \"\"\"\n",
        "        # Create comprehensive representation for debugging purposes\n",
        "        return (\n",
        "            f\"{self.__class__.__name__}(\"\n",
        "            f\"message='{self.message}', \"\n",
        "            f\"severity={self.severity}, \"\n",
        "            f\"operation_name='{self.operation_name}', \"\n",
        "            f\"error_code='{self.error_code}', \"\n",
        "            f\"timestamp='{self.timestamp.isoformat()}', \"\n",
        "            f\"has_forensic_metadata={self.forensic_metadata is not None}, \"\n",
        "            f\"has_algorithm_context={bool(self.algorithm_context)}, \"\n",
        "            f\"has_original_exception={self.original_exception is not None}\"\n",
        "            f\")\"\n",
        "        )\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Serialize exception to dictionary for structured error analysis.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all error information and context\n",
        "        \"\"\"\n",
        "        # Construct comprehensive error dictionary for serialization\n",
        "        error_dict = {\n",
        "            \"error_class\": self.__class__.__name__,\n",
        "            \"message\": self.message,\n",
        "            \"severity\": self.severity.value,\n",
        "            \"operation_name\": self.operation_name,\n",
        "            \"error_code\": self.error_code,\n",
        "            \"timestamp\": self.timestamp.isoformat(),\n",
        "            \"algorithm_context\": self.algorithm_context,\n",
        "            \"suggested_remediation\": self.suggested_remediation\n",
        "        }\n",
        "\n",
        "        # Include forensic metadata if available\n",
        "        if self.forensic_metadata:\n",
        "            error_dict[\"forensic_metadata\"] = self.forensic_metadata.to_dict()\n",
        "\n",
        "        # Include original exception details if present\n",
        "        if self.original_exception:\n",
        "            error_dict[\"original_exception\"] = {\n",
        "                \"type\": type(self.original_exception).__name__,\n",
        "                \"message\": str(self.original_exception),\n",
        "                \"args\": self.original_exception.args\n",
        "            }\n",
        "\n",
        "        return error_dict\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        \"\"\"\n",
        "        Serialize exception to JSON for logging and external systems.\n",
        "\n",
        "        Returns:\n",
        "            JSON string representation of complete error context\n",
        "        \"\"\"\n",
        "        # Convert to dictionary then serialize with proper formatting\n",
        "        return json.dumps(self.to_dict(), indent=2, default=str)\n",
        "\n",
        "    def log_error(self, logger: Optional[logging.Logger] = None) -> None:\n",
        "        \"\"\"\n",
        "        Log error using structured logging for systematic analysis.\n",
        "\n",
        "        Args:\n",
        "            logger: Optional logger instance, creates default if None\n",
        "        \"\"\"\n",
        "        # Create or use provided logger for error recording\n",
        "        error_logger = logger or logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "\n",
        "        # Log error with appropriate severity level mapping\n",
        "        log_level = {\n",
        "            ErrorSeverity.LOW: logging.INFO,\n",
        "            ErrorSeverity.MEDIUM: logging.WARNING,\n",
        "            ErrorSeverity.HIGH: logging.ERROR,\n",
        "            ErrorSeverity.CRITICAL: logging.CRITICAL\n",
        "        }.get(self.severity, logging.ERROR)\n",
        "\n",
        "        # Record structured error information for analysis\n",
        "        error_logger.log(\n",
        "            log_level,\n",
        "            \"Image similarity error occurred\",\n",
        "            extra={\n",
        "                \"error_data\": self.to_dict(),\n",
        "                \"error_class\": self.__class__.__name__,\n",
        "                \"operation_name\": self.operation_name,\n",
        "                \"error_code\": self.error_code\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "class ImageValidationError(ImageSimilarityError):\n",
        "    \"\"\"\n",
        "    Base exception for image validation failures with file system context.\n",
        "\n",
        "    Extends base error with file-specific forensic metadata including\n",
        "    path information, file attributes, and validation step details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        file_path: Optional[Union[str, Path]] = None,\n",
        "        validation_step: Optional[str] = None,\n",
        "        file_attributes: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize image validation error with file system context.\n",
        "\n",
        "        Args:\n",
        "            message: Descriptive error message\n",
        "            file_path: Path to file that failed validation\n",
        "            validation_step: Specific validation step that failed\n",
        "            file_attributes: File system attributes at validation time\n",
        "            **kwargs: Additional arguments passed to base class\n",
        "        \"\"\"\n",
        "        # Store file path with null safety and path normalization\n",
        "        self.file_path: Optional[Path] = Path(file_path) if file_path else None\n",
        "\n",
        "        # Record specific validation step for debugging workflow\n",
        "        self.validation_step: str = validation_step or \"unknown_validation\"\n",
        "\n",
        "        # Capture file attributes for forensic analysis\n",
        "        self.file_attributes: Dict[str, Any] = file_attributes or {}\n",
        "\n",
        "        # Enhance algorithm context with file validation information\n",
        "        algorithm_context = kwargs.get('algorithm_context', {})\n",
        "        algorithm_context.update({\n",
        "            \"file_path\": str(self.file_path) if self.file_path else None,\n",
        "            \"validation_step\": self.validation_step,\n",
        "            \"file_attributes\": self.file_attributes\n",
        "        })\n",
        "        kwargs['algorithm_context'] = algorithm_context\n",
        "\n",
        "        # Initialize base class with enhanced context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class ImageNotFoundError(ImageValidationError):\n",
        "    \"\"\"\n",
        "    Raised when specified image file does not exist in filesystem.\n",
        "\n",
        "    Implements file system state capture and path resolution debugging\n",
        "    for systematic analysis of file accessibility issues.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        file_path: Optional[Union[str, Path]] = None,\n",
        "        search_paths: Optional[List[Union[str, Path]]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize image not found error with path search context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            file_path: Primary file path that was not found\n",
        "            search_paths: Additional paths searched for file\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store attempted search paths for debugging path resolution\n",
        "        self.search_paths: List[Path] = [\n",
        "            Path(p) for p in (search_paths or [])\n",
        "        ]\n",
        "\n",
        "        # Set default severity to high for missing required files\n",
        "        kwargs.setdefault('severity', ErrorSeverity.HIGH)\n",
        "\n",
        "        # Set validation step context for this error type\n",
        "        kwargs.setdefault('validation_step', 'file_existence_check')\n",
        "\n",
        "        # Provide specific remediation guidance for file not found\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify file path exists and has correct permissions'\n",
        "        )\n",
        "\n",
        "        # Initialize validation error with file context\n",
        "        super().__init__(message, file_path=file_path, **kwargs)\n",
        "\n",
        "\n",
        "class NotAFileError(ImageValidationError):\n",
        "    \"\"\"\n",
        "    Raised when path exists but is not a regular file.\n",
        "\n",
        "    Captures file type information and inode details for debugging\n",
        "    symbolic links, directories, and special files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        file_path: Optional[Union[str, Path]] = None,\n",
        "        actual_file_type: Optional[str] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize not-a-file error with file type context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            file_path: Path that is not a regular file\n",
        "            actual_file_type: Actual type of file system object\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Record actual file type for debugging file system issues\n",
        "        self.actual_file_type: str = actual_file_type or \"unknown\"\n",
        "\n",
        "        # Set medium severity for file type mismatches\n",
        "        kwargs.setdefault('severity', ErrorSeverity.MEDIUM)\n",
        "\n",
        "        # Set validation step context\n",
        "        kwargs.setdefault('validation_step', 'file_type_verification')\n",
        "\n",
        "        # Provide specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            f'Path points to {self.actual_file_type}, expected regular file'\n",
        "        )\n",
        "\n",
        "        # Initialize validation error with type context\n",
        "        super().__init__(message, file_path=file_path, **kwargs)\n",
        "\n",
        "\n",
        "class SymlinkNotAllowedError(ImageValidationError):\n",
        "    \"\"\"\n",
        "    Raised when symlinks are encountered but not permitted by policy.\n",
        "\n",
        "    Includes symlink resolution chain analysis for security auditing\n",
        "    and circular reference detection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        file_path: Optional[Union[str, Path]] = None,\n",
        "        symlink_target: Optional[Union[str, Path]] = None,\n",
        "        resolution_chain: Optional[List[Union[str, Path]]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize symlink policy violation error with resolution context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            file_path: Symlink path that was rejected\n",
        "            symlink_target: Final target of symlink resolution\n",
        "            resolution_chain: Complete symlink resolution chain\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store symlink target for security analysis\n",
        "        self.symlink_target: Optional[Path] = Path(symlink_target) if symlink_target else None\n",
        "\n",
        "        # Store complete resolution chain for circular reference detection\n",
        "        self.resolution_chain: List[Path] = [\n",
        "            Path(p) for p in (resolution_chain or [])\n",
        "        ]\n",
        "\n",
        "        # Set medium severity for policy violations\n",
        "        kwargs.setdefault('severity', ErrorSeverity.MEDIUM)\n",
        "\n",
        "        # Set validation step context\n",
        "        kwargs.setdefault('validation_step', 'symlink_policy_enforcement')\n",
        "\n",
        "        # Provide specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Enable symlink support or use direct file path'\n",
        "        )\n",
        "\n",
        "        # Initialize validation error with symlink context\n",
        "        super().__init__(message, file_path=file_path, **kwargs)\n",
        "\n",
        "\n",
        "class ImageUnreadableError(ImageValidationError):\n",
        "    \"\"\"\n",
        "    Raised when image file exists but cannot be read or parsed.\n",
        "\n",
        "    Captures file corruption indicators, format validation results,\n",
        "    and binary header analysis for comprehensive debugging.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        file_path: Optional[Union[str, Path]] = None,\n",
        "        corruption_indicators: Optional[Dict[str, Any]] = None,\n",
        "        format_analysis: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize image unreadable error with corruption analysis.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            file_path: Path to unreadable image file\n",
        "            corruption_indicators: Detected corruption indicators\n",
        "            format_analysis: Image format validation results\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store corruption analysis for debugging file integrity\n",
        "        self.corruption_indicators: Dict[str, Any] = corruption_indicators or {}\n",
        "\n",
        "        # Store format analysis for debugging parsing failures\n",
        "        self.format_analysis: Dict[str, Any] = format_analysis or {}\n",
        "\n",
        "        # Set high severity for data corruption issues\n",
        "        kwargs.setdefault('severity', ErrorSeverity.HIGH)\n",
        "\n",
        "        # Set validation step context\n",
        "        kwargs.setdefault('validation_step', 'image_readability_check')\n",
        "\n",
        "        # Provide specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify file integrity and format compatibility'\n",
        "        )\n",
        "\n",
        "        # Initialize validation error with corruption context\n",
        "        super().__init__(message, file_path=file_path, **kwargs)\n",
        "\n",
        "\n",
        "class PermissionDeniedError(ImageValidationError):\n",
        "    \"\"\"\n",
        "    Raised when insufficient permissions to access image file.\n",
        "\n",
        "    Captures permission analysis, ownership information, and ACL details\n",
        "    for systematic permission debugging and security auditing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        file_path: Optional[Union[str, Path]] = None,\n",
        "        permission_analysis: Optional[Dict[str, Any]] = None,\n",
        "        required_permissions: Optional[List[str]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize permission error with access control context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            file_path: Path with permission issues\n",
        "            permission_analysis: Detailed permission analysis\n",
        "            required_permissions: List of required permissions\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store permission analysis for access control debugging\n",
        "        self.permission_analysis: Dict[str, Any] = permission_analysis or {}\n",
        "\n",
        "        # Store required permissions for remediation guidance\n",
        "        self.required_permissions: List[str] = required_permissions or [\"read\"]\n",
        "\n",
        "        # Set high severity for security-related access issues\n",
        "        kwargs.setdefault('severity', ErrorSeverity.HIGH)\n",
        "\n",
        "        # Set validation step context\n",
        "        kwargs.setdefault('validation_step', 'permission_verification')\n",
        "\n",
        "        # Provide specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            f'Grant {\", \".join(self.required_permissions)} permissions to file'\n",
        "        )\n",
        "\n",
        "        # Initialize validation error with permission context\n",
        "        super().__init__(message, file_path=file_path, **kwargs)\n",
        "\n",
        "\n",
        "class InitializationError(ImageSimilarityError):\n",
        "    \"\"\"\n",
        "    Base exception for component initialization failures.\n",
        "\n",
        "    Captures system resource state, dependency information, and\n",
        "    initialization sequence details for systematic debugging.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        component_name: Optional[str] = None,\n",
        "        initialization_stage: Optional[str] = None,\n",
        "        system_resources: Optional[Dict[str, Any]] = None,\n",
        "        dependency_info: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize component initialization error with system context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            component_name: Name of component that failed to initialize\n",
        "            initialization_stage: Specific initialization stage that failed\n",
        "            system_resources: System resource state at failure\n",
        "            dependency_info: Dependency availability and version information\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store component identification for debugging\n",
        "        self.component_name: str = component_name or \"unknown_component\"\n",
        "\n",
        "        # Record initialization stage for workflow debugging\n",
        "        self.initialization_stage: str = initialization_stage or \"unknown_stage\"\n",
        "\n",
        "        # Capture system resource state for resource-related failures\n",
        "        self.system_resources: Dict[str, Any] = system_resources or {}\n",
        "\n",
        "        # Store dependency information for version compatibility analysis\n",
        "        self.dependency_info: Dict[str, Any] = dependency_info or {}\n",
        "\n",
        "        # Set high severity for initialization failures\n",
        "        kwargs.setdefault('severity', ErrorSeverity.HIGH)\n",
        "\n",
        "        # Set operation context for initialization\n",
        "        kwargs.setdefault('operation_name', f'{self.component_name}_initialization')\n",
        "\n",
        "        # Initialize base class with initialization context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class OpenCVInitializationError(InitializationError):\n",
        "    \"\"\"\n",
        "    Raised when OpenCV components fail to initialize.\n",
        "\n",
        "    Captures OpenCV version information, build configuration,\n",
        "    and hardware compatibility details for debugging.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        opencv_version: Optional[str] = None,\n",
        "        build_info: Optional[Dict[str, Any]] = None,\n",
        "        hardware_compatibility: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize OpenCV error with library-specific context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            opencv_version: OpenCV library version\n",
        "            build_info: OpenCV build configuration details\n",
        "            hardware_compatibility: Hardware compatibility analysis\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store OpenCV version for compatibility debugging\n",
        "        self.opencv_version: Optional[str] = opencv_version\n",
        "\n",
        "        # Store build configuration for debugging compilation issues\n",
        "        self.build_info: Dict[str, Any] = build_info or {}\n",
        "\n",
        "        # Store hardware compatibility analysis\n",
        "        self.hardware_compatibility: Dict[str, Any] = hardware_compatibility or {}\n",
        "\n",
        "        # Set component name for OpenCV\n",
        "        kwargs.setdefault('component_name', 'opencv')\n",
        "\n",
        "        # Provide OpenCV-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify OpenCV installation and hardware compatibility'\n",
        "        )\n",
        "\n",
        "        # Initialize base initialization error\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class ResourceAllocationError(InitializationError):\n",
        "    \"\"\"\n",
        "    Raised when system resources cannot be allocated.\n",
        "\n",
        "    Captures memory usage, CPU utilization, and GPU availability\n",
        "    for comprehensive resource constraint analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        resource_type: Optional[str] = None,\n",
        "        requested_amount: Optional[Union[int, float]] = None,\n",
        "        available_amount: Optional[Union[int, float]] = None,\n",
        "        resource_metrics: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize resource allocation error with usage metrics.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            resource_type: Type of resource that failed allocation\n",
        "            requested_amount: Amount of resource requested\n",
        "            available_amount: Amount of resource available\n",
        "            resource_metrics: Comprehensive resource usage metrics\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store resource type for allocation debugging\n",
        "        self.resource_type: str = resource_type or \"unknown_resource\"\n",
        "\n",
        "        # Store resource amounts for capacity analysis\n",
        "        self.requested_amount: Optional[Union[int, float]] = requested_amount\n",
        "        self.available_amount: Optional[Union[int, float]] = available_amount\n",
        "\n",
        "        # Store comprehensive resource metrics\n",
        "        self.resource_metrics: Dict[str, Any] = resource_metrics or {}\n",
        "\n",
        "        # Set critical severity for resource exhaustion\n",
        "        kwargs.setdefault('severity', ErrorSeverity.CRITICAL)\n",
        "\n",
        "        # Set component name for resource allocation\n",
        "        kwargs.setdefault('component_name', f'{self.resource_type}_allocator')\n",
        "\n",
        "        # Provide resource-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            f'Increase available {self.resource_type} or reduce usage'\n",
        "        )\n",
        "\n",
        "        # Initialize base initialization error\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class ModelLoadError(ImageSimilarityError):\n",
        "    \"\"\"\n",
        "    Raised when machine learning models fail to load.\n",
        "\n",
        "    Captures model metadata, loading parameters, and hardware requirements\n",
        "    for systematic model deployment debugging.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        model_name: Optional[str] = None,\n",
        "        model_version: Optional[str] = None,\n",
        "        loading_parameters: Optional[Dict[str, Any]] = None,\n",
        "        hardware_requirements: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize model loading error with model-specific context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            model_name: Name of model that failed to load\n",
        "            model_version: Version of model that failed to load\n",
        "            loading_parameters: Parameters used for model loading\n",
        "            hardware_requirements: Hardware requirements for model\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store model identification for debugging\n",
        "        self.model_name: str = model_name or \"unknown_model\"\n",
        "        self.model_version: Optional[str] = model_version\n",
        "\n",
        "        # Store loading parameters for configuration debugging\n",
        "        self.loading_parameters: Dict[str, Any] = loading_parameters or {}\n",
        "\n",
        "        # Store hardware requirements for compatibility analysis\n",
        "        self.hardware_requirements: Dict[str, Any] = hardware_requirements or {}\n",
        "\n",
        "        # Set high severity for model loading failures\n",
        "        kwargs.setdefault('severity', ErrorSeverity.HIGH)\n",
        "\n",
        "        # Set operation context for model loading\n",
        "        kwargs.setdefault('operation_name', f'{self.model_name}_loading')\n",
        "\n",
        "        # Provide model-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify model availability and hardware compatibility'\n",
        "        )\n",
        "\n",
        "        # Initialize base class with model context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class ModelInferenceError(ImageSimilarityError):\n",
        "    \"\"\"\n",
        "    Raised when model inference operations fail.\n",
        "\n",
        "    Captures inference parameters, input characteristics, and performance metrics\n",
        "    for debugging inference pipeline failures.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        model_name: Optional[str] = None,\n",
        "        inference_parameters: Optional[Dict[str, Any]] = None,\n",
        "        input_characteristics: Optional[Dict[str, Any]] = None,\n",
        "        performance_metrics: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize model inference error with inference context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            model_name: Name of model during inference\n",
        "            inference_parameters: Parameters used for inference\n",
        "            input_characteristics: Characteristics of input data\n",
        "            performance_metrics: Performance metrics at failure\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store model identification for debugging\n",
        "        self.model_name: str = model_name or \"unknown_model\"\n",
        "\n",
        "        # Store inference parameters for configuration debugging\n",
        "        self.inference_parameters: Dict[str, Any] = inference_parameters or {}\n",
        "\n",
        "        # Store input characteristics for data validation debugging\n",
        "        self.input_characteristics: Dict[str, Any] = input_characteristics or {}\n",
        "\n",
        "        # Store performance metrics for optimization analysis\n",
        "        self.performance_metrics: Dict[str, Any] = performance_metrics or {}\n",
        "\n",
        "        # Set high severity for inference failures\n",
        "        kwargs.setdefault('severity', ErrorSeverity.HIGH)\n",
        "\n",
        "        # Set operation context for inference\n",
        "        kwargs.setdefault('operation_name', f'{self.model_name}_inference')\n",
        "\n",
        "        # Provide inference-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify input data format and model state'\n",
        "        )\n",
        "\n",
        "        # Initialize base class with inference context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class HistogramError(ImageSimilarityError):\n",
        "    \"\"\"\n",
        "    Raised when histogram computation or comparison fails.\n",
        "\n",
        "    Captures histogram parameters, statistical properties, and\n",
        "    mathematical constraints for systematic histogram analysis debugging.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        histogram_parameters: Optional[Dict[str, Any]] = None,\n",
        "        statistical_properties: Optional[Dict[str, Any]] = None,\n",
        "        mathematical_constraints: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize histogram error with statistical context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            histogram_parameters: Parameters used for histogram computation\n",
        "            statistical_properties: Statistical properties of histograms\n",
        "            mathematical_constraints: Mathematical constraints that were violated\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store histogram parameters for algorithm debugging\n",
        "        self.histogram_parameters: Dict[str, Any] = histogram_parameters or {}\n",
        "\n",
        "        # Store statistical properties for mathematical analysis\n",
        "        self.statistical_properties: Dict[str, Any] = statistical_properties or {}\n",
        "\n",
        "        # Store mathematical constraints for validation debugging\n",
        "        self.mathematical_constraints: Dict[str, Any] = mathematical_constraints or {}\n",
        "\n",
        "        # Set medium severity for histogram computation issues\n",
        "        kwargs.setdefault('severity', ErrorSeverity.MEDIUM)\n",
        "\n",
        "        # Set operation context for histogram operations\n",
        "        kwargs.setdefault('operation_name', 'histogram_computation')\n",
        "\n",
        "        # Provide histogram-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify histogram parameters and input data validity'\n",
        "        )\n",
        "\n",
        "        # Initialize base class with histogram context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class ReverseSearchError(ImageSimilarityError):\n",
        "    \"\"\"\n",
        "    Base exception for reverse image search failures.\n",
        "\n",
        "    Captures web automation context, browser state, and network conditions\n",
        "    for debugging web scraping and automation failures.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        browser_info: Optional[Dict[str, Any]] = None,\n",
        "        network_conditions: Optional[Dict[str, Any]] = None,\n",
        "        automation_state: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize reverse search error with web automation context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            browser_info: Browser version and configuration information\n",
        "            network_conditions: Network connectivity and performance metrics\n",
        "            automation_state: Web automation state at failure\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store browser information for compatibility debugging\n",
        "        self.browser_info: Dict[str, Any] = browser_info or {}\n",
        "\n",
        "        # Store network conditions for connectivity debugging\n",
        "        self.network_conditions: Dict[str, Any] = network_conditions or {}\n",
        "\n",
        "        # Store automation state for workflow debugging\n",
        "        self.automation_state: Dict[str, Any] = automation_state or {}\n",
        "\n",
        "        # Set medium severity for web automation issues\n",
        "        kwargs.setdefault('severity', ErrorSeverity.MEDIUM)\n",
        "\n",
        "        # Set operation context for reverse search\n",
        "        kwargs.setdefault('operation_name', 'reverse_image_search')\n",
        "\n",
        "        # Initialize base class with web automation context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class LaunchError(ReverseSearchError):\n",
        "    \"\"\"\n",
        "    Raised when browser launch fails.\n",
        "\n",
        "    Captures browser installation details, driver compatibility,\n",
        "    and system environment for debugging launch failures.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        driver_path: Optional[Union[str, Path]] = None,\n",
        "        browser_path: Optional[Union[str, Path]] = None,\n",
        "        driver_version: Optional[str] = None,\n",
        "        browser_version: Optional[str] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize browser launch error with driver context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            driver_path: Path to WebDriver executable\n",
        "            browser_path: Path to browser executable\n",
        "            driver_version: WebDriver version\n",
        "            browser_version: Browser version\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store driver and browser paths for installation debugging\n",
        "        self.driver_path: Optional[Path] = Path(driver_path) if driver_path else None\n",
        "        self.browser_path: Optional[Path] = Path(browser_path) if browser_path else None\n",
        "\n",
        "        # Store version information for compatibility debugging\n",
        "        self.driver_version: Optional[str] = driver_version\n",
        "        self.browser_version: Optional[str] = browser_version\n",
        "\n",
        "        # Set high severity for launch failures\n",
        "        kwargs.setdefault('severity', ErrorSeverity.HIGH)\n",
        "\n",
        "        # Set operation context for browser launch\n",
        "        kwargs.setdefault('operation_name', 'browser_launch')\n",
        "\n",
        "        # Provide launch-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify WebDriver and browser installation compatibility'\n",
        "        )\n",
        "\n",
        "        # Initialize reverse search error with launch context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class NavigationError(ReverseSearchError):\n",
        "    \"\"\"\n",
        "    Raised when page navigation fails.\n",
        "\n",
        "    Captures URL information, HTTP response codes, and page load metrics\n",
        "    for debugging navigation and connectivity issues.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        target_url: Optional[str] = None,\n",
        "        http_status_code: Optional[int] = None,\n",
        "        page_load_metrics: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize navigation error with page context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            target_url: URL that failed to load\n",
        "            http_status_code: HTTP response status code\n",
        "            page_load_metrics: Page loading performance metrics\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store target URL for navigation debugging\n",
        "        self.target_url: Optional[str] = target_url\n",
        "\n",
        "        # Store HTTP status for connectivity debugging\n",
        "        self.http_status_code: Optional[int] = http_status_code\n",
        "\n",
        "        # Store page load metrics for performance debugging\n",
        "        self.page_load_metrics: Dict[str, Any] = page_load_metrics or {}\n",
        "\n",
        "        # Set medium severity for navigation issues\n",
        "        kwargs.setdefault('severity', ErrorSeverity.MEDIUM)\n",
        "\n",
        "        # Set operation context for navigation\n",
        "        kwargs.setdefault('operation_name', 'page_navigation')\n",
        "\n",
        "        # Provide navigation-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify network connectivity and target URL accessibility'\n",
        "        )\n",
        "\n",
        "        # Initialize reverse search error with navigation context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class UploadError(ReverseSearchError):\n",
        "    \"\"\"\n",
        "    Raised when file upload fails.\n",
        "\n",
        "    Captures upload parameters, file characteristics, and browser state\n",
        "    for debugging file upload mechanism failures.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        file_path: Optional[Union[str, Path]] = None,\n",
        "        file_size: Optional[int] = None,\n",
        "        upload_parameters: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize upload error with file context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            file_path: Path to file that failed upload\n",
        "            file_size: Size of file in bytes\n",
        "            upload_parameters: Parameters used for upload attempt\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store file information for upload debugging\n",
        "        self.file_path: Optional[Path] = Path(file_path) if file_path else None\n",
        "        self.file_size: Optional[int] = file_size\n",
        "\n",
        "        # Store upload parameters for mechanism debugging\n",
        "        self.upload_parameters: Dict[str, Any] = upload_parameters or {}\n",
        "\n",
        "        # Set medium severity for upload issues\n",
        "        kwargs.setdefault('severity', ErrorSeverity.MEDIUM)\n",
        "\n",
        "        # Set operation context for file upload\n",
        "        kwargs.setdefault('operation_name', 'file_upload')\n",
        "\n",
        "        # Provide upload-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify file accessibility and upload mechanism compatibility'\n",
        "        )\n",
        "\n",
        "        # Initialize reverse search error with upload context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class ExtractionError(ReverseSearchError):\n",
        "    \"\"\"\n",
        "    Raised when data extraction from results fails.\n",
        "\n",
        "    Captures DOM state, extraction parameters, and page content\n",
        "    for debugging web scraping and data extraction failures.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        message: str,\n",
        "        extraction_target: Optional[str] = None,\n",
        "        dom_state: Optional[Dict[str, Any]] = None,\n",
        "        extraction_parameters: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize extraction error with DOM context.\n",
        "\n",
        "        Args:\n",
        "            message: Error description\n",
        "            extraction_target: Target element or data for extraction\n",
        "            dom_state: DOM state at extraction time\n",
        "            extraction_parameters: Parameters used for extraction\n",
        "            **kwargs: Additional arguments for base class\n",
        "        \"\"\"\n",
        "        # Store extraction target for debugging data location\n",
        "        self.extraction_target: Optional[str] = extraction_target\n",
        "\n",
        "        # Store DOM state for page structure debugging\n",
        "        self.dom_state: Dict[str, Any] = dom_state or {}\n",
        "\n",
        "        # Store extraction parameters for algorithm debugging\n",
        "        self.extraction_parameters: Dict[str, Any] = extraction_parameters or {}\n",
        "\n",
        "        # Set medium severity for extraction issues\n",
        "        kwargs.setdefault('severity', ErrorSeverity.MEDIUM)\n",
        "\n",
        "        # Set operation context for data extraction\n",
        "        kwargs.setdefault('operation_name', 'data_extraction')\n",
        "\n",
        "        # Provide extraction-specific remediation guidance\n",
        "        kwargs.setdefault(\n",
        "            'suggested_remediation',\n",
        "            'Verify page structure and extraction selector validity'\n",
        "        )\n",
        "\n",
        "        # Initialize reverse search error with extraction context\n",
        "        super().__init__(message, **kwargs)\n",
        "\n",
        "\n",
        "class AlgorithmComplexity(Enum):\n",
        "    \"\"\"Enumeration of algorithmic complexity classifications for performance analysis.\"\"\"\n",
        "    LINEAR = \"O(n)\"\n",
        "    QUADRATIC = \"O(n²)\"\n",
        "    LOGARITHMIC = \"O(log n)\"\n",
        "    EXPONENTIAL = \"O(2^n)\"\n",
        "    CONSTANT = \"O(1)\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class KeypointConstraints:\n",
        "    \"\"\"\n",
        "    Mathematical constraints for keypoint detection algorithms.\n",
        "\n",
        "    Defines dimensional requirements, numerical ranges, and geometric properties\n",
        "    for keypoint data structures in computer vision applications.\n",
        "    \"\"\"\n",
        "\n",
        "    # Minimum number of keypoints for statistically significant analysis\n",
        "    min_keypoints: int = 10\n",
        "\n",
        "    # Maximum number of keypoints to prevent computational explosion\n",
        "    max_keypoints: int = 5000\n",
        "\n",
        "    # Coordinate bounds for image space [0, width] × [0, height]\n",
        "    coordinate_bounds: Tuple[float, float, float, float] = (0.0, 0.0, float('inf'), float('inf'))\n",
        "\n",
        "    # Response strength bounds for corner detection quality\n",
        "    response_bounds: Tuple[float, float] = (0.0, float('inf'))\n",
        "\n",
        "    # Angle bounds for orientation estimation [0, 2π]\n",
        "    angle_bounds: Tuple[float, float] = (0.0, 2.0 * np.pi)\n",
        "\n",
        "    # Scale bounds for scale-invariant detection\n",
        "    scale_bounds: Tuple[float, float] = (0.1, 10.0)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DescriptorConstraints:\n",
        "    \"\"\"\n",
        "    Mathematical constraints for binary descriptor algorithms.\n",
        "\n",
        "    Defines bit string properties, dimensionality requirements, and\n",
        "    information-theoretic constraints for binary descriptors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Standard descriptor length for BRIEF/ORB (256 bits = 32 bytes)\n",
        "    descriptor_length: int = 32\n",
        "\n",
        "    # Data type for binary descriptors (unsigned 8-bit integers)\n",
        "    descriptor_dtype: np.dtype = np.dtype(np.uint8)\n",
        "\n",
        "    # Hamming distance bounds [0, descriptor_length * 8]\n",
        "    hamming_distance_bounds: Tuple[int, int] = (0, 256)\n",
        "\n",
        "    # Entropy bounds for descriptor quality [0, log₂(descriptor_length * 8)]\n",
        "    entropy_bounds: Tuple[float, float] = (0.0, 8.0)\n",
        "\n",
        "    # Bit distribution bounds for uniform descriptor quality\n",
        "    bit_distribution_bounds: Tuple[float, float] = (0.4, 0.6)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MatchConstraints:\n",
        "    \"\"\"\n",
        "    Mathematical constraints for descriptor matching algorithms.\n",
        "\n",
        "    Defines distance thresholds, ratio test parameters, and\n",
        "    statistical significance requirements for feature matching.\n",
        "    \"\"\"\n",
        "\n",
        "    # Maximum Hamming distance for binary descriptor matching\n",
        "    max_hamming_distance: int = 64\n",
        "\n",
        "    # Lowe's ratio test threshold for disambiguation\n",
        "    lowe_ratio_threshold: float = 0.75\n",
        "\n",
        "    # Minimum number of matches for geometric verification\n",
        "    min_matches_for_verification: int = 4\n",
        "\n",
        "    # Maximum number of matches to prevent computational overflow\n",
        "    max_matches: int = 1000\n",
        "\n",
        "    # Cross-check validation requirement for bidirectional matching\n",
        "    require_cross_check: bool = True\n",
        "\n",
        "\n",
        "def protocol_validator(\n",
        "    constraints: Optional[Dict[str, Any]] = None,\n",
        "    log_violations: bool = True\n",
        ") -> Callable:\n",
        "    \"\"\"\n",
        "    Decorator factory for runtime protocol validation with mathematical constraints.\n",
        "\n",
        "    Implements comprehensive validation of protocol method calls including\n",
        "    parameter bounds checking, return value validation, and constraint verification.\n",
        "\n",
        "    Args:\n",
        "        constraints: Dictionary of mathematical constraints to enforce\n",
        "        log_violations: Whether to log constraint violations for debugging\n",
        "\n",
        "    Returns:\n",
        "        Decorator function for protocol method validation\n",
        "    \"\"\"\n",
        "\n",
        "    def decorator(func: Callable) -> Callable:\n",
        "        \"\"\"\n",
        "        Protocol method decorator implementing runtime validation.\n",
        "\n",
        "        Args:\n",
        "            func: Protocol method to be validated\n",
        "\n",
        "        Returns:\n",
        "            Wrapped function with validation logic\n",
        "        \"\"\"\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            \"\"\"\n",
        "            Validation wrapper implementing constraint checking and logging.\n",
        "\n",
        "            Args:\n",
        "                *args: Positional arguments to protocol method\n",
        "                **kwargs: Keyword arguments to protocol method\n",
        "\n",
        "            Returns:\n",
        "                Validated result from protocol method call\n",
        "\n",
        "            Raises:\n",
        "                ValueError: If constraints are violated\n",
        "                TypeError: If parameter types are invalid\n",
        "            \"\"\"\n",
        "            # Extract logger for validation reporting\n",
        "            logger = logging.getLogger(f\"{__name__}.protocol_validator\")\n",
        "\n",
        "            # Validate input parameters against mathematical constraints\n",
        "            if constraints:\n",
        "                # Iterate through constraint specifications\n",
        "                for param_name, constraint_spec in constraints.items():\n",
        "                    # Extract parameter value from args/kwargs\n",
        "                    param_value = kwargs.get(param_name)\n",
        "                    if param_value is None and len(args) > 0:\n",
        "                        # Attempt to extract from positional arguments\n",
        "                        sig = inspect.signature(func)\n",
        "                        param_names = list(sig.parameters.keys())\n",
        "                        if param_name in param_names:\n",
        "                            param_index = param_names.index(param_name)\n",
        "                            if param_index < len(args):\n",
        "                                param_value = args[param_index]\n",
        "\n",
        "                    # Apply constraint validation if parameter found\n",
        "                    if param_value is not None:\n",
        "                        # Validate numerical bounds constraints\n",
        "                        if 'bounds' in constraint_spec:\n",
        "                            min_val, max_val = constraint_spec['bounds']\n",
        "                            if not (min_val <= param_value <= max_val):\n",
        "                                error_msg = f\"Parameter {param_name} = {param_value} violates bounds [{min_val}, {max_val}]\"\n",
        "                                if log_violations:\n",
        "                                    logger.error(error_msg)\n",
        "                                raise ValueError(error_msg)\n",
        "\n",
        "                        # Validate type constraints\n",
        "                        if 'type' in constraint_spec:\n",
        "                            expected_type = constraint_spec['type']\n",
        "                            if not isinstance(param_value, expected_type):\n",
        "                                error_msg = f\"Parameter {param_name} has type {type(param_value)}, expected {expected_type}\"\n",
        "                                if log_violations:\n",
        "                                    logger.error(error_msg)\n",
        "                                raise TypeError(error_msg)\n",
        "\n",
        "                        # Validate shape constraints for array parameters\n",
        "                        if 'shape' in constraint_spec and hasattr(param_value, 'shape'):\n",
        "                            expected_shape = constraint_spec['shape']\n",
        "                            if param_value.shape != expected_shape and expected_shape is not None:\n",
        "                                error_msg = f\"Parameter {param_name} has shape {param_value.shape}, expected {expected_shape}\"\n",
        "                                if log_violations:\n",
        "                                    logger.error(error_msg)\n",
        "                                raise ValueError(error_msg)\n",
        "\n",
        "            # Execute protocol method with validated parameters\n",
        "            result = func(*args, **kwargs)\n",
        "\n",
        "            # Validate return value constraints if specified\n",
        "            if constraints and 'return_value' in constraints:\n",
        "                return_constraints = constraints['return_value']\n",
        "\n",
        "                # Validate return type constraints\n",
        "                if 'type' in return_constraints:\n",
        "                    expected_type = return_constraints['type']\n",
        "                    if not isinstance(result, expected_type):\n",
        "                        error_msg = f\"Return value has type {type(result)}, expected {expected_type}\"\n",
        "                        if log_violations:\n",
        "                            logger.error(error_msg)\n",
        "                        raise TypeError(error_msg)\n",
        "\n",
        "                # Validate return value bounds\n",
        "                if 'bounds' in return_constraints and result is not None:\n",
        "                    min_val, max_val = return_constraints['bounds']\n",
        "                    if hasattr(result, '__len__'):\n",
        "                        # Validate collection length bounds\n",
        "                        if not (min_val <= len(result) <= max_val):\n",
        "                            error_msg = f\"Return value length {len(result)} violates bounds [{min_val}, {max_val}]\"\n",
        "                            if log_violations:\n",
        "                                logger.error(error_msg)\n",
        "                            raise ValueError(error_msg)\n",
        "                    else:\n",
        "                        # Validate scalar value bounds\n",
        "                        if not (min_val <= result <= max_val):\n",
        "                            error_msg = f\"Return value {result} violates bounds [{min_val}, {max_val}]\"\n",
        "                            if log_violations:\n",
        "                                logger.error(error_msg)\n",
        "                            raise ValueError(error_msg)\n",
        "\n",
        "            return result\n",
        "\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "\n",
        "@runtime_checkable\n",
        "class FeatureDetectorProtocol(Protocol):\n",
        "    \"\"\"\n",
        "    Mathematical protocol defining interface for corner detection and feature extraction algorithms.\n",
        "\n",
        "    Implements the theoretical foundation for scale-invariant feature detection based on:\n",
        "\n",
        "    1. FAST Corner Detection:\n",
        "       - Corner response: R = det(M) - k·trace²(M) where M is structure tensor\n",
        "       - Harris matrix: M = G * [Ix² IxIy; IxIy Iy²] convolved with Gaussian G\n",
        "       - Corner threshold: R > threshold for corner classification\n",
        "\n",
        "    2. Scale-Space Representation:\n",
        "       - Gaussian pyramid: L(x,y,σ) = G(x,y,σ) * I(x,y)\n",
        "       - Scale selection: σ = 1.2^octave · 2^(layer/nOctaveLayers)\n",
        "       - Extrema detection: local maxima in scale-normalized Laplacian\n",
        "\n",
        "    3. Orientation Assignment:\n",
        "       - Gradient magnitude: m(x,y) = √((L(x+1,y) - L(x-1,y))² + (L(x,y+1) - L(x,y-1))²)\n",
        "       - Gradient orientation: θ(x,y) = atan2(L(x,y+1) - L(x,y-1), L(x+1,y) - L(x-1,y))\n",
        "       - Dominant orientation: peak in weighted orientation histogram\n",
        "\n",
        "    Mathematical Complexity: O(n·m·s) where n×m is image dimensions, s is scale levels\n",
        "    Memory Complexity: O(n·m·s) for scale pyramid storage\n",
        "    Numerical Stability: Requires σ > 0.5 for proper Gaussian approximation\n",
        "    \"\"\"\n",
        "\n",
        "    @protocol_validator(constraints={\n",
        "        'image': {\n",
        "            'type': np.ndarray,\n",
        "            'bounds': (0, float('inf'))  # Non-negative pixel values\n",
        "        },\n",
        "        'mask': {\n",
        "            'type': (type(None), np.ndarray)\n",
        "        },\n",
        "        'return_value': {\n",
        "            'type': tuple,\n",
        "            'bounds': (0, 10000)  # Reasonable keypoint count bounds\n",
        "        }\n",
        "    })\n",
        "    def detectAndCompute(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        mask: Optional[np.ndarray] = None\n",
        "    ) -> Tuple[List[cv2.KeyPoint], Optional[np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Detect keypoints and compute binary descriptors with mathematical guarantees.\n",
        "\n",
        "        Implements the complete feature detection pipeline:\n",
        "\n",
        "        1. Image Preprocessing:\n",
        "           - Gaussian smoothing: I' = G_σ * I for noise reduction\n",
        "           - Gradient computation: ∇I = [∂I/∂x, ∂I/∂y]\n",
        "           - Structure tensor: M = G_ρ * (∇I ∇I^T)\n",
        "\n",
        "        2. Keypoint Detection (FAST Algorithm):\n",
        "           - Circle test: |I(p) - I(x_i)| > t for i ∈ {1,2,...,16}\n",
        "           - Non-maximal suppression: local maxima in corner response\n",
        "           - Sub-pixel refinement: quadratic interpolation for precision\n",
        "\n",
        "        3. Descriptor Computation (BRIEF/ORB):\n",
        "           - Binary tests: τ(p; x,y) := 1 if p(x) < p(y), else 0\n",
        "           - Descriptor vector: f_n(p) := Σ_{1≤i≤n} 2^(i-1)τ(p; x_i, y_i)\n",
        "           - Rotation invariance: orientation compensation via patch rotation\n",
        "\n",
        "        Mathematical Requirements:\n",
        "        - Image must be single-channel grayscale: I: ℝ² → [0,255]\n",
        "        - Mask must be binary if provided: M: ℝ² → {0,1}\n",
        "        - Keypoints satisfy Harris corner criterion: R > threshold\n",
        "        - Descriptors maintain Hamming distance properties: d_H ∈ [0,256]\n",
        "\n",
        "        Args:\n",
        "            image: Input grayscale image as 2D numpy array of uint8 values\n",
        "                  Mathematical domain: I ∈ [0,255]^(H×W)\n",
        "            mask: Optional binary mask for region-of-interest detection\n",
        "                 Mathematical domain: M ∈ {0,1}^(H×W) or None\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - keypoints: List of cv2.KeyPoint objects with mathematical properties:\n",
        "              * pt: (x,y) coordinates in image space ℝ²\n",
        "              * response: corner strength R ∈ ℝ⁺\n",
        "              * angle: orientation θ ∈ [0,2π)\n",
        "              * size: characteristic scale σ ∈ ℝ⁺\n",
        "            - descriptors: Binary descriptor matrix of shape (n_keypoints, 32)\n",
        "              * Mathematical domain: D ∈ {0,1,...,255}^(n×32)\n",
        "              * Hamming distance: d_H(d_i, d_j) = Σ_k |d_i[k] ⊕ d_j[k]|\n",
        "              * None if no keypoints detected\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If image is not 2D grayscale or mask shape incompatible\n",
        "            TypeError: If input types do not conform to mathematical requirements\n",
        "\n",
        "        Mathematical Guarantees:\n",
        "        - Keypoint coordinates: (x,y) ∈ [0,W] × [0,H]\n",
        "        - Corner response: R ≥ threshold > 0\n",
        "        - Orientation: θ ∈ [0,2π) with ±π/12 accuracy\n",
        "        - Scale: σ ∈ [1.2^0, 1.2^nOctaves]\n",
        "        - Descriptor consistency: |d_i| = 256 bits ∀i\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "@runtime_checkable\n",
        "class MatcherProtocol(Protocol):\n",
        "    \"\"\"\n",
        "    Mathematical protocol defining interface for binary descriptor matching algorithms.\n",
        "\n",
        "    Implements theoretical foundation for nearest neighbor search in Hamming space:\n",
        "\n",
        "    1. Hamming Distance Computation:\n",
        "       - Binary XOR operation: d_H(x,y) = Σᵢ |xᵢ ⊕ yᵢ|\n",
        "       - Population count: popcount(x ⊕ y) for efficient computation\n",
        "       - Distance bounds: d_H ∈ [0, descriptor_length]\n",
        "\n",
        "    2. Brute-Force Matching:\n",
        "       - Exhaustive search: min_{j} d_H(qᵢ, tⱼ) for query qᵢ\n",
        "       - Time complexity: O(n·m·k) where n,m are descriptor counts, k is length\n",
        "       - Space complexity: O(n·m) for distance matrix storage\n",
        "\n",
        "    3. Cross-Check Validation:\n",
        "       - Bidirectional consistency: match(qᵢ, tⱼ) iff match(tⱼ, qᵢ)\n",
        "       - Reduces false positives: P(false_positive) ≈ 1/m\n",
        "       - Improves precision at cost of recall\n",
        "\n",
        "    Mathematical Properties:\n",
        "    - Metric space: (Hamming space, d_H) satisfies triangle inequality\n",
        "    - Symmetry: d_H(x,y) = d_H(y,x)\n",
        "    - Non-negativity: d_H(x,y) ≥ 0 with equality iff x = y\n",
        "    \"\"\"\n",
        "\n",
        "    @protocol_validator(constraints={\n",
        "        'queryDescriptors': {\n",
        "            'type': np.ndarray,\n",
        "            'shape': (None, 32)  # Standard BRIEF/ORB descriptor length\n",
        "        },\n",
        "        'trainDescriptors': {\n",
        "            'type': np.ndarray,\n",
        "            'shape': (None, 32)\n",
        "        },\n",
        "        'return_value': {\n",
        "            'type': list,\n",
        "            'bounds': (0, 10000)\n",
        "        }\n",
        "    })\n",
        "    def match(\n",
        "        self,\n",
        "        queryDescriptors: np.ndarray,\n",
        "        trainDescriptors: np.ndarray\n",
        "    ) -> List[cv2.DMatch]:\n",
        "        \"\"\"\n",
        "        Perform optimal assignment matching in Hamming space with mathematical guarantees.\n",
        "\n",
        "        Implements exhaustive nearest neighbor search with the following algorithm:\n",
        "\n",
        "        1. Distance Matrix Computation:\n",
        "           - Compute D[i,j] = d_H(query[i], train[j]) ∀i,j\n",
        "           - Use bit manipulation: d_H = popcount(query[i] ⊕ train[j])\n",
        "           - Matrix domain: D ∈ ℕ₀^(n_query × n_train)\n",
        "\n",
        "        2. Optimal Assignment:\n",
        "           - For each query descriptor qᵢ: match[i] = argmin_j D[i,j]\n",
        "           - Distance constraint: D[i, match[i]] ≤ threshold\n",
        "           - Uniqueness enforcement via cross-check if enabled\n",
        "\n",
        "        3. Cross-Check Validation (if enabled):\n",
        "           - Forward match: f(i) = argmin_j d_H(qᵢ, tⱼ)\n",
        "           - Backward match: b(j) = argmin_i d_H(qᵢ, tⱼ)\n",
        "           - Accept match iff b(f(i)) = i (bidirectional consistency)\n",
        "\n",
        "        Mathematical Requirements:\n",
        "        - Query descriptors: Q ∈ {0,1,...,255}^(n×32)\n",
        "        - Train descriptors: T ∈ {0,1,...,255}^(m×32)\n",
        "        - Hamming distance: d_H: {0,1}^k × {0,1}^k → [0,k]\n",
        "\n",
        "        Args:\n",
        "            queryDescriptors: Binary descriptors from first image\n",
        "                            Mathematical domain: Q ∈ {0,...,255}^(n×32)\n",
        "                            Each row represents 256-bit binary descriptor\n",
        "            trainDescriptors: Binary descriptors from second image\n",
        "                            Mathematical domain: T ∈ {0,...,255}^(m×32)\n",
        "                            Each row represents 256-bit binary descriptor\n",
        "\n",
        "        Returns:\n",
        "            List of cv2.DMatch objects with mathematical properties:\n",
        "            - queryIdx: index i ∈ [0, n-1] in query descriptor array\n",
        "            - trainIdx: index j ∈ [0, m-1] in train descriptor array\n",
        "            - distance: Hamming distance d_H(qᵢ, tⱼ) ∈ [0, 256]\n",
        "            - imgIdx: image index (typically 0 for single image matching)\n",
        "\n",
        "            Mathematical guarantees:\n",
        "            - Optimal assignment: distance[i] = min_j d_H(query[i], train[j])\n",
        "            - Valid indices: queryIdx ∈ [0,n), trainIdx ∈ [0,m)\n",
        "            - Metric properties: distance ≥ 0, symmetric if cross-check enabled\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If descriptor arrays have incompatible shapes\n",
        "            TypeError: If descriptors are not uint8 numpy arrays\n",
        "\n",
        "        Complexity Analysis:\n",
        "        - Time: O(n·m·k) where k=32 bytes, n,m are descriptor counts\n",
        "        - Space: O(n·m) for distance matrix plus O(min(n,m)) for matches\n",
        "        - Cache efficiency: depends on descriptor layout and SIMD utilization\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    @protocol_validator(constraints={\n",
        "        'queryDescriptors': {\n",
        "            'type': np.ndarray,\n",
        "            'shape': (None, 32)\n",
        "        },\n",
        "        'trainDescriptors': {\n",
        "            'type': np.ndarray,\n",
        "            'shape': (None, 32)\n",
        "        },\n",
        "        'k': {\n",
        "            'type': int,\n",
        "            'bounds': (1, 10)\n",
        "        },\n",
        "        'return_value': {\n",
        "            'type': list,\n",
        "            'bounds': (0, 100000)\n",
        "        }\n",
        "    })\n",
        "    def knnMatch(\n",
        "        self,\n",
        "        queryDescriptors: np.ndarray,\n",
        "        trainDescriptors: np.ndarray,\n",
        "        k: int\n",
        "    ) -> List[List[cv2.DMatch]]:\n",
        "        \"\"\"\n",
        "        K-nearest neighbor matching in Hamming space with Lowe's ratio test support.\n",
        "\n",
        "        Implements k-NN search with mathematical foundation:\n",
        "\n",
        "        1. K-Nearest Neighbor Search:\n",
        "           - For query qᵢ, find k smallest distances: d₁ ≤ d₂ ≤ ... ≤ dₖ\n",
        "           - Partial sorting: maintain k-element min-heap during search\n",
        "           - Time complexity: O(n·m·log k) with heap optimization\n",
        "\n",
        "        2. Distance Ordering:\n",
        "           - Sort matches by Hamming distance: d_H(qᵢ, t_{j₁}) ≤ d_H(qᵢ, t_{j₂}) ≤ ...\n",
        "           - Tie breaking: lexicographic order on train descriptor indices\n",
        "           - Consistency: deterministic ordering for reproducible results\n",
        "\n",
        "        3. Lowe's Ratio Test Preparation:\n",
        "           - First-to-second ratio: r = d₁/d₂ for disambiguation\n",
        "           - Threshold: typically r < 0.75 for reliable matches\n",
        "           - Mathematical basis: assumes descriptor noise follows uniform distribution\n",
        "\n",
        "        Statistical Foundation:\n",
        "        - Probability of random match: P(d_H ≤ t) = Σᵢ₌₀ᵗ C(256,i) / 2²⁵⁶\n",
        "        - Discrimination threshold: chosen to minimize false positive rate\n",
        "        - Power law distribution: P(distance) ∝ distance^(-α) for natural images\n",
        "\n",
        "        Args:\n",
        "            queryDescriptors: Binary descriptors from query image\n",
        "                            Mathematical domain: Q ∈ {0,...,255}^(n×32)\n",
        "            trainDescriptors: Binary descriptors from train image\n",
        "                            Mathematical domain: T ∈ {0,...,255}^(m×32)\n",
        "            k: Number of nearest neighbors to return\n",
        "               Mathematical constraint: k ∈ [1, min(m, reasonable_limit)]\n",
        "\n",
        "        Returns:\n",
        "            List of lists containing k-nearest matches per query descriptor:\n",
        "            - Outer list: length n (one entry per query descriptor)\n",
        "            - Inner lists: length ≤ k (k-nearest matches sorted by distance)\n",
        "            - Each DMatch: (queryIdx, trainIdx, distance, imgIdx)\n",
        "\n",
        "            Mathematical properties:\n",
        "            - Ordering: distance[0] ≤ distance[1] ≤ ... ≤ distance[k-1]\n",
        "            - Completeness: returns all matches if m < k\n",
        "            - Consistency: deterministic results for identical inputs\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If k > number of train descriptors or k ≤ 0\n",
        "            TypeError: If descriptor arrays have wrong type or shape\n",
        "\n",
        "        Applications:\n",
        "        - Lowe's ratio test: reject if distance[0]/distance[1] > threshold\n",
        "        - RANSAC initialization: use multiple correspondences per query\n",
        "        - Descriptor quality assessment: analyze distance distributions\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "@runtime_checkable\n",
        "class ClipModelLoaderProtocol(Protocol):\n",
        "    \"\"\"\n",
        "    Mathematical protocol defining interface for CLIP model loading and initialization.\n",
        "\n",
        "    Implements theoretical foundation for contrastive language-image pre-training:\n",
        "\n",
        "    1. Vision Transformer Architecture:\n",
        "       - Multi-head self-attention: Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
        "       - Layer normalization: LN(x) = γ(x-μ)/σ + β\n",
        "       - Feed-forward networks: FFN(x) = max(0, xW₁ + b₁)W₂ + b₂\n",
        "\n",
        "    2. Patch Embedding:\n",
        "       - Image tokenization: x_patch = Flatten(x_img) ∈ ℝ^(P²·C)\n",
        "       - Linear projection: z₀ = [x_class; E·x_patch] + E_pos\n",
        "       - Positional encoding: learned embeddings for spatial relationships\n",
        "\n",
        "    3. Contrastive Learning:\n",
        "       - Joint embedding space: f_v: Images → ℝ^d, f_t: Text → ℝ^d\n",
        "       - Cosine similarity: sim(I,T) = (f_v(I)·f_t(T))/(||f_v(I)||·||f_t(T)||)\n",
        "       - InfoNCE loss: ℒ = -log(exp(sim(I,T)/τ) / Σ_j exp(sim(I,T_j)/τ))\n",
        "\n",
        "    Memory Requirements:\n",
        "    - Model parameters: ~151M for ViT-B/32, ~428M for ViT-L/14\n",
        "    - Activation memory: O(batch_size · sequence_length · hidden_dim)\n",
        "    - GPU memory: typically 2-8GB depending on model size and batch size\n",
        "    \"\"\"\n",
        "\n",
        "    @protocol_validator(constraints={\n",
        "        'model_name': {\n",
        "            'type': str,\n",
        "            'bounds': (1, 100)  # Reasonable model name length\n",
        "        },\n",
        "        'device': {\n",
        "            'type': str\n",
        "        },\n",
        "        'return_value': {\n",
        "            'type': tuple,\n",
        "            'bounds': (2, 2)  # Exactly 2 elements: model and preprocess\n",
        "        }\n",
        "    })\n",
        "    def __call__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        device: str\n",
        "    ) -> Tuple[torch.nn.Module, Callable[[Any], torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Load pre-trained CLIP model with mathematical guarantees and optimization.\n",
        "\n",
        "        Implements comprehensive model loading pipeline:\n",
        "\n",
        "        1. Model Architecture Instantiation:\n",
        "           - Vision encoder: ViT with patch size P, hidden dimension d\n",
        "           - Text encoder: Transformer with vocabulary size V, context length L\n",
        "           - Projection heads: linear mappings to joint embedding space ℝ^d_joint\n",
        "\n",
        "        2. Weight Loading and Verification:\n",
        "           - Parameter initialization: load pre-trained weights from checkpoint\n",
        "           - Numerical precision: ensure float32/float16 consistency\n",
        "           - Architecture verification: validate layer dimensions and connections\n",
        "\n",
        "        3. Device Optimization:\n",
        "           - Memory allocation: efficient GPU/CPU memory management\n",
        "           - Compute optimization: enable appropriate acceleration (CUDA, MPS)\n",
        "           - Precision selection: mixed precision for memory efficiency\n",
        "\n",
        "        4. Preprocessing Pipeline:\n",
        "           - Image normalization: (pixel - μ) / σ where μ, σ are ImageNet statistics\n",
        "           - Resize and crop: maintain aspect ratio, center crop to model input size\n",
        "           - Tensor conversion: PIL/numpy → torch.Tensor with proper device placement\n",
        "\n",
        "        Mathematical Requirements:\n",
        "        - Model input: images ∈ ℝ^(B×3×H×W) where H,W depend on model variant\n",
        "        - Embedding output: features ∈ ℝ^(B×d_embed) with unit L2 norm\n",
        "        - Numerical stability: gradients bounded, no NaN/Inf propagation\n",
        "\n",
        "        Args:\n",
        "            model_name: CLIP model variant identifier\n",
        "                       Valid options: \"ViT-B/32\", \"ViT-B/16\", \"ViT-L/14\", \"RN50\", \"RN101\"\n",
        "                       Mathematical specification:\n",
        "                       - ViT-B/32: 12 layers, 768 hidden, 12 heads, 32×32 patches\n",
        "                       - ViT-L/14: 24 layers, 1024 hidden, 16 heads, 14×14 patches\n",
        "            device: Target computational device\n",
        "                   Options: \"cpu\", \"cuda\", \"cuda:N\", \"mps\"\n",
        "                   Mathematical consideration: affects precision and memory layout\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "            - model: torch.nn.Module with mathematical properties:\n",
        "              * encode_image: ℝ^(B×3×H×W) → ℝ^(B×d_embed)\n",
        "              * encode_text: ℝ^(B×L) → ℝ^(B×d_embed)\n",
        "              * Parameters: θ ∈ ℝ^N where N ≈ 151M for ViT-B/32\n",
        "              * Embedding dimension: d_embed = 512 for most variants\n",
        "            - preprocess: Callable with mathematical specification:\n",
        "              * Input: PIL.Image or numpy.ndarray\n",
        "              * Output: torch.Tensor ∈ ℝ^(3×H×W) with values in [-1,1]\n",
        "              * Normalization: (x - μ)/σ where μ=[0.485,0.456,0.406], σ=[0.229,0.224,0.225]\n",
        "              * Resize: bilinear interpolation to target resolution\n",
        "\n",
        "        Raises:\n",
        "            ModelLoadError: If model loading fails due to network, memory, or compatibility issues\n",
        "            ValueError: If model_name is not supported or device is invalid\n",
        "            RuntimeError: If insufficient memory or device incompatibility\n",
        "\n",
        "        Performance Characteristics:\n",
        "        - Loading time: O(model_size / bandwidth) + initialization overhead\n",
        "        - Memory usage: 1.5-2x model parameter count during loading\n",
        "        - Inference speed: depends on batch size, resolution, device compute capability\n",
        "\n",
        "        Mathematical Guarantees:\n",
        "        - Deterministic output: same input → same embedding (within numerical precision)\n",
        "        - Embedding norm: ||encode_image(x)||₂ ≈ 1 after normalization\n",
        "        - Lipschitz continuity: bounded sensitivity to input perturbations\n",
        "        - Cosine similarity preservation: sim(x₁,x₂) consistent across devices\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "@runtime_checkable\n",
        "class ImageHashProtocol(Protocol):\n",
        "    \"\"\"\n",
        "    Mathematical protocol defining interface for perceptual image hashing algorithms.\n",
        "\n",
        "    Implements theoretical foundation for content-based image fingerprinting:\n",
        "\n",
        "    1. Discrete Cosine Transform (DCT):\n",
        "       - 2D DCT: F(u,v) = α(u)α(v) Σₓ Σᵧ f(x,y)cos((2x+1)uπ/2N)cos((2y+1)vπ/2N)\n",
        "       - Frequency compaction: energy concentrated in low-frequency coefficients\n",
        "       - Basis functions: separable cosine functions with orthogonality\n",
        "\n",
        "    2. Perceptual Hash Generation:\n",
        "       - Low-frequency extraction: keep top-left N×N DCT coefficients\n",
        "       - Median thresholding: binary hash h[i,j] = 1 if F[i,j] > median(F), else 0\n",
        "       - Hash concatenation: serialize 2D binary matrix to 1D bit string\n",
        "\n",
        "    3. Hamming Distance:\n",
        "       - Bit difference: d_H(h₁,h₂) = Σᵢ |h₁[i] ⊕ h₂[i]|\n",
        "       - Similarity metric: sim = 1 - d_H/(hash_size²)\n",
        "       - Robustness: invariant to compression, scaling, minor modifications\n",
        "    \"\"\"\n",
        "\n",
        "    @protocol_validator(constraints={\n",
        "        'image': {\n",
        "            'type': (np.ndarray, Any)  # PIL Image or numpy array\n",
        "        },\n",
        "        'hash_size': {\n",
        "            'type': int,\n",
        "            'bounds': (4, 64)  # Practical hash size limits\n",
        "        },\n",
        "        'return_value': {\n",
        "            'type': object  # ImageHash object\n",
        "        }\n",
        "    })\n",
        "    def compute_hash(\n",
        "        self,\n",
        "        image: Union[np.ndarray, Any],\n",
        "        hash_size: int = 8\n",
        "    ) -> Any:\n",
        "        \"\"\"\n",
        "        Compute perceptual hash using DCT-based algorithm with mathematical rigor.\n",
        "\n",
        "        Implementation follows the mathematical specification:\n",
        "\n",
        "        1. Image Preprocessing:\n",
        "           - Grayscale conversion: I_gray = 0.299R + 0.587G + 0.114B\n",
        "           - Resize to (hash_size×4, hash_size×4): maintains frequency characteristics\n",
        "           - Gaussian smoothing: optional noise reduction filter\n",
        "\n",
        "        2. DCT Computation:\n",
        "           - Apply 2D DCT to preprocessed image\n",
        "           - Extract low-frequency coefficients: top-left hash_size×hash_size block\n",
        "           - Mathematical domain: DCT coefficients ∈ ℝ^(hash_size×hash_size)\n",
        "\n",
        "        3. Binary Hash Generation:\n",
        "           - Compute median of DCT coefficients: τ = median(DCT_coeffs)\n",
        "           - Threshold: h[i,j] = 1 if DCT[i,j] > τ, else 0\n",
        "           - Serialize to bit string: hash_bits ∈ {0,1}^(hash_size²)\n",
        "\n",
        "        Args:\n",
        "            image: Input image for hash computation\n",
        "                  Mathematical requirements: I ∈ [0,255]^(H×W×C) or ℝ^(H×W)\n",
        "            hash_size: Dimension N of N×N hash matrix\n",
        "                      Mathematical constraint: N ∈ [4,64] for practical applications\n",
        "\n",
        "        Returns:\n",
        "            ImageHash object with mathematical properties:\n",
        "            - Binary representation: bit string of length hash_size²\n",
        "            - Hamming distance: metric d_H: Hash × Hash → [0, hash_size²]\n",
        "            - Hex representation: compact string encoding\n",
        "\n",
        "        Mathematical Guarantees:\n",
        "        - Perceptual invariance: robust to JPEG compression, scaling, rotation\n",
        "        - Discrimination: different images → different hashes with high probability\n",
        "        - Efficiency: O(N²log N) computation via FFT-based DCT\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "def validate_protocol_implementation(\n",
        "    obj: Any,\n",
        "    protocol_class: Type[Protocol],\n",
        "    strict: bool = True\n",
        ") -> Tuple[bool, List[str]]:\n",
        "    \"\"\"\n",
        "    Validate that object properly implements protocol with mathematical constraints.\n",
        "\n",
        "    Performs comprehensive protocol conformance checking including:\n",
        "    - Method signature validation\n",
        "    - Return type compatibility\n",
        "    - Mathematical constraint verification\n",
        "    - Runtime behavior analysis\n",
        "\n",
        "    Args:\n",
        "        obj: Object to validate against protocol\n",
        "        protocol_class: Protocol class defining interface requirements\n",
        "        strict: Whether to enforce strict mathematical constraints\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (is_valid, violation_messages)\n",
        "\n",
        "    Mathematical Analysis:\n",
        "    - Signature compatibility: parameter types and counts must match exactly\n",
        "    - Return type variance: covariant return types allowed for subtypes\n",
        "    - Constraint satisfaction: all mathematical bounds must be respected\n",
        "    \"\"\"\n",
        "    # Initialize validation state\n",
        "    is_valid = True\n",
        "    violations = []\n",
        "\n",
        "    # Extract protocol methods for validation\n",
        "    protocol_methods = [\n",
        "        name for name in dir(protocol_class)\n",
        "        if not name.startswith('_') and callable(getattr(protocol_class, name, None))\n",
        "    ]\n",
        "\n",
        "    # Validate each protocol method\n",
        "    for method_name in protocol_methods:\n",
        "        # Check method existence on implementation object\n",
        "        if not hasattr(obj, method_name):\n",
        "            is_valid = False\n",
        "            violations.append(f\"Missing required method: {method_name}\")\n",
        "            continue\n",
        "\n",
        "        # Extract method objects for signature comparison\n",
        "        protocol_method = getattr(protocol_class, method_name)\n",
        "        impl_method = getattr(obj, method_name)\n",
        "\n",
        "        # Validate method is callable\n",
        "        if not callable(impl_method):\n",
        "            is_valid = False\n",
        "            violations.append(f\"Method {method_name} is not callable\")\n",
        "            continue\n",
        "\n",
        "        # Compare method signatures for compatibility\n",
        "        try:\n",
        "            protocol_sig = inspect.signature(protocol_method)\n",
        "            impl_sig = inspect.signature(impl_method)\n",
        "\n",
        "            # Validate parameter count compatibility\n",
        "            if len(protocol_sig.parameters) != len(impl_sig.parameters):\n",
        "                is_valid = False\n",
        "                violations.append(\n",
        "                    f\"Method {method_name} parameter count mismatch: \"\n",
        "                    f\"protocol={len(protocol_sig.parameters)}, \"\n",
        "                    f\"implementation={len(impl_sig.parameters)}\"\n",
        "                )\n",
        "\n",
        "            # Validate parameter types if strict checking enabled\n",
        "            if strict:\n",
        "                for proto_param, impl_param in zip(\n",
        "                    protocol_sig.parameters.values(),\n",
        "                    impl_sig.parameters.values()\n",
        "                ):\n",
        "                    # Check parameter name consistency\n",
        "                    if proto_param.name != impl_param.name:\n",
        "                        violations.append(\n",
        "                            f\"Method {method_name} parameter name mismatch: \"\n",
        "                            f\"protocol={proto_param.name}, implementation={impl_param.name}\"\n",
        "                        )\n",
        "\n",
        "                    # Check parameter annotation compatibility\n",
        "                    if (proto_param.annotation != inspect.Parameter.empty and\n",
        "                        impl_param.annotation != inspect.Parameter.empty and\n",
        "                        proto_param.annotation != impl_param.annotation):\n",
        "                        violations.append(\n",
        "                            f\"Method {method_name} parameter {proto_param.name} type mismatch: \"\n",
        "                            f\"protocol={proto_param.annotation}, implementation={impl_param.annotation}\"\n",
        "                        )\n",
        "\n",
        "        except (ValueError, TypeError) as e:\n",
        "            # Handle signature inspection failures\n",
        "            violations.append(f\"Failed to inspect method {method_name}: {e}\")\n",
        "\n",
        "    # Update validation result based on violations found\n",
        "    if violations:\n",
        "        is_valid = False\n",
        "\n",
        "    return is_valid, violations\n",
        "\n",
        "\n",
        "# Enhanced protocol registration for runtime checking\n",
        "def register_protocol_implementation(\n",
        "    impl_class: Type,\n",
        "    protocol_class: Type[Protocol],\n",
        "    validate_on_registration: bool = True\n",
        ") -> Type:\n",
        "    \"\"\"\n",
        "    Register class as protocol implementation with optional validation.\n",
        "\n",
        "    Provides runtime registration of protocol implementations with comprehensive\n",
        "    validation and mathematical constraint checking capabilities.\n",
        "\n",
        "    Args:\n",
        "        impl_class: Implementation class to register\n",
        "        protocol_class: Protocol class defining interface\n",
        "        validate_on_registration: Whether to validate immediately\n",
        "\n",
        "    Returns:\n",
        "        Registered implementation class\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If implementation does not conform to protocol\n",
        "        ValueError: If mathematical constraints are violated\n",
        "    \"\"\"\n",
        "    # Perform immediate validation if requested\n",
        "    if validate_on_registration:\n",
        "        # Create temporary instance for validation (if possible)\n",
        "        try:\n",
        "            temp_instance = impl_class()\n",
        "            is_valid, violations = validate_protocol_implementation(\n",
        "                temp_instance, protocol_class, strict=True\n",
        "            )\n",
        "\n",
        "            # Raise error if validation fails\n",
        "            if not is_valid:\n",
        "                violation_summary = \"; \".join(violations)\n",
        "                raise TypeError(\n",
        "                    f\"Class {impl_class.__name__} does not properly implement \"\n",
        "                    f\"{protocol_class.__name__}: {violation_summary}\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log validation attempt failure but allow registration\n",
        "            logger = logging.getLogger(__name__)\n",
        "            logger.warning(f\"Could not validate {impl_class.__name__} on registration: {e}\")\n",
        "\n",
        "    # Register class with protocol for isinstance checks\n",
        "    protocol_class.register(impl_class)\n",
        "\n",
        "    return impl_class\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LUG9NCKCWIkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Factory Classes for Resource Management\n",
        "\n",
        "class OptimizationStrategy(Enum):\n",
        "    \"\"\"Enumeration of parameter optimization strategies for algorithm tuning.\"\"\"\n",
        "    GRID_SEARCH = \"grid_search\"\n",
        "    RANDOM_SEARCH = \"random_search\"\n",
        "    BAYESIAN_OPTIMIZATION = \"bayesian_optimization\"\n",
        "    ADAPTIVE_TUNING = \"adaptive_tuning\"\n",
        "    PERFORMANCE_BASED = \"performance_based\"\n",
        "\n",
        "\n",
        "class ResourceConstraints(Enum):\n",
        "    \"\"\"System resource constraint levels for factory configuration.\"\"\"\n",
        "    LOW_MEMORY = \"low_memory\"\n",
        "    BALANCED = \"balanced\"\n",
        "    HIGH_PERFORMANCE = \"high_performance\"\n",
        "    UNLIMITED = \"unlimited\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ParameterConstraints:\n",
        "    \"\"\"\n",
        "    Mathematical constraints for algorithm parameters with optimization bounds.\n",
        "\n",
        "    Defines valid parameter ranges, optimization objectives, and performance\n",
        "    characteristics for systematic parameter tuning and validation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Parameter name for identification and logging\n",
        "    name: str\n",
        "\n",
        "    # Mathematical bounds: [min_value, max_value] for parameter domain\n",
        "    bounds: Tuple[Union[int, float], Union[int, float]]\n",
        "\n",
        "    # Data type constraint for parameter values\n",
        "    dtype: Type[Union[int, float, bool]]\n",
        "\n",
        "    # Optimization objective: 'minimize' or 'maximize' for tuning\n",
        "    optimization_objective: str = \"maximize\"\n",
        "\n",
        "    # Performance weight: importance factor for multi-objective optimization\n",
        "    performance_weight: float = 1.0\n",
        "\n",
        "    # Stability requirement: parameter sensitivity to input variations\n",
        "    stability_threshold: float = 0.1\n",
        "\n",
        "    # Computational cost factor: relative cost of parameter increase\n",
        "    computational_cost: float = 1.0\n",
        "\n",
        "    # Mathematical relationship to other parameters\n",
        "    dependencies: List[str] = field(default_factory=list)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PerformanceMetrics:\n",
        "    \"\"\"\n",
        "    Comprehensive performance metrics for algorithm evaluation and optimization.\n",
        "\n",
        "    Captures computational efficiency, mathematical accuracy, and resource\n",
        "    utilization for systematic parameter optimization and benchmarking.\n",
        "    \"\"\"\n",
        "\n",
        "    # Execution time measurements in seconds\n",
        "    mean_execution_time: float = 0.0\n",
        "    std_execution_time: float = 0.0\n",
        "    min_execution_time: float = float('inf')\n",
        "    max_execution_time: float = 0.0\n",
        "\n",
        "    # Memory usage statistics in bytes\n",
        "    peak_memory_usage: int = 0\n",
        "    average_memory_usage: int = 0\n",
        "    memory_efficiency_ratio: float = 0.0\n",
        "\n",
        "    # Algorithm quality metrics\n",
        "    detection_accuracy: float = 0.0\n",
        "    repeatability_score: float = 0.0\n",
        "    robustness_measure: float = 0.0\n",
        "\n",
        "    # Mathematical properties\n",
        "    numerical_stability: float = 0.0\n",
        "    convergence_rate: float = 0.0\n",
        "    error_variance: float = 0.0\n",
        "\n",
        "    # Resource utilization\n",
        "    cpu_utilization: float = 0.0\n",
        "    gpu_utilization: float = 0.0\n",
        "    cache_hit_ratio: float = 0.0\n",
        "\n",
        "    # Statistical significance\n",
        "    sample_count: int = 0\n",
        "    confidence_interval: Tuple[float, float] = (0.0, 0.0)\n",
        "    statistical_power: float = 0.0\n",
        "\n",
        "\n",
        "class ParameterOptimizer:\n",
        "    \"\"\"\n",
        "    Mathematical parameter optimization engine for computer vision algorithms.\n",
        "\n",
        "    Implements systematic parameter tuning using multiple optimization strategies\n",
        "    with performance profiling, statistical analysis, and convergence guarantees.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        constraints: Dict[str, ParameterConstraints],\n",
        "        optimization_strategy: OptimizationStrategy = OptimizationStrategy.ADAPTIVE_TUNING,\n",
        "        performance_samples: int = 10,\n",
        "        convergence_threshold: float = 0.01,\n",
        "        max_iterations: int = 100\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize parameter optimizer with mathematical constraints and strategy.\n",
        "\n",
        "        Args:\n",
        "            constraints: Dictionary mapping parameter names to constraint specifications\n",
        "            optimization_strategy: Algorithm for parameter space exploration\n",
        "            performance_samples: Number of performance measurements per configuration\n",
        "            convergence_threshold: Minimum improvement threshold for convergence\n",
        "            max_iterations: Maximum optimization iterations before termination\n",
        "        \"\"\"\n",
        "        # Store parameter constraints for validation and optimization\n",
        "        self.constraints: Dict[str, ParameterConstraints] = constraints\n",
        "\n",
        "        # Set optimization strategy for parameter space exploration\n",
        "        self.optimization_strategy: OptimizationStrategy = optimization_strategy\n",
        "\n",
        "        # Configure performance measurement parameters\n",
        "        self.performance_samples: int = performance_samples\n",
        "        self.convergence_threshold: float = convergence_threshold\n",
        "        self.max_iterations: int = max_iterations\n",
        "\n",
        "        # Initialize optimization history for analysis\n",
        "        self.optimization_history: List[Dict[str, Any]] = []\n",
        "\n",
        "        # Track best parameter configuration found\n",
        "        self.best_parameters: Optional[Dict[str, Any]] = None\n",
        "        self.best_performance: Optional[PerformanceMetrics] = None\n",
        "\n",
        "        # Setup logging for optimization monitoring\n",
        "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "\n",
        "    def validate_parameters(\n",
        "        self,\n",
        "        parameters: Dict[str, Any]\n",
        "    ) -> Tuple[bool, List[str]]:\n",
        "        \"\"\"\n",
        "        Validate parameter configuration against mathematical constraints.\n",
        "\n",
        "        Implements comprehensive parameter validation including bounds checking,\n",
        "        type verification, dependency analysis, and mathematical consistency.\n",
        "\n",
        "        Args:\n",
        "            parameters: Dictionary of parameter values to validate\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_valid, violation_messages) for constraint compliance\n",
        "        \"\"\"\n",
        "        # Initialize validation state\n",
        "        is_valid = True\n",
        "        violations = []\n",
        "\n",
        "        # Validate each parameter against its constraints\n",
        "        for param_name, param_value in parameters.items():\n",
        "            # Check if parameter has defined constraints\n",
        "            if param_name not in self.constraints:\n",
        "                violations.append(f\"Unknown parameter: {param_name}\")\n",
        "                is_valid = False\n",
        "                continue\n",
        "\n",
        "            # Extract constraint specification for parameter\n",
        "            constraint = self.constraints[param_name]\n",
        "\n",
        "            # Validate parameter type consistency\n",
        "            if not isinstance(param_value, constraint.dtype):\n",
        "                violations.append(\n",
        "                    f\"Parameter {param_name} has type {type(param_value)}, \"\n",
        "                    f\"expected {constraint.dtype}\"\n",
        "                )\n",
        "                is_valid = False\n",
        "                continue\n",
        "\n",
        "            # Validate parameter bounds compliance\n",
        "            min_bound, max_bound = constraint.bounds\n",
        "            if not (min_bound <= param_value <= max_bound):\n",
        "                violations.append(\n",
        "                    f\"Parameter {param_name} = {param_value} violates bounds \"\n",
        "                    f\"[{min_bound}, {max_bound}]\"\n",
        "                )\n",
        "                is_valid = False\n",
        "\n",
        "            # Validate parameter dependencies if specified\n",
        "            for dependency in constraint.dependencies:\n",
        "                if dependency in parameters:\n",
        "                    # Check dependency-specific constraints\n",
        "                    dep_value = parameters[dependency]\n",
        "                    if not self._validate_dependency(param_name, param_value, dependency, dep_value):\n",
        "                        violations.append(\n",
        "                            f\"Parameter {param_name} = {param_value} violates \"\n",
        "                            f\"dependency constraint with {dependency} = {dep_value}\"\n",
        "                        )\n",
        "                        is_valid = False\n",
        "\n",
        "        return is_valid, violations\n",
        "\n",
        "    def _validate_dependency(\n",
        "        self,\n",
        "        param_name: str,\n",
        "        param_value: Any,\n",
        "        dep_name: str,\n",
        "        dep_value: Any\n",
        "    ) -> bool:\n",
        "        \"\"\"\n",
        "        Validate parameter dependency constraints for mathematical consistency.\n",
        "\n",
        "        Args:\n",
        "            param_name: Name of parameter being validated\n",
        "            param_value: Value of parameter being validated\n",
        "            dep_name: Name of dependency parameter\n",
        "            dep_value: Value of dependency parameter\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating whether dependency constraint is satisfied\n",
        "        \"\"\"\n",
        "        # Implement specific dependency validation logic\n",
        "        # Example: scaleFactor must be > 1.0 and nlevels must be > 0\n",
        "        if param_name == \"scaleFactor\" and dep_name == \"nlevels\":\n",
        "            # Mathematical constraint: pyramid levels must support scale factor\n",
        "            max_possible_levels = int(np.log2(min(1024, 1024)) / np.log2(param_value))\n",
        "            return dep_value <= max_possible_levels\n",
        "\n",
        "        # Example: edgeThreshold must be < patchSize for ORB\n",
        "        if param_name == \"edgeThreshold\" and dep_name == \"patchSize\":\n",
        "            return param_value < dep_value\n",
        "\n",
        "        # Default: assume dependency is satisfied if no specific rule\n",
        "        return True\n",
        "\n",
        "    def optimize_parameters(\n",
        "        self,\n",
        "        objective_function: Callable[[Dict[str, Any]], PerformanceMetrics],\n",
        "        initial_parameters: Optional[Dict[str, Any]] = None\n",
        "    ) -> Tuple[Dict[str, Any], PerformanceMetrics]:\n",
        "        \"\"\"\n",
        "        Optimize algorithm parameters using specified optimization strategy.\n",
        "\n",
        "        Implements systematic parameter space exploration with convergence\n",
        "        monitoring, performance tracking, and statistical significance testing.\n",
        "\n",
        "        Args:\n",
        "            objective_function: Function mapping parameters to performance metrics\n",
        "            initial_parameters: Starting parameter configuration for optimization\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (optimal_parameters, best_performance_metrics)\n",
        "        \"\"\"\n",
        "        # Initialize optimization with starting parameters\n",
        "        if initial_parameters is None:\n",
        "            # Generate default starting parameters from constraint midpoints\n",
        "            current_parameters = self._generate_default_parameters()\n",
        "        else:\n",
        "            # Validate provided starting parameters\n",
        "            is_valid, violations = self.validate_parameters(initial_parameters)\n",
        "            if not is_valid:\n",
        "                raise ValueError(f\"Invalid initial parameters: {violations}\")\n",
        "            current_parameters = initial_parameters.copy()\n",
        "\n",
        "        # Initialize optimization tracking variables\n",
        "        iteration = 0\n",
        "        best_score = float('-inf')\n",
        "        consecutive_no_improvement = 0\n",
        "\n",
        "        # Log optimization start\n",
        "        self.logger.info(f\"Starting parameter optimization with strategy: {self.optimization_strategy}\")\n",
        "\n",
        "        # Main optimization loop with convergence monitoring\n",
        "        while iteration < self.max_iterations:\n",
        "            # Generate candidate parameters based on optimization strategy\n",
        "            if self.optimization_strategy == OptimizationStrategy.GRID_SEARCH:\n",
        "                candidate_parameters = self._grid_search_step(current_parameters, iteration)\n",
        "            elif self.optimization_strategy == OptimizationStrategy.RANDOM_SEARCH:\n",
        "                candidate_parameters = self._random_search_step()\n",
        "            elif self.optimization_strategy == OptimizationStrategy.ADAPTIVE_TUNING:\n",
        "                candidate_parameters = self._adaptive_tuning_step(current_parameters, iteration)\n",
        "            else:\n",
        "                # Default to random perturbation\n",
        "                candidate_parameters = self._random_perturbation(current_parameters)\n",
        "\n",
        "            # Validate candidate parameters before evaluation\n",
        "            is_valid, violations = self.validate_parameters(candidate_parameters)\n",
        "            if not is_valid:\n",
        "                # Skip invalid parameter configurations\n",
        "                self.logger.warning(f\"Skipping invalid parameters: {violations}\")\n",
        "                iteration += 1\n",
        "                continue\n",
        "\n",
        "            # Evaluate candidate parameters with performance measurement\n",
        "            try:\n",
        "                performance = self._evaluate_parameters_with_statistics(\n",
        "                    objective_function, candidate_parameters\n",
        "                )\n",
        "            except Exception as e:\n",
        "                # Handle evaluation failures gracefully\n",
        "                self.logger.error(f\"Parameter evaluation failed: {e}\")\n",
        "                iteration += 1\n",
        "                continue\n",
        "\n",
        "            # Compute composite performance score for optimization\n",
        "            composite_score = self._compute_composite_score(performance)\n",
        "\n",
        "            # Update best configuration if improvement found\n",
        "            if composite_score > best_score + self.convergence_threshold:\n",
        "                # Record new best configuration\n",
        "                best_score = composite_score\n",
        "                current_parameters = candidate_parameters.copy()\n",
        "                self.best_parameters = candidate_parameters.copy()\n",
        "                self.best_performance = performance\n",
        "                consecutive_no_improvement = 0\n",
        "\n",
        "                # Log improvement\n",
        "                self.logger.info(\n",
        "                    f\"Iteration {iteration}: New best score {composite_score:.6f} \"\n",
        "                    f\"with parameters {candidate_parameters}\"\n",
        "                )\n",
        "            else:\n",
        "                # Track consecutive iterations without improvement\n",
        "                consecutive_no_improvement += 1\n",
        "\n",
        "            # Record optimization history for analysis\n",
        "            self.optimization_history.append({\n",
        "                'iteration': iteration,\n",
        "                'parameters': candidate_parameters.copy(),\n",
        "                'performance': performance,\n",
        "                'composite_score': composite_score,\n",
        "                'is_best': composite_score > best_score\n",
        "            })\n",
        "\n",
        "            # Check for convergence based on consecutive no-improvement\n",
        "            if consecutive_no_improvement >= 10:\n",
        "                self.logger.info(f\"Optimization converged after {iteration} iterations\")\n",
        "                break\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        # Log optimization completion\n",
        "        self.logger.info(\n",
        "            f\"Optimization completed: best score {best_score:.6f} \"\n",
        "            f\"after {iteration} iterations\"\n",
        "        )\n",
        "\n",
        "        return self.best_parameters, self.best_performance\n",
        "\n",
        "    def _generate_default_parameters(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate default parameter configuration from constraint midpoints.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of default parameter values\n",
        "        \"\"\"\n",
        "        # Initialize default parameters dictionary\n",
        "        default_params = {}\n",
        "\n",
        "        # Generate midpoint values for each constrained parameter\n",
        "        for param_name, constraint in self.constraints.items():\n",
        "            min_bound, max_bound = constraint.bounds\n",
        "\n",
        "            # Compute midpoint based on parameter type\n",
        "            if constraint.dtype == int:\n",
        "                # Integer midpoint with proper rounding\n",
        "                default_params[param_name] = int((min_bound + max_bound) / 2)\n",
        "            elif constraint.dtype == float:\n",
        "                # Floating point midpoint\n",
        "                default_params[param_name] = (min_bound + max_bound) / 2.0\n",
        "            elif constraint.dtype == bool:\n",
        "                # Boolean default to True for most algorithm parameters\n",
        "                default_params[param_name] = True\n",
        "\n",
        "        return default_params\n",
        "\n",
        "    def _evaluate_parameters_with_statistics(\n",
        "        self,\n",
        "        objective_function: Callable[[Dict[str, Any]], PerformanceMetrics],\n",
        "        parameters: Dict[str, Any]\n",
        "    ) -> PerformanceMetrics:\n",
        "        \"\"\"\n",
        "        Evaluate parameter configuration with statistical sampling and analysis.\n",
        "\n",
        "        Args:\n",
        "            objective_function: Performance evaluation function\n",
        "            parameters: Parameter configuration to evaluate\n",
        "\n",
        "        Returns:\n",
        "            Aggregated performance metrics with statistical properties\n",
        "        \"\"\"\n",
        "        # Collect multiple performance samples for statistical analysis\n",
        "        performance_samples = []\n",
        "        execution_times = []\n",
        "        memory_usages = []\n",
        "\n",
        "        # Perform multiple evaluations for statistical significance\n",
        "        for sample_idx in range(self.performance_samples):\n",
        "            # Record start time and memory for performance measurement\n",
        "            start_time = time.perf_counter()\n",
        "            start_memory = psutil.Process().memory_info().rss\n",
        "\n",
        "            # Execute objective function with current parameters\n",
        "            try:\n",
        "                sample_performance = objective_function(parameters)\n",
        "                performance_samples.append(sample_performance)\n",
        "            except Exception as e:\n",
        "                # Log evaluation error and continue with remaining samples\n",
        "                self.logger.warning(f\"Sample {sample_idx} evaluation failed: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Record execution time and memory usage\n",
        "            execution_time = time.perf_counter() - start_time\n",
        "            memory_usage = psutil.Process().memory_info().rss - start_memory\n",
        "            execution_times.append(execution_time)\n",
        "            memory_usages.append(memory_usage)\n",
        "\n",
        "        # Aggregate performance metrics across samples\n",
        "        if not performance_samples:\n",
        "            # Return default metrics if all samples failed\n",
        "            return PerformanceMetrics(sample_count=0)\n",
        "\n",
        "        # Compute statistical aggregates for performance metrics\n",
        "        aggregated_performance = PerformanceMetrics(\n",
        "            mean_execution_time=statistics.mean(execution_times),\n",
        "            std_execution_time=statistics.stdev(execution_times) if len(execution_times) > 1 else 0.0,\n",
        "            min_execution_time=min(execution_times),\n",
        "            max_execution_time=max(execution_times),\n",
        "            peak_memory_usage=max(memory_usages) if memory_usages else 0,\n",
        "            average_memory_usage=int(statistics.mean(memory_usages)) if memory_usages else 0,\n",
        "            sample_count=len(performance_samples)\n",
        "        )\n",
        "\n",
        "        # Compute confidence interval for performance estimate\n",
        "        if len(performance_samples) > 1:\n",
        "            # Calculate 95% confidence interval using t-distribution\n",
        "            sample_std = statistics.stdev(execution_times)\n",
        "            t_critical = 2.086  # t-value for 95% CI with small samples\n",
        "            margin_of_error = t_critical * (sample_std / np.sqrt(len(execution_times)))\n",
        "            mean_time = aggregated_performance.mean_execution_time\n",
        "            aggregated_performance.confidence_interval = (\n",
        "                mean_time - margin_of_error,\n",
        "                mean_time + margin_of_error\n",
        "            )\n",
        "\n",
        "        return aggregated_performance\n",
        "\n",
        "    def _compute_composite_score(self, performance: PerformanceMetrics) -> float:\n",
        "        \"\"\"\n",
        "        Compute composite performance score for optimization objective.\n",
        "\n",
        "        Args:\n",
        "            performance: Performance metrics to score\n",
        "\n",
        "        Returns:\n",
        "            Composite score for optimization comparison\n",
        "        \"\"\"\n",
        "        # Initialize composite score with base performance metrics\n",
        "        score = 0.0\n",
        "\n",
        "        # Weight execution time performance (lower is better)\n",
        "        if performance.mean_execution_time > 0:\n",
        "            time_score = 1.0 / (1.0 + performance.mean_execution_time)\n",
        "            score += 0.3 * time_score\n",
        "\n",
        "        # Weight memory efficiency (lower usage is better)\n",
        "        if performance.peak_memory_usage > 0:\n",
        "            memory_score = 1.0 / (1.0 + performance.peak_memory_usage / 1e6)  # Normalize to MB\n",
        "            score += 0.2 * memory_score\n",
        "\n",
        "        # Weight algorithm accuracy (higher is better)\n",
        "        score += 0.3 * performance.detection_accuracy\n",
        "\n",
        "        # Weight numerical stability (higher is better)\n",
        "        score += 0.1 * performance.numerical_stability\n",
        "\n",
        "        # Weight statistical confidence (higher sample count is better)\n",
        "        if performance.sample_count > 0:\n",
        "            confidence_score = min(1.0, performance.sample_count / self.performance_samples)\n",
        "            score += 0.1 * confidence_score\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _grid_search_step(\n",
        "        self,\n",
        "        current_parameters: Dict[str, Any],\n",
        "        iteration: int\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate next parameter configuration using grid search strategy.\n",
        "\n",
        "        Args:\n",
        "            current_parameters: Current parameter configuration\n",
        "            iteration: Current optimization iteration\n",
        "\n",
        "        Returns:\n",
        "            Next parameter configuration to evaluate\n",
        "        \"\"\"\n",
        "        # Implement systematic grid search over parameter space\n",
        "        # This is a simplified implementation - full grid search would be more complex\n",
        "        candidate_params = current_parameters.copy()\n",
        "\n",
        "        # Select parameter to modify based on iteration\n",
        "        param_names = list(self.constraints.keys())\n",
        "        param_to_modify = param_names[iteration % len(param_names)]\n",
        "\n",
        "        # Generate grid point for selected parameter\n",
        "        constraint = self.constraints[param_to_modify]\n",
        "        min_bound, max_bound = constraint.bounds\n",
        "\n",
        "        # Create 5-point grid and select next point\n",
        "        grid_points = np.linspace(min_bound, max_bound, 5)\n",
        "        grid_index = (iteration // len(param_names)) % len(grid_points)\n",
        "\n",
        "        if constraint.dtype == int:\n",
        "            candidate_params[param_to_modify] = int(grid_points[grid_index])\n",
        "        else:\n",
        "            candidate_params[param_to_modify] = float(grid_points[grid_index])\n",
        "\n",
        "        return candidate_params\n",
        "\n",
        "    def _random_search_step(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate random parameter configuration for random search strategy.\n",
        "\n",
        "        Returns:\n",
        "            Randomly generated parameter configuration\n",
        "        \"\"\"\n",
        "        # Generate random parameters within constraints\n",
        "        random_params = {}\n",
        "\n",
        "        # Sample each parameter randomly from its constraint bounds\n",
        "        for param_name, constraint in self.constraints.items():\n",
        "            min_bound, max_bound = constraint.bounds\n",
        "\n",
        "            # Generate random value based on parameter type\n",
        "            if constraint.dtype == int:\n",
        "                random_params[param_name] = np.random.randint(min_bound, max_bound + 1)\n",
        "            elif constraint.dtype == float:\n",
        "                random_params[param_name] = np.random.uniform(min_bound, max_bound)\n",
        "            elif constraint.dtype == bool:\n",
        "                random_params[param_name] = np.random.choice([True, False])\n",
        "\n",
        "        return random_params\n",
        "\n",
        "    def _adaptive_tuning_step(\n",
        "        self,\n",
        "        current_parameters: Dict[str, Any],\n",
        "        iteration: int\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate parameter configuration using adaptive tuning strategy.\n",
        "\n",
        "        Args:\n",
        "            current_parameters: Current best parameter configuration\n",
        "            iteration: Current optimization iteration\n",
        "\n",
        "        Returns:\n",
        "            Adaptively tuned parameter configuration\n",
        "        \"\"\"\n",
        "        # Implement adaptive parameter adjustment based on optimization history\n",
        "        candidate_params = current_parameters.copy()\n",
        "\n",
        "        # Analyze optimization history for parameter sensitivity\n",
        "        if len(self.optimization_history) > 5:\n",
        "            # Compute parameter sensitivity from recent history\n",
        "            sensitivity_scores = self._compute_parameter_sensitivity()\n",
        "\n",
        "            # Focus on most sensitive parameters for tuning\n",
        "            most_sensitive_param = max(sensitivity_scores.keys(),\n",
        "                                     key=lambda k: sensitivity_scores[k])\n",
        "\n",
        "            # Adjust most sensitive parameter adaptively\n",
        "            constraint = self.constraints[most_sensitive_param]\n",
        "            current_value = current_parameters[most_sensitive_param]\n",
        "            min_bound, max_bound = constraint.bounds\n",
        "\n",
        "            # Compute adaptive step size based on constraint range\n",
        "            step_size = (max_bound - min_bound) * 0.1 * (1.0 / (1.0 + iteration * 0.1))\n",
        "\n",
        "            # Apply random perturbation with adaptive step size\n",
        "            if constraint.dtype == int:\n",
        "                perturbation = int(np.random.normal(0, step_size))\n",
        "                new_value = np.clip(current_value + perturbation, min_bound, max_bound)\n",
        "                candidate_params[most_sensitive_param] = int(new_value)\n",
        "            elif constraint.dtype == float:\n",
        "                perturbation = np.random.normal(0, step_size)\n",
        "                new_value = np.clip(current_value + perturbation, min_bound, max_bound)\n",
        "                candidate_params[most_sensitive_param] = float(new_value)\n",
        "        else:\n",
        "            # Use random perturbation for initial iterations\n",
        "            candidate_params = self._random_perturbation(current_parameters)\n",
        "\n",
        "        return candidate_params\n",
        "\n",
        "    def _compute_parameter_sensitivity(self) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute parameter sensitivity scores from optimization history.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping parameter names to sensitivity scores\n",
        "        \"\"\"\n",
        "        # Initialize sensitivity tracking\n",
        "        sensitivity_scores = defaultdict(float)\n",
        "\n",
        "        # Analyze recent optimization history\n",
        "        recent_history = self.optimization_history[-10:]  # Last 10 iterations\n",
        "\n",
        "        # Compute parameter variance and performance correlation\n",
        "        for param_name in self.constraints.keys():\n",
        "            # Extract parameter values and performance scores\n",
        "            param_values = [entry['parameters'][param_name] for entry in recent_history]\n",
        "            performance_scores = [entry['composite_score'] for entry in recent_history]\n",
        "\n",
        "            # Compute correlation between parameter changes and performance\n",
        "            if len(param_values) > 1 and len(set(param_values)) > 1:\n",
        "                # Calculate Pearson correlation coefficient\n",
        "                correlation = np.corrcoef(param_values, performance_scores)[0, 1]\n",
        "                # Use absolute correlation as sensitivity measure\n",
        "                sensitivity_scores[param_name] = abs(correlation) if not np.isnan(correlation) else 0.0\n",
        "\n",
        "        return dict(sensitivity_scores)\n",
        "\n",
        "    def _random_perturbation(self, base_parameters: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Apply random perturbation to base parameter configuration.\n",
        "\n",
        "        Args:\n",
        "            base_parameters: Base parameter configuration to perturb\n",
        "\n",
        "        Returns:\n",
        "            Perturbed parameter configuration\n",
        "        \"\"\"\n",
        "        # Create copy of base parameters for modification\n",
        "        perturbed_params = base_parameters.copy()\n",
        "\n",
        "        # Apply random perturbation to subset of parameters\n",
        "        param_names = list(self.constraints.keys())\n",
        "        num_params_to_perturb = max(1, len(param_names) // 3)  # Perturb ~1/3 of parameters\n",
        "\n",
        "        # Randomly select parameters to perturb\n",
        "        params_to_perturb = np.random.choice(param_names, num_params_to_perturb, replace=False)\n",
        "\n",
        "        # Apply perturbation to selected parameters\n",
        "        for param_name in params_to_perturb:\n",
        "            constraint = self.constraints[param_name]\n",
        "            current_value = base_parameters[param_name]\n",
        "            min_bound, max_bound = constraint.bounds\n",
        "\n",
        "            # Compute perturbation magnitude as fraction of range\n",
        "            range_size = max_bound - min_bound\n",
        "            perturbation_magnitude = range_size * 0.2  # 20% of range\n",
        "\n",
        "            # Apply type-specific perturbation\n",
        "            if constraint.dtype == int:\n",
        "                perturbation = int(np.random.normal(0, perturbation_magnitude))\n",
        "                new_value = np.clip(current_value + perturbation, min_bound, max_bound)\n",
        "                perturbed_params[param_name] = int(new_value)\n",
        "            elif constraint.dtype == float:\n",
        "                perturbation = np.random.normal(0, perturbation_magnitude)\n",
        "                new_value = np.clip(current_value + perturbation, min_bound, max_bound)\n",
        "                perturbed_params[param_name] = float(new_value)\n",
        "            elif constraint.dtype == bool:\n",
        "                # Flip boolean with 30% probability\n",
        "                if np.random.random() < 0.3:\n",
        "                    perturbed_params[param_name] = not current_value\n",
        "\n",
        "        return perturbed_params\n",
        "\n",
        "\n",
        "class ResourceManager:\n",
        "    \"\"\"\n",
        "    Enterprise-grade resource management for computer vision algorithms.\n",
        "\n",
        "    Implements comprehensive resource monitoring, allocation optimization,\n",
        "    and cleanup strategies for production deployment environments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        resource_constraints: ResourceConstraints = ResourceConstraints.BALANCED,\n",
        "        monitoring_interval: float = 1.0,\n",
        "        cleanup_threshold: float = 0.8\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize resource manager with monitoring and constraint configuration.\n",
        "\n",
        "        Args:\n",
        "            resource_constraints: Target resource utilization profile\n",
        "            monitoring_interval: Seconds between resource monitoring updates\n",
        "            cleanup_threshold: Resource utilization threshold for cleanup trigger\n",
        "        \"\"\"\n",
        "        # Store resource management configuration\n",
        "        self.resource_constraints = resource_constraints\n",
        "        self.monitoring_interval = monitoring_interval\n",
        "        self.cleanup_threshold = cleanup_threshold\n",
        "\n",
        "        # Initialize resource monitoring state\n",
        "        self.monitoring_active = False\n",
        "        self.monitoring_thread: Optional[threading.Thread] = None\n",
        "        self.resource_history: List[Dict[str, Any]] = []\n",
        "\n",
        "        # Setup resource tracking locks for thread safety\n",
        "        self._monitoring_lock = threading.Lock()\n",
        "        self._cleanup_lock = threading.Lock()\n",
        "\n",
        "        # Initialize logger for resource management events\n",
        "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "\n",
        "        # Configure resource limits based on constraints\n",
        "        self._configure_resource_limits()\n",
        "\n",
        "    def _configure_resource_limits(self) -> None:\n",
        "        \"\"\"Configure resource limits based on constraint profile.\"\"\"\n",
        "        # Set memory and CPU limits based on constraint level\n",
        "        if self.resource_constraints == ResourceConstraints.LOW_MEMORY:\n",
        "            # Conservative limits for memory-constrained environments\n",
        "            self.max_memory_usage = 1.0 * 1024**3  # 1GB\n",
        "            self.max_cpu_cores = 2\n",
        "            self.enable_aggressive_cleanup = True\n",
        "        elif self.resource_constraints == ResourceConstraints.BALANCED:\n",
        "            # Balanced limits for typical production environments\n",
        "            self.max_memory_usage = 4.0 * 1024**3  # 4GB\n",
        "            self.max_cpu_cores = psutil.cpu_count() // 2\n",
        "            self.enable_aggressive_cleanup = False\n",
        "        elif self.resource_constraints == ResourceConstraints.HIGH_PERFORMANCE:\n",
        "            # Generous limits for high-performance environments\n",
        "            self.max_memory_usage = 16.0 * 1024**3  # 16GB\n",
        "            self.max_cpu_cores = psutil.cpu_count()\n",
        "            self.enable_aggressive_cleanup = False\n",
        "        else:  # UNLIMITED\n",
        "            # No artificial limits for unlimited environments\n",
        "            self.max_memory_usage = float('inf')\n",
        "            self.max_cpu_cores = psutil.cpu_count()\n",
        "            self.enable_aggressive_cleanup = False\n",
        "\n",
        "    def start_monitoring(self) -> None:\n",
        "        \"\"\"Start background resource monitoring thread.\"\"\"\n",
        "        # Acquire lock to prevent concurrent monitoring starts\n",
        "        with self._monitoring_lock:\n",
        "            # Start monitoring only if not already active\n",
        "            if not self.monitoring_active:\n",
        "                # Set monitoring state and create background thread\n",
        "                self.monitoring_active = True\n",
        "                self.monitoring_thread = threading.Thread(\n",
        "                    target=self._monitoring_loop,\n",
        "                    daemon=True,\n",
        "                    name=\"ResourceMonitor\"\n",
        "                )\n",
        "                # Start monitoring thread\n",
        "                self.monitoring_thread.start()\n",
        "                self.logger.info(\"Resource monitoring started\")\n",
        "\n",
        "    def stop_monitoring(self) -> None:\n",
        "        \"\"\"Stop background resource monitoring thread.\"\"\"\n",
        "        # Acquire lock to prevent concurrent monitoring stops\n",
        "        with self._monitoring_lock:\n",
        "            # Stop monitoring if currently active\n",
        "            if self.monitoring_active:\n",
        "                # Signal monitoring thread to stop\n",
        "                self.monitoring_active = False\n",
        "                # Wait for monitoring thread to complete\n",
        "                if self.monitoring_thread and self.monitoring_thread.is_alive():\n",
        "                    self.monitoring_thread.join(timeout=5.0)\n",
        "                self.logger.info(\"Resource monitoring stopped\")\n",
        "\n",
        "    def _monitoring_loop(self) -> None:\n",
        "        \"\"\"Background monitoring loop for resource tracking.\"\"\"\n",
        "        # Continue monitoring while active flag is set\n",
        "        while self.monitoring_active:\n",
        "            try:\n",
        "                # Collect current resource usage statistics\n",
        "                resource_stats = self._collect_resource_stats()\n",
        "\n",
        "                # Store resource statistics in history\n",
        "                with self._monitoring_lock:\n",
        "                    self.resource_history.append(resource_stats)\n",
        "                    # Limit history size to prevent memory growth\n",
        "                    if len(self.resource_history) > 1000:\n",
        "                        self.resource_history = self.resource_history[-500:]\n",
        "\n",
        "                # Check if cleanup is needed based on resource usage\n",
        "                if self._should_trigger_cleanup(resource_stats):\n",
        "                    # Trigger resource cleanup in separate thread\n",
        "                    cleanup_thread = threading.Thread(\n",
        "                        target=self._perform_cleanup,\n",
        "                        daemon=True,\n",
        "                        name=\"ResourceCleanup\"\n",
        "                    )\n",
        "                    cleanup_thread.start()\n",
        "\n",
        "                # Sleep until next monitoring interval\n",
        "                time.sleep(self.monitoring_interval)\n",
        "\n",
        "            except Exception as e:\n",
        "                # Log monitoring errors but continue operation\n",
        "                self.logger.error(f\"Resource monitoring error: {e}\")\n",
        "                time.sleep(self.monitoring_interval)\n",
        "\n",
        "    def _collect_resource_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Collect comprehensive system resource usage statistics.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing current resource usage metrics\n",
        "        \"\"\"\n",
        "        # Get current process for resource measurement\n",
        "        current_process = psutil.Process()\n",
        "\n",
        "        # Collect memory usage statistics\n",
        "        memory_info = current_process.memory_info()\n",
        "        virtual_memory = psutil.virtual_memory()\n",
        "\n",
        "        # Collect CPU usage statistics\n",
        "        cpu_percent = current_process.cpu_percent()\n",
        "        system_cpu_percent = psutil.cpu_percent()\n",
        "\n",
        "        # Collect GPU usage if available\n",
        "        gpu_usage = 0.0\n",
        "        gpu_memory_usage = 0\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                # Get GPU utilization and memory usage\n",
        "                gpu_usage = torch.cuda.utilization()\n",
        "                gpu_memory_usage = torch.cuda.memory_allocated()\n",
        "            except Exception:\n",
        "                # Handle GPU query failures gracefully\n",
        "                pass\n",
        "\n",
        "        # Construct comprehensive resource statistics\n",
        "        resource_stats = {\n",
        "            'timestamp': time.time(),\n",
        "            'memory': {\n",
        "                'rss': memory_info.rss,  # Resident Set Size\n",
        "                'vms': memory_info.vms,  # Virtual Memory Size\n",
        "                'percent': current_process.memory_percent(),\n",
        "                'available': virtual_memory.available,\n",
        "                'total': virtual_memory.total\n",
        "            },\n",
        "            'cpu': {\n",
        "                'process_percent': cpu_percent,\n",
        "                'system_percent': system_cpu_percent,\n",
        "                'core_count': psutil.cpu_count()\n",
        "            },\n",
        "            'gpu': {\n",
        "                'utilization': gpu_usage,\n",
        "                'memory_allocated': gpu_memory_usage,\n",
        "                'memory_reserved': torch.cuda.memory_reserved() if torch.cuda.is_available() else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return resource_stats\n",
        "\n",
        "    def _should_trigger_cleanup(self, resource_stats: Dict[str, Any]) -> bool:\n",
        "        \"\"\"\n",
        "        Determine if resource cleanup should be triggered based on usage.\n",
        "\n",
        "        Args:\n",
        "            resource_stats: Current resource usage statistics\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating whether cleanup should be performed\n",
        "        \"\"\"\n",
        "        # Check memory usage threshold\n",
        "        memory_usage_ratio = resource_stats['memory']['rss'] / self.max_memory_usage\n",
        "        if memory_usage_ratio > self.cleanup_threshold:\n",
        "            return True\n",
        "\n",
        "        # Check GPU memory usage threshold if available\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_memory_ratio = resource_stats['gpu']['memory_allocated'] / torch.cuda.get_device_properties(0).total_memory\n",
        "            if gpu_memory_ratio > self.cleanup_threshold:\n",
        "                return True\n",
        "\n",
        "        # Check system memory pressure\n",
        "        system_memory_ratio = resource_stats['memory']['percent'] / 100.0\n",
        "        if system_memory_ratio > 0.9:  # System memory > 90%\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _perform_cleanup(self) -> None:\n",
        "        \"\"\"Perform comprehensive resource cleanup operations.\"\"\"\n",
        "        # Acquire cleanup lock to prevent concurrent cleanup\n",
        "        with self._cleanup_lock:\n",
        "            try:\n",
        "                # Log cleanup initiation\n",
        "                self.logger.info(\"Initiating resource cleanup\")\n",
        "\n",
        "                # Perform garbage collection for Python objects\n",
        "                gc.collect()\n",
        "\n",
        "                # Clear GPU memory cache if available\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                    torch.cuda.synchronize()\n",
        "\n",
        "                # Force garbage collection again after GPU cleanup\n",
        "                gc.collect()\n",
        "\n",
        "                # Log cleanup completion\n",
        "                self.logger.info(\"Resource cleanup completed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # Log cleanup errors but continue operation\n",
        "                self.logger.error(f\"Resource cleanup failed: {e}\")\n",
        "\n",
        "    def get_resource_summary(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get comprehensive resource usage summary and statistics.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing resource usage analysis and recommendations\n",
        "        \"\"\"\n",
        "        # Acquire lock to safely access resource history\n",
        "        with self._monitoring_lock:\n",
        "            if not self.resource_history:\n",
        "                return {'status': 'no_data', 'message': 'No monitoring data available'}\n",
        "\n",
        "            # Compute resource usage statistics from history\n",
        "            recent_stats = self.resource_history[-10:]  # Last 10 measurements\n",
        "\n",
        "            # Extract memory usage timeseries\n",
        "            memory_usage = [stats['memory']['rss'] for stats in recent_stats]\n",
        "            cpu_usage = [stats['cpu']['process_percent'] for stats in recent_stats]\n",
        "\n",
        "            # Compute statistical summaries\n",
        "            resource_summary = {\n",
        "                'monitoring_duration': len(self.resource_history) * self.monitoring_interval,\n",
        "                'sample_count': len(self.resource_history),\n",
        "                'memory': {\n",
        "                    'current_mb': memory_usage[-1] / 1024**2 if memory_usage else 0,\n",
        "                    'peak_mb': max(memory_usage) / 1024**2 if memory_usage else 0,\n",
        "                    'average_mb': statistics.mean(memory_usage) / 1024**2 if memory_usage else 0,\n",
        "                    'std_mb': statistics.stdev(memory_usage) / 1024**2 if len(memory_usage) > 1 else 0\n",
        "                },\n",
        "                'cpu': {\n",
        "                    'current_percent': cpu_usage[-1] if cpu_usage else 0,\n",
        "                    'peak_percent': max(cpu_usage) if cpu_usage else 0,\n",
        "                    'average_percent': statistics.mean(cpu_usage) if cpu_usage else 0,\n",
        "                    'std_percent': statistics.stdev(cpu_usage) if len(cpu_usage) > 1 else 0\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Add GPU statistics if available\n",
        "            if torch.cuda.is_available() and recent_stats:\n",
        "                gpu_memory = [stats['gpu']['memory_allocated'] for stats in recent_stats]\n",
        "                resource_summary['gpu'] = {\n",
        "                    'current_mb': gpu_memory[-1] / 1024**2 if gpu_memory else 0,\n",
        "                    'peak_mb': max(gpu_memory) / 1024**2 if gpu_memory else 0,\n",
        "                    'average_mb': statistics.mean(gpu_memory) / 1024**2 if gpu_memory else 0\n",
        "                }\n",
        "\n",
        "            # Add resource recommendations\n",
        "            resource_summary['recommendations'] = self._generate_resource_recommendations(resource_summary)\n",
        "\n",
        "            return resource_summary\n",
        "\n",
        "    def _generate_resource_recommendations(self, summary: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate resource optimization recommendations based on usage patterns.\n",
        "\n",
        "        Args:\n",
        "            summary: Resource usage summary statistics\n",
        "\n",
        "        Returns:\n",
        "            List of actionable resource optimization recommendations\n",
        "        \"\"\"\n",
        "        # Initialize recommendations list\n",
        "        recommendations = []\n",
        "\n",
        "        # Analyze memory usage patterns\n",
        "        if summary['memory']['peak_mb'] > 1000:  # > 1GB peak usage\n",
        "            recommendations.append(\"Consider reducing batch sizes or image resolution for memory optimization\")\n",
        "\n",
        "        if summary['memory']['std_mb'] > 100:  # High memory variance\n",
        "            recommendations.append(\"Memory usage is highly variable - consider implementing memory pooling\")\n",
        "\n",
        "        # Analyze CPU usage patterns\n",
        "        if summary['cpu']['average_percent'] > 80:  # High CPU usage\n",
        "            recommendations.append(\"High CPU utilization detected - consider parallel processing optimization\")\n",
        "\n",
        "        if summary['cpu']['std_percent'] > 20:  # High CPU variance\n",
        "            recommendations.append(\"CPU usage is irregular - consider load balancing improvements\")\n",
        "\n",
        "        # Analyze GPU usage if available\n",
        "        if 'gpu' in summary and summary['gpu']['peak_mb'] > 4000:  # > 4GB GPU usage\n",
        "            recommendations.append(\"High GPU memory usage - consider gradient checkpointing or model sharding\")\n",
        "\n",
        "        # Add general recommendations\n",
        "        if not recommendations:\n",
        "            recommendations.append(\"Resource usage appears optimal for current workload\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "\n",
        "class DefaultFeatureDetectorFactory:\n",
        "    \"\"\"\n",
        "    Production-grade factory for ORB feature detectors with mathematical optimization.\n",
        "\n",
        "    Implements systematic parameter tuning, performance profiling, and resource\n",
        "    management for optimal feature detection performance in quantitative applications.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        resource_manager: Optional[ResourceManager] = None,\n",
        "        enable_optimization: bool = True,\n",
        "        cache_size: int = 10\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize feature detector factory with optimization and caching.\n",
        "\n",
        "        Args:\n",
        "            resource_manager: Optional resource management system\n",
        "            enable_optimization: Whether to enable automatic parameter optimization\n",
        "            cache_size: Maximum number of detector instances to cache\n",
        "        \"\"\"\n",
        "        # Initialize resource management system\n",
        "        self.resource_manager = resource_manager or ResourceManager()\n",
        "\n",
        "        # Configure optimization and caching settings\n",
        "        self.enable_optimization = enable_optimization\n",
        "        self.cache_size = cache_size\n",
        "\n",
        "        # Initialize detector cache for performance optimization\n",
        "        self._detector_cache: Dict[str, cv2.ORB] = {}\n",
        "        self._cache_access_count: Dict[str, int] = defaultdict(int)\n",
        "        self._cache_lock = threading.Lock()\n",
        "\n",
        "        # Define mathematical constraints for ORB parameters\n",
        "        self.parameter_constraints = {\n",
        "            'nfeatures': ParameterConstraints(\n",
        "                name='nfeatures',\n",
        "                bounds=(100, 10000),\n",
        "                dtype=int,\n",
        "                optimization_objective='maximize',\n",
        "                performance_weight=0.3,\n",
        "                computational_cost=1.5\n",
        "            ),\n",
        "            'scaleFactor': ParameterConstraints(\n",
        "                name='scaleFactor',\n",
        "                bounds=(1.1, 2.0),\n",
        "                dtype=float,\n",
        "                optimization_objective='minimize',\n",
        "                performance_weight=0.2,\n",
        "                dependencies=['nlevels']\n",
        "            ),\n",
        "            'nlevels': ParameterConstraints(\n",
        "                name='nlevels',\n",
        "                bounds=(3, 12),\n",
        "                dtype=int,\n",
        "                optimization_objective='maximize',\n",
        "                performance_weight=0.15,\n",
        "                computational_cost=2.0\n",
        "            ),\n",
        "            'edgeThreshold': ParameterConstraints(\n",
        "                name='edgeThreshold',\n",
        "                bounds=(10, 50),\n",
        "                dtype=int,\n",
        "                optimization_objective='minimize',\n",
        "                performance_weight=0.1,\n",
        "                dependencies=['patchSize']\n",
        "            ),\n",
        "            'patchSize': ParameterConstraints(\n",
        "                name='patchSize',\n",
        "                bounds=(15, 63),\n",
        "                dtype=int,\n",
        "                optimization_objective='maximize',\n",
        "                performance_weight=0.15\n",
        "            ),\n",
        "            'fastThreshold': ParameterConstraints(\n",
        "                name='fastThreshold',\n",
        "                bounds=(5, 30),\n",
        "                dtype=int,\n",
        "                optimization_objective='minimize',\n",
        "                performance_weight=0.1\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Initialize parameter optimizer if optimization enabled\n",
        "        if self.enable_optimization:\n",
        "            self.optimizer = ParameterOptimizer(\n",
        "                constraints=self.parameter_constraints,\n",
        "                optimization_strategy=OptimizationStrategy.ADAPTIVE_TUNING\n",
        "            )\n",
        "\n",
        "        # Setup logging for factory operations\n",
        "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "\n",
        "    def create(\n",
        "        self,\n",
        "        nfeatures: int = 500,\n",
        "        scaleFactor: float = 1.2,\n",
        "        nlevels: int = 8,\n",
        "        edgeThreshold: int = 31,\n",
        "        firstLevel: int = 0,\n",
        "        WTA_K: int = 2,\n",
        "        scoreType: int = cv2.ORB_HARRIS_SCORE,\n",
        "        patchSize: int = 31,\n",
        "        fastThreshold: int = 20,\n",
        "        optimize_parameters: bool = False,\n",
        "        target_performance: Optional[str] = None\n",
        "    ) -> cv2.ORB:\n",
        "        \"\"\"\n",
        "        Create optimized ORB feature detector with mathematical parameter validation.\n",
        "\n",
        "        Implements the complete ORB algorithm with mathematical foundations:\n",
        "\n",
        "        1. FAST Corner Detection:\n",
        "           - Circle test: |I(p) - I(x_i)| > fastThreshold for i ∈ {1,2,...,16}\n",
        "           - Non-maximal suppression: local maxima in Harris corner response\n",
        "           - Sub-pixel refinement: quadratic interpolation for precision\n",
        "\n",
        "        2. Scale Pyramid Construction:\n",
        "           - Gaussian pyramid: L(x,y,σ) = G(x,y,σ) * I(x,y)\n",
        "           - Scale levels: σₖ = scaleFactor^k for k ∈ {0,1,...,nlevels-1}\n",
        "           - Computational complexity: O(n·m·nlevels) for n×m image\n",
        "\n",
        "        3. Orientation Assignment:\n",
        "           - Gradient computation: ∇I = [∂I/∂x, ∂I/∂y]\n",
        "           - Orientation histogram: weighted by gradient magnitude\n",
        "           - Dominant orientation: peak in 36-bin histogram\n",
        "\n",
        "        4. BRIEF Descriptor Computation:\n",
        "           - Binary tests: τ(p; x,y) := 1 if p(x) < p(y), else 0\n",
        "           - Rotation compensation: apply estimated orientation\n",
        "           - Descriptor length: 256 bits = 32 bytes\n",
        "\n",
        "        Mathematical Requirements:\n",
        "        - scaleFactor ∈ (1,2]: pyramid decimation ratio must exceed 1\n",
        "        - nlevels ∈ [3,12]: sufficient scales without excessive computation\n",
        "        - edgeThreshold < patchSize: border exclusion smaller than patch\n",
        "        - fastThreshold ∈ [5,30]: balance detection sensitivity vs noise\n",
        "\n",
        "        Args:\n",
        "            nfeatures: Maximum number of features to detect per image\n",
        "                      Mathematical constraint: sufficient for statistical analysis\n",
        "            scaleFactor: Pyramid decimation ratio between consecutive levels\n",
        "                        Mathematical requirement: scaleFactor > 1.0 for proper sampling\n",
        "            nlevels: Number of pyramid levels for scale invariance\n",
        "                    Mathematical trade-off: more levels vs computational cost\n",
        "            edgeThreshold: Size of border where features are not detected\n",
        "                          Mathematical constraint: edgeThreshold < min(width,height)/2\n",
        "            firstLevel: Pyramid level to put source image (typically 0)\n",
        "                       Mathematical meaning: initial scale σ₀ = scaleFactor^firstLevel\n",
        "            WTA_K: Number of points for oriented BRIEF descriptor (2 or 4)\n",
        "                  Mathematical impact: affects descriptor discriminability\n",
        "            scoreType: Algorithm for ranking features (Harris vs FAST)\n",
        "                      Mathematical difference: corner response computation method\n",
        "            patchSize: Size of patch used by oriented BRIEF descriptor\n",
        "                      Mathematical constraint: odd integer for symmetric patch\n",
        "            fastThreshold: FAST corner detection threshold\n",
        "                          Mathematical meaning: minimum intensity difference\n",
        "            optimize_parameters: Whether to optimize parameters for target performance\n",
        "            target_performance: Performance optimization target ('speed', 'accuracy', 'balanced')\n",
        "\n",
        "        Returns:\n",
        "            Configured ORB detector with mathematical guarantees:\n",
        "            - Feature count: up to nfeatures keypoints per image\n",
        "            - Scale invariance: features detected across nlevels scales\n",
        "            - Rotation invariance: orientation-compensated descriptors\n",
        "            - Descriptor length: 256-bit binary strings\n",
        "            - Hamming distance: efficient matching in {0,1}^256 space\n",
        "\n",
        "        Raises:\n",
        "            OpenCVInitializationError: If ORB creation fails due to invalid parameters\n",
        "            ResourceAllocationError: If insufficient memory for detector creation\n",
        "            ValueError: If parameter constraints are violated\n",
        "        \"\"\"\n",
        "        # Validate input parameters against mathematical constraints\n",
        "        parameters = {\n",
        "            'nfeatures': nfeatures,\n",
        "            'scaleFactor': scaleFactor,\n",
        "            'nlevels': nlevels,\n",
        "            'edgeThreshold': edgeThreshold,\n",
        "            'patchSize': patchSize,\n",
        "            'fastThreshold': fastThreshold\n",
        "        }\n",
        "\n",
        "        # Perform parameter validation if optimization enabled\n",
        "        if self.enable_optimization:\n",
        "            is_valid, violations = self.optimizer.validate_parameters(parameters)\n",
        "            if not is_valid:\n",
        "                raise ValueError(f\"Invalid ORB parameters: {violations}\")\n",
        "\n",
        "        # Optimize parameters if requested and optimization enabled\n",
        "        if optimize_parameters and self.enable_optimization:\n",
        "            # Define optimization objective function\n",
        "            def objective_function(params: Dict[str, Any]) -> PerformanceMetrics:\n",
        "                return self._evaluate_detector_performance(params, target_performance)\n",
        "\n",
        "            # Perform parameter optimization\n",
        "            optimal_params, _ = self.optimizer.optimize_parameters(\n",
        "                objective_function, parameters\n",
        "            )\n",
        "\n",
        "            # Update parameters with optimized values\n",
        "            nfeatures = optimal_params.get('nfeatures', nfeatures)\n",
        "            scaleFactor = optimal_params.get('scaleFactor', scaleFactor)\n",
        "            nlevels = optimal_params.get('nlevels', nlevels)\n",
        "            edgeThreshold = optimal_params.get('edgeThreshold', edgeThreshold)\n",
        "            patchSize = optimal_params.get('patchSize', patchSize)\n",
        "            fastThreshold = optimal_params.get('fastThreshold', fastThreshold)\n",
        "\n",
        "        # Generate cache key for detector configuration\n",
        "        cache_key = self._generate_cache_key(\n",
        "            nfeatures, scaleFactor, nlevels, edgeThreshold,\n",
        "            firstLevel, WTA_K, scoreType, patchSize, fastThreshold\n",
        "        )\n",
        "\n",
        "        # Check cache for existing detector instance\n",
        "        with self._cache_lock:\n",
        "            if cache_key in self._detector_cache:\n",
        "                # Increment access count and return cached detector\n",
        "                self._cache_access_count[cache_key] += 1\n",
        "                self.logger.debug(f\"Retrieved ORB detector from cache: {cache_key}\")\n",
        "                return self._detector_cache[cache_key]\n",
        "\n",
        "        # Create new ORB detector with comprehensive error handling\n",
        "        try:\n",
        "            # Validate scaleFactor mathematical constraint\n",
        "            if scaleFactor <= 1.0:\n",
        "                raise ValueError(f\"scaleFactor must be > 1.0, got {scaleFactor}\")\n",
        "\n",
        "            # Validate pyramid level mathematical constraint\n",
        "            if nlevels < 1:\n",
        "                raise ValueError(f\"nlevels must be >= 1, got {nlevels}\")\n",
        "\n",
        "            # Validate edge threshold vs patch size constraint\n",
        "            if edgeThreshold >= patchSize:\n",
        "                raise ValueError(f\"edgeThreshold ({edgeThreshold}) must be < patchSize ({patchSize})\")\n",
        "\n",
        "            # Validate patch size mathematical constraint (must be odd)\n",
        "            if patchSize % 2 == 0:\n",
        "                patchSize += 1  # Ensure odd patch size\n",
        "                self.logger.warning(f\"Adjusted patchSize to {patchSize} (must be odd)\")\n",
        "\n",
        "            # Record memory usage before detector creation\n",
        "            memory_before = psutil.Process().memory_info().rss\n",
        "\n",
        "            # Create ORB detector with validated parameters\n",
        "            detector = cv2.ORB_create(\n",
        "                nfeatures=nfeatures,\n",
        "                scaleFactor=scaleFactor,\n",
        "                nlevels=nlevels,\n",
        "                edgeThreshold=edgeThreshold,\n",
        "                firstLevel=firstLevel,\n",
        "                WTA_K=WTA_K,\n",
        "                scoreType=scoreType,\n",
        "                patchSize=patchSize,\n",
        "                fastThreshold=fastThreshold\n",
        "            )\n",
        "\n",
        "            # Validate detector creation success\n",
        "            if detector is None:\n",
        "                raise OpenCVInitializationError(\"ORB detector creation returned None\")\n",
        "\n",
        "            # Record memory usage after detector creation\n",
        "            memory_after = psutil.Process().memory_info().rss\n",
        "            memory_delta = memory_after - memory_before\n",
        "\n",
        "            # Log successful detector creation with memory usage\n",
        "            self.logger.info(\n",
        "                f\"Created ORB detector: features={nfeatures}, scale={scaleFactor}, \"\n",
        "                f\"levels={nlevels}, memory_delta={memory_delta/1024:.1f}KB\"\n",
        "            )\n",
        "\n",
        "            # Cache detector instance for reuse\n",
        "            with self._cache_lock:\n",
        "                # Implement LRU cache eviction if cache is full\n",
        "                if len(self._detector_cache) >= self.cache_size:\n",
        "                    # Find least recently used detector\n",
        "                    lru_key = min(self._cache_access_count.keys(),\n",
        "                                 key=lambda k: self._cache_access_count[k])\n",
        "                    # Remove LRU detector from cache\n",
        "                    del self._detector_cache[lru_key]\n",
        "                    del self._cache_access_count[lru_key]\n",
        "                    self.logger.debug(f\"Evicted LRU detector from cache: {lru_key}\")\n",
        "\n",
        "                # Add new detector to cache\n",
        "                self._detector_cache[cache_key] = detector\n",
        "                self._cache_access_count[cache_key] = 1\n",
        "\n",
        "            return detector\n",
        "\n",
        "        except cv2.error as e:\n",
        "            # Handle OpenCV-specific errors with detailed context\n",
        "            error_context = {\n",
        "                'opencv_error': str(e),\n",
        "                'parameters': parameters,\n",
        "                'memory_available': psutil.virtual_memory().available\n",
        "            }\n",
        "            raise OpenCVInitializationError(\n",
        "                f\"OpenCV ORB creation failed: {e}\",\n",
        "                algorithm_context=error_context\n",
        "            ) from e\n",
        "\n",
        "        except MemoryError as e:\n",
        "            # Handle memory allocation failures with resource context\n",
        "            memory_info = psutil.virtual_memory()\n",
        "            error_context = {\n",
        "                'memory_total': memory_info.total,\n",
        "                'memory_available': memory_info.available,\n",
        "                'memory_percent': memory_info.percent,\n",
        "                'requested_features': nfeatures\n",
        "            }\n",
        "            raise ResourceAllocationError(\n",
        "                f\"Insufficient memory for ORB detector: {e}\",\n",
        "                resource_type=\"memory\",\n",
        "                algorithm_context=error_context\n",
        "            ) from e\n",
        "\n",
        "    def _generate_cache_key(self, *args) -> str:\n",
        "        \"\"\"\n",
        "        Generate unique cache key for detector configuration.\n",
        "\n",
        "        Args:\n",
        "            *args: Detector configuration parameters\n",
        "\n",
        "        Returns:\n",
        "            Unique string key for cache storage\n",
        "        \"\"\"\n",
        "        # Convert parameters to string representation for hashing\n",
        "        param_str = \"_\".join(str(arg) for arg in args)\n",
        "\n",
        "        # Generate hash-based cache key for efficient lookup\n",
        "        import hashlib\n",
        "        cache_key = hashlib.md5(param_str.encode()).hexdigest()[:16]\n",
        "\n",
        "        return cache_key\n",
        "\n",
        "    def _evaluate_detector_performance(\n",
        "        self,\n",
        "        parameters: Dict[str, Any],\n",
        "        target_performance: Optional[str] = None\n",
        "    ) -> PerformanceMetrics:\n",
        "        \"\"\"\n",
        "        Evaluate detector performance for parameter optimization.\n",
        "\n",
        "        Args:\n",
        "            parameters: Detector parameter configuration\n",
        "            target_performance: Performance optimization target\n",
        "\n",
        "        Returns:\n",
        "            Performance metrics for optimization objective\n",
        "        \"\"\"\n",
        "        # Create test detector with specified parameters\n",
        "        try:\n",
        "            test_detector = cv2.ORB_create(\n",
        "                nfeatures=parameters['nfeatures'],\n",
        "                scaleFactor=parameters['scaleFactor'],\n",
        "                nlevels=parameters['nlevels'],\n",
        "                edgeThreshold=parameters['edgeThreshold'],\n",
        "                patchSize=parameters['patchSize'],\n",
        "                fastThreshold=parameters['fastThreshold']\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # Return poor performance metrics if detector creation fails\n",
        "            return PerformanceMetrics(\n",
        "                mean_execution_time=float('inf'),\n",
        "                detection_accuracy=0.0,\n",
        "                numerical_stability=0.0\n",
        "            )\n",
        "\n",
        "        # Generate test image for performance evaluation\n",
        "        test_image = np.random.randint(0, 256, (640, 480), dtype=np.uint8)\n",
        "\n",
        "        # Measure detection performance\n",
        "        execution_times = []\n",
        "        keypoint_counts = []\n",
        "\n",
        "        # Perform multiple detection runs for statistical analysis\n",
        "        for _ in range(5):\n",
        "            start_time = time.perf_counter()\n",
        "            keypoints, descriptors = test_detector.detectAndCompute(test_image, None)\n",
        "            execution_time = time.perf_counter() - start_time\n",
        "\n",
        "            execution_times.append(execution_time)\n",
        "            keypoint_counts.append(len(keypoints) if keypoints else 0)\n",
        "\n",
        "        # Compute performance metrics\n",
        "        performance = PerformanceMetrics(\n",
        "            mean_execution_time=statistics.mean(execution_times),\n",
        "            std_execution_time=statistics.stdev(execution_times) if len(execution_times) > 1 else 0.0,\n",
        "            detection_accuracy=statistics.mean(keypoint_counts) / parameters['nfeatures'],\n",
        "            numerical_stability=1.0 / (1.0 + statistics.stdev(keypoint_counts)) if len(keypoint_counts) > 1 else 1.0,\n",
        "            sample_count=len(execution_times)\n",
        "        )\n",
        "\n",
        "        # Adjust metrics based on target performance\n",
        "        if target_performance == 'speed':\n",
        "            # Prioritize execution speed over detection quality\n",
        "            performance.detection_accuracy *= 0.7  # Reduce accuracy weight\n",
        "        elif target_performance == 'accuracy':\n",
        "            # Prioritize detection quality over speed\n",
        "            performance.mean_execution_time *= 1.5  # Penalize slower execution\n",
        "\n",
        "        return performance\n",
        "\n",
        "    def clear_cache(self) -> None:\n",
        "        \"\"\"Clear detector cache to free memory resources.\"\"\"\n",
        "        # Acquire cache lock for thread-safe operation\n",
        "        with self._cache_lock:\n",
        "            # Clear cache dictionaries\n",
        "            self._detector_cache.clear()\n",
        "            self._cache_access_count.clear()\n",
        "\n",
        "            # Log cache clearing operation\n",
        "            self.logger.info(\"Detector cache cleared\")\n",
        "\n",
        "    def get_cache_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get detector cache usage statistics.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing cache performance metrics\n",
        "        \"\"\"\n",
        "        # Acquire cache lock for thread-safe access\n",
        "        with self._cache_lock:\n",
        "            # Compute cache statistics\n",
        "            cache_stats = {\n",
        "                'cache_size': len(self._detector_cache),\n",
        "                'max_cache_size': self.cache_size,\n",
        "                'total_access_count': sum(self._cache_access_count.values()),\n",
        "                'cache_utilization': len(self._detector_cache) / self.cache_size if self.cache_size > 0 else 0.0\n",
        "            }\n",
        "\n",
        "            # Add access pattern analysis if cache has entries\n",
        "            if self._cache_access_count:\n",
        "                access_counts = list(self._cache_access_count.values())\n",
        "                cache_stats.update({\n",
        "                    'mean_access_count': statistics.mean(access_counts),\n",
        "                    'max_access_count': max(access_counts),\n",
        "                    'min_access_count': min(access_counts)\n",
        "                })\n",
        "\n",
        "            return cache_stats\n",
        "\n",
        "\n",
        "class DefaultMatcherFactory:\n",
        "    \"\"\"\n",
        "    Production-grade factory for BFMatcher instances with optimization and profiling.\n",
        "\n",
        "    Implements systematic matcher configuration, performance analysis, and\n",
        "    resource optimization for high-throughput matching applications.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        resource_manager: Optional[ResourceManager] = None,\n",
        "        enable_profiling: bool = True\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize matcher factory with performance profiling capabilities.\n",
        "\n",
        "        Args:\n",
        "            resource_manager: Optional resource management system\n",
        "            enable_profiling: Whether to enable matcher performance profiling\n",
        "        \"\"\"\n",
        "        # Initialize resource management system\n",
        "        self.resource_manager = resource_manager or ResourceManager()\n",
        "        self.enable_profiling = enable_profiling\n",
        "\n",
        "        # Initialize performance tracking\n",
        "        self.performance_history: List[Dict[str, Any]] = []\n",
        "        self._profiling_lock = threading.Lock()\n",
        "\n",
        "        # Setup logging for factory operations\n",
        "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "\n",
        "    def create(\n",
        "        self,\n",
        "        normType: int = cv2.NORM_HAMMING,\n",
        "        crossCheck: bool = True,\n",
        "        optimize_for_throughput: bool = False\n",
        "    ) -> cv2.BFMatcher:\n",
        "        \"\"\"\n",
        "        Create optimized BFMatcher with mathematical validation and profiling.\n",
        "\n",
        "        Implements brute-force matching in Hamming space with mathematical foundations:\n",
        "\n",
        "        1. Hamming Distance Computation:\n",
        "           - Binary XOR: d_H(x,y) = popcount(x ⊕ y) for binary descriptors\n",
        "           - L2 Distance: d_L2(x,y) = √(Σᵢ(xᵢ-yᵢ)²) for floating-point descriptors\n",
        "           - L1 Distance: d_L1(x,y) = Σᵢ|xᵢ-yᵢ| for Manhattan distance\n",
        "\n",
        "        2. Brute-Force Search Algorithm:\n",
        "           - Exhaustive comparison: O(n·m·k) where n,m are descriptor counts, k is length\n",
        "           - Optimal assignment: find global minimum distance for each query\n",
        "           - Memory complexity: O(n·m) for distance matrix storage\n",
        "\n",
        "        3. Cross-Check Validation:\n",
        "           - Bidirectional consistency: match(qᵢ,tⱼ) iff match(tⱼ,qᵢ)\n",
        "           - False positive reduction: P(false_positive) ≈ 1/m with cross-check\n",
        "           - Precision improvement: higher confidence at cost of recall\n",
        "\n",
        "        Mathematical Properties:\n",
        "        - Metric space: Hamming distance satisfies triangle inequality\n",
        "        - Optimality: brute-force guarantees global optimum\n",
        "        - Symmetry: cross-check ensures symmetric matching relation\n",
        "\n",
        "        Args:\n",
        "            normType: Distance norm type for descriptor comparison\n",
        "                     CV_NORM_HAMMING: binary descriptors (ORB, BRIEF)\n",
        "                     CV_NORM_L2: floating-point descriptors (SIFT, SURF)\n",
        "                     CV_NORM_L1: Manhattan distance for specific applications\n",
        "            crossCheck: Enable bidirectional consistency validation\n",
        "                       Mathematical effect: improves precision, reduces recall\n",
        "            optimize_for_throughput: Configure for high-throughput applications\n",
        "                                   Trade-off: may sacrifice match quality for speed\n",
        "\n",
        "        Returns:\n",
        "            Configured BFMatcher with mathematical guarantees:\n",
        "            - Optimal matching: globally minimal distance assignments\n",
        "            - Metric compliance: distance function satisfies metric properties\n",
        "            - Cross-check consistency: bidirectional match validation if enabled\n",
        "            - Performance optimization: configured for target application\n",
        "\n",
        "        Raises:\n",
        "            OpenCVInitializationError: If matcher creation fails\n",
        "            ValueError: If normType is invalid for intended descriptor type\n",
        "        \"\"\"\n",
        "        # Validate norm type parameter against mathematical requirements\n",
        "        valid_norm_types = {\n",
        "            cv2.NORM_HAMMING,     # Binary descriptors (ORB, BRIEF)\n",
        "            cv2.NORM_HAMMING2,    # Multi-byte binary descriptors\n",
        "            cv2.NORM_L1,          # Manhattan distance\n",
        "            cv2.NORM_L2,          # Euclidean distance\n",
        "            cv2.NORM_L2SQR        # Squared Euclidean distance\n",
        "        }\n",
        "\n",
        "        if normType not in valid_norm_types:\n",
        "            raise ValueError(f\"Invalid normType: {normType}. Valid options: {valid_norm_types}\")\n",
        "\n",
        "        # Apply throughput optimizations if requested\n",
        "        if optimize_for_throughput:\n",
        "            # Disable cross-check for faster matching (trade precision for speed)\n",
        "            crossCheck = False\n",
        "            self.logger.info(\"Disabled cross-check for throughput optimization\")\n",
        "\n",
        "        # Record matcher creation start time for profiling\n",
        "        creation_start_time = time.perf_counter()\n",
        "        memory_before = psutil.Process().memory_info().rss\n",
        "\n",
        "        try:\n",
        "            # Create BFMatcher with specified configuration\n",
        "            matcher = cv2.BFMatcher(normType=normType, crossCheck=crossCheck)\n",
        "\n",
        "            # Validate matcher creation success\n",
        "            if matcher is None:\n",
        "                raise OpenCVInitializationError(\"BFMatcher creation returned None\")\n",
        "\n",
        "            # Record creation performance metrics\n",
        "            creation_time = time.perf_counter() - creation_start_time\n",
        "            memory_after = psutil.Process().memory_info().rss\n",
        "            memory_delta = memory_after - memory_before\n",
        "\n",
        "            # Store performance metrics if profiling enabled\n",
        "            if self.enable_profiling:\n",
        "                performance_record = {\n",
        "                    'timestamp': time.time(),\n",
        "                    'creation_time': creation_time,\n",
        "                    'memory_delta': memory_delta,\n",
        "                    'norm_type': normType,\n",
        "                    'cross_check': crossCheck,\n",
        "                    'optimized_for_throughput': optimize_for_throughput\n",
        "                }\n",
        "\n",
        "                # Thread-safe performance history update\n",
        "                with self._profiling_lock:\n",
        "                    self.performance_history.append(performance_record)\n",
        "                    # Limit history size to prevent memory growth\n",
        "                    if len(self.performance_history) > 1000:\n",
        "                        self.performance_history = self.performance_history[-500:]\n",
        "\n",
        "            # Log successful matcher creation\n",
        "            self.logger.info(\n",
        "                f\"Created BFMatcher: norm={normType}, cross_check={crossCheck}, \"\n",
        "                f\"creation_time={creation_time*1000:.2f}ms, memory_delta={memory_delta/1024:.1f}KB\"\n",
        "            )\n",
        "\n",
        "            return matcher\n",
        "\n",
        "        except cv2.error as e:\n",
        "            # Handle OpenCV-specific errors with detailed context\n",
        "            error_context = {\n",
        "                'opencv_error': str(e),\n",
        "                'norm_type': normType,\n",
        "                'cross_check': crossCheck,\n",
        "                'memory_available': psutil.virtual_memory().available\n",
        "            }\n",
        "            raise OpenCVInitializationError(\n",
        "                f\"OpenCV BFMatcher creation failed: {e}\",\n",
        "                algorithm_context=error_context\n",
        "            ) from e\n",
        "\n",
        "    def benchmark_matcher_performance(\n",
        "        self,\n",
        "        descriptor_sizes: List[Tuple[int, int]] = [(100, 100), (500, 500), (1000, 1000)],\n",
        "        norm_types: List[int] = [cv2.NORM_HAMMING, cv2.NORM_L2],\n",
        "        cross_check_options: List[bool] = [True, False]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Comprehensive matcher performance benchmarking across configurations.\n",
        "\n",
        "        Args:\n",
        "            descriptor_sizes: List of (query_count, train_count) tuples for testing\n",
        "            norm_types: List of norm types to benchmark\n",
        "            cross_check_options: List of cross-check settings to test\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing comprehensive benchmark results\n",
        "        \"\"\"\n",
        "        # Initialize benchmark results storage\n",
        "        benchmark_results = {\n",
        "            'configurations': [],\n",
        "            'performance_matrix': {},\n",
        "            'recommendations': []\n",
        "        }\n",
        "\n",
        "        # Log benchmark start\n",
        "        self.logger.info(f\"Starting matcher performance benchmark with {len(descriptor_sizes)} size configurations\")\n",
        "\n",
        "        # Benchmark each configuration combination\n",
        "        for query_count, train_count in descriptor_sizes:\n",
        "            for norm_type in norm_types:\n",
        "                for cross_check in cross_check_options:\n",
        "                    # Create test configuration\n",
        "                    config_name = f\"q{query_count}_t{train_count}_n{norm_type}_x{cross_check}\"\n",
        "\n",
        "                    try:\n",
        "                        # Benchmark this configuration\n",
        "                        config_performance = self._benchmark_single_configuration(\n",
        "                            query_count, train_count, norm_type, cross_check\n",
        "                        )\n",
        "\n",
        "                        # Store configuration results\n",
        "                        benchmark_results['configurations'].append({\n",
        "                            'name': config_name,\n",
        "                            'query_count': query_count,\n",
        "                            'train_count': train_count,\n",
        "                            'norm_type': norm_type,\n",
        "                            'cross_check': cross_check,\n",
        "                            'performance': config_performance\n",
        "                        })\n",
        "\n",
        "                        # Store in performance matrix for analysis\n",
        "                        benchmark_results['performance_matrix'][config_name] = config_performance\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # Log benchmark failures but continue with other configurations\n",
        "                        self.logger.error(f\"Benchmark failed for {config_name}: {e}\")\n",
        "\n",
        "        # Generate performance recommendations\n",
        "        benchmark_results['recommendations'] = self._generate_performance_recommendations(\n",
        "            benchmark_results['configurations']\n",
        "        )\n",
        "\n",
        "        # Log benchmark completion\n",
        "        self.logger.info(f\"Matcher benchmark completed: {len(benchmark_results['configurations'])} configurations tested\")\n",
        "\n",
        "        return benchmark_results\n",
        "\n",
        "    def _benchmark_single_configuration(\n",
        "        self,\n",
        "        query_count: int,\n",
        "        train_count: int,\n",
        "        norm_type: int,\n",
        "        cross_check: bool\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Benchmark single matcher configuration with synthetic data.\n",
        "\n",
        "        Args:\n",
        "            query_count: Number of query descriptors\n",
        "            train_count: Number of train descriptors\n",
        "            norm_type: Distance norm type\n",
        "            cross_check: Cross-check validation setting\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing performance metrics\n",
        "        \"\"\"\n",
        "        # Create matcher for benchmarking\n",
        "        matcher = self.create(norm_type, cross_check)\n",
        "\n",
        "        # Generate synthetic test data based on norm type\n",
        "        if norm_type in [cv2.NORM_HAMMING, cv2.NORM_HAMMING2]:\n",
        "            # Binary descriptors for Hamming distance\n",
        "            query_descriptors = np.random.randint(0, 256, (query_count, 32), dtype=np.uint8)\n",
        "            train_descriptors = np.random.randint(0, 256, (train_count, 32), dtype=np.uint8)\n",
        "        else:\n",
        "            # Floating-point descriptors for L2/L1 distance\n",
        "            query_descriptors = np.random.randn(query_count, 128).astype(np.float32)\n",
        "            train_descriptors = np.random.randn(train_count, 128).astype(np.float32)\n",
        "\n",
        "        # Benchmark matching performance with multiple runs\n",
        "        execution_times = []\n",
        "        memory_usages = []\n",
        "        match_counts = []\n",
        "\n",
        "        # Perform multiple benchmark runs for statistical analysis\n",
        "        for run_idx in range(5):\n",
        "            # Record memory before matching\n",
        "            memory_before = psutil.Process().memory_info().rss\n",
        "\n",
        "            # Measure matching execution time\n",
        "            start_time = time.perf_counter()\n",
        "            matches = matcher.match(query_descriptors, train_descriptors)\n",
        "            execution_time = time.perf_counter() - start_time\n",
        "\n",
        "            # Record memory after matching\n",
        "            memory_after = psutil.Process().memory_info().rss\n",
        "            memory_usage = memory_after - memory_before\n",
        "\n",
        "            # Store performance measurements\n",
        "            execution_times.append(execution_time)\n",
        "            memory_usages.append(memory_usage)\n",
        "            match_counts.append(len(matches))\n",
        "\n",
        "        # Compute performance statistics\n",
        "        performance_metrics = {\n",
        "            'mean_execution_time': statistics.mean(execution_times),\n",
        "            'std_execution_time': statistics.stdev(execution_times) if len(execution_times) > 1 else 0.0,\n",
        "            'min_execution_time': min(execution_times),\n",
        "            'max_execution_time': max(execution_times),\n",
        "            'mean_memory_usage': statistics.mean(memory_usages),\n",
        "            'mean_match_count': statistics.mean(match_counts),\n",
        "            'throughput_matches_per_second': statistics.mean(match_counts) / statistics.mean(execution_times),\n",
        "            'memory_efficiency': statistics.mean(match_counts) / max(1, statistics.mean(memory_usages) / 1024)  # matches per KB\n",
        "        }\n",
        "\n",
        "        return performance_metrics\n",
        "\n",
        "    def _generate_performance_recommendations(\n",
        "        self,\n",
        "        configurations: List[Dict[str, Any]]\n",
        "    ) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate performance optimization recommendations based on benchmark results.\n",
        "\n",
        "        Args:\n",
        "            configurations: List of benchmarked configuration results\n",
        "\n",
        "        Returns:\n",
        "            List of actionable performance recommendations\n",
        "        \"\"\"\n",
        "        # Initialize recommendations list\n",
        "        recommendations = []\n",
        "\n",
        "        # Analyze configurations if data available\n",
        "        if not configurations:\n",
        "            recommendations.append(\"No benchmark data available for analysis\")\n",
        "            return recommendations\n",
        "\n",
        "        # Find best performing configurations for different metrics\n",
        "        best_speed = min(configurations, key=lambda c: c['performance']['mean_execution_time'])\n",
        "        best_memory = min(configurations, key=lambda c: c['performance']['mean_memory_usage'])\n",
        "        best_throughput = max(configurations, key=lambda c: c['performance']['throughput_matches_per_second'])\n",
        "\n",
        "        # Generate speed optimization recommendations\n",
        "        recommendations.append(\n",
        "            f\"For fastest matching: use configuration {best_speed['name']} \"\n",
        "            f\"(execution time: {best_speed['performance']['mean_execution_time']*1000:.2f}ms)\"\n",
        "        )\n",
        "\n",
        "        # Generate memory optimization recommendations\n",
        "        recommendations.append(\n",
        "            f\"For lowest memory usage: use configuration {best_memory['name']} \"\n",
        "            f\"(memory: {best_memory['performance']['mean_memory_usage']/1024:.1f}KB)\"\n",
        "        )\n",
        "\n",
        "        # Generate throughput optimization recommendations\n",
        "        recommendations.append(\n",
        "            f\"For highest throughput: use configuration {best_throughput['name']} \"\n",
        "            f\"(throughput: {best_throughput['performance']['throughput_matches_per_second']:.1f} matches/sec)\"\n",
        "        )\n",
        "\n",
        "        # Analyze cross-check impact\n",
        "        cross_check_configs = [c for c in configurations if c['cross_check']]\n",
        "        no_cross_check_configs = [c for c in configurations if not c['cross_check']]\n",
        "\n",
        "        if cross_check_configs and no_cross_check_configs:\n",
        "            avg_speed_with_cross_check = statistics.mean([c['performance']['mean_execution_time'] for c in cross_check_configs])\n",
        "            avg_speed_without_cross_check = statistics.mean([c['performance']['mean_execution_time'] for c in no_cross_check_configs])\n",
        "\n",
        "            speed_improvement = (avg_speed_with_cross_check - avg_speed_without_cross_check) / avg_speed_with_cross_check * 100\n",
        "\n",
        "            if speed_improvement > 10:\n",
        "                recommendations.append(\n",
        "                    f\"Disabling cross-check improves speed by {speed_improvement:.1f}% \"\n",
        "                    f\"but may reduce match precision\"\n",
        "                )\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def get_performance_summary(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get comprehensive performance summary from profiling history.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing performance analysis and trends\n",
        "        \"\"\"\n",
        "        # Acquire profiling lock for thread-safe access\n",
        "        with self._profiling_lock:\n",
        "            if not self.performance_history:\n",
        "                return {'status': 'no_data', 'message': 'No profiling data available'}\n",
        "\n",
        "            # Extract performance metrics from history\n",
        "            creation_times = [record['creation_time'] for record in self.performance_history]\n",
        "            memory_deltas = [record['memory_delta'] for record in self.performance_history]\n",
        "\n",
        "            # Compute statistical summary\n",
        "            performance_summary = {\n",
        "                'total_matchers_created': len(self.performance_history),\n",
        "                'creation_time': {\n",
        "                    'mean_ms': statistics.mean(creation_times) * 1000,\n",
        "                    'std_ms': statistics.stdev(creation_times) * 1000 if len(creation_times) > 1 else 0.0,\n",
        "                    'min_ms': min(creation_times) * 1000,\n",
        "                    'max_ms': max(creation_times) * 1000\n",
        "                },\n",
        "                'memory_usage': {\n",
        "                    'mean_kb': statistics.mean(memory_deltas) / 1024,\n",
        "                    'std_kb': statistics.stdev(memory_deltas) / 1024 if len(memory_deltas) > 1 else 0.0,\n",
        "                    'total_kb': sum(memory_deltas) / 1024\n",
        "                },\n",
        "                'configuration_analysis': self._analyze_configuration_trends()\n",
        "            }\n",
        "\n",
        "            return performance_summary\n",
        "\n",
        "    def _analyze_configuration_trends(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze configuration usage trends from performance history.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing configuration usage analysis\n",
        "        \"\"\"\n",
        "        # Count configuration usage patterns\n",
        "        norm_type_counts = defaultdict(int)\n",
        "        cross_check_counts = defaultdict(int)\n",
        "        throughput_opt_counts = defaultdict(int)\n",
        "\n",
        "        # Analyze configuration patterns in history\n",
        "        for record in self.performance_history:\n",
        "            norm_type_counts[record['norm_type']] += 1\n",
        "            cross_check_counts[record['cross_check']] += 1\n",
        "            throughput_opt_counts[record['optimized_for_throughput']] += 1\n",
        "\n",
        "        # Generate configuration analysis\n",
        "        configuration_analysis = {\n",
        "            'most_used_norm_type': max(norm_type_counts.keys(), key=lambda k: norm_type_counts[k]) if norm_type_counts else None,\n",
        "            'cross_check_usage_percent': (cross_check_counts[True] / len(self.performance_history)) * 100 if self.performance_history else 0,\n",
        "            'throughput_optimization_percent': (throughput_opt_counts[True] / len(self.performance_history)) * 100 if self.performance_history else 0,\n",
        "            'norm_type_distribution': dict(norm_type_counts),\n",
        "            'configuration_diversity': len(set((r['norm_type'], r['cross_check']) for r in self.performance_history))\n",
        "        }\n",
        "\n",
        "        return configuration_analysis\n",
        "\n",
        "\n",
        "class DefaultClipModelLoader:\n",
        "    \"\"\"\n",
        "    Enterprise-grade CLIP model loader with advanced optimization and monitoring.\n",
        "\n",
        "    Implements sophisticated model lifecycle management, performance optimization,\n",
        "    and resource utilization tracking for production deployment environments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        resource_manager: Optional[ResourceManager] = None,\n",
        "        enable_model_caching: bool = True,\n",
        "        cache_directory: Optional[Path] = None,\n",
        "        enable_performance_monitoring: bool = True\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize CLIP model loader with enterprise-grade capabilities.\n",
        "\n",
        "        Args:\n",
        "            resource_manager: Optional resource management system\n",
        "            enable_model_caching: Whether to enable model caching for reuse\n",
        "            cache_directory: Directory for model cache storage\n",
        "            enable_performance_monitoring: Whether to monitor loading performance\n",
        "        \"\"\"\n",
        "        # Initialize resource management system\n",
        "        self.resource_manager = resource_manager or ResourceManager()\n",
        "\n",
        "        # Configure model caching system\n",
        "        self.enable_model_caching = enable_model_caching\n",
        "        self.cache_directory = cache_directory or Path.home() / \".clip_cache\"\n",
        "\n",
        "        # Initialize performance monitoring\n",
        "        self.enable_performance_monitoring = enable_performance_monitoring\n",
        "        self.loading_history: List[Dict[str, Any]] = []\n",
        "        self._monitoring_lock = threading.Lock()\n",
        "\n",
        "        # Initialize model cache for loaded models\n",
        "        self._model_cache: Dict[str, Tuple[torch.nn.Module, Callable]] = {}\n",
        "        self._cache_lock = threading.Lock()\n",
        "\n",
        "        # Setup logging for loader operations\n",
        "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "\n",
        "        # Create cache directory if caching enabled\n",
        "        if self.enable_model_caching:\n",
        "            self.cache_directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        device: str,\n",
        "        enable_optimization: bool = True,\n",
        "        precision: str = \"float32\"\n",
        "    ) -> Tuple[torch.nn.Module, Callable[[Any], torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Load CLIP model with comprehensive optimization and monitoring.\n",
        "\n",
        "        Implements enterprise-grade model loading with mathematical foundations:\n",
        "\n",
        "        1. Vision Transformer Architecture:\n",
        "           - Multi-head attention: Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
        "           - Layer normalization: LN(x) = γ(x-μ)/σ + β\n",
        "           - Position embeddings: learnable spatial position encoding\n",
        "           - Patch embeddings: linear projection of image patches\n",
        "\n",
        "        2. Contrastive Learning Framework:\n",
        "           - Joint embedding space: f_v: Images → ℝ^d, f_t: Text → ℝ^d\n",
        "           - Temperature scaling: sim = (f_v(I)·f_t(T))/(τ·||f_v(I)||·||f_t(T)||)\n",
        "           - InfoNCE loss: ℒ = -log(exp(sim_pos/τ)/Σⱼexp(sim_j/τ))\n",
        "\n",
        "        3. Model Optimization:\n",
        "           - Mixed precision: float16 computation, float32 accumulation\n",
        "           - Memory optimization: gradient checkpointing, activation offloading\n",
        "           - Device optimization: optimal tensor placement and data movement\n",
        "\n",
        "        Performance Characteristics:\n",
        "        - ViT-B/32: ~151M parameters, 512-dim embeddings, 224×224 input\n",
        "        - ViT-L/14: ~428M parameters, 768-dim embeddings, 224×224 input\n",
        "        - Memory: 2-8GB GPU depending on model size and batch size\n",
        "        - Inference: 10-50ms per image depending on hardware\n",
        "\n",
        "        Args:\n",
        "            model_name: CLIP model variant identifier\n",
        "                       Supported: \"ViT-B/32\", \"ViT-B/16\", \"ViT-L/14\", \"RN50\", \"RN101\"\n",
        "                       Mathematical specs:\n",
        "                       - ViT-B/32: 12 layers, 768 hidden, 32×32 patches\n",
        "                       - ViT-L/14: 24 layers, 1024 hidden, 14×14 patches\n",
        "            device: Target computational device (\"cpu\", \"cuda\", \"cuda:N\", \"mps\")\n",
        "                   Mathematical consideration: affects precision and memory layout\n",
        "            enable_optimization: Whether to apply performance optimizations\n",
        "                               Trade-off: memory usage vs inference speed\n",
        "            precision: Numerical precision (\"float32\", \"float16\", \"bfloat16\")\n",
        "                      Mathematical impact: accuracy vs memory/speed trade-off\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing optimized model and preprocessing function:\n",
        "            - model: torch.nn.Module with mathematical properties:\n",
        "              * encode_image: ℝ^(B×3×H×W) → ℝ^(B×d_embed)\n",
        "              * encode_text: ℝ^(B×L) → ℝ^(B×d_embed)\n",
        "              * L2-normalized outputs: ||embed||₂ = 1\n",
        "            - preprocess: Callable implementing mathematical transformations:\n",
        "              * Resize: bilinear interpolation to target resolution\n",
        "              * Normalize: (x - μ)/σ with ImageNet statistics\n",
        "              * Tensor conversion: PIL/numpy → torch.Tensor\n",
        "\n",
        "        Raises:\n",
        "            ModelLoadError: If model loading fails with detailed context\n",
        "            ValueError: If model_name or device specification is invalid\n",
        "            RuntimeError: If insufficient resources or device incompatibility\n",
        "        \"\"\"\n",
        "        # Validate model name against supported variants\n",
        "        supported_models = {\n",
        "            \"ViT-B/32\": {\"layers\": 12, \"hidden\": 768, \"patch_size\": 32, \"params\": 151e6},\n",
        "            \"ViT-B/16\": {\"layers\": 12, \"hidden\": 768, \"patch_size\": 16, \"params\": 149e6},\n",
        "            \"ViT-L/14\": {\"layers\": 24, \"hidden\": 1024, \"patch_size\": 14, \"params\": 428e6},\n",
        "            \"RN50\": {\"architecture\": \"ResNet50\", \"params\": 102e6},\n",
        "            \"RN101\": {\"architecture\": \"ResNet101\", \"params\": 119e6}\n",
        "        }\n",
        "\n",
        "        if model_name not in supported_models:\n",
        "            raise ValueError(f\"Unsupported model: {model_name}. Supported: {list(supported_models.keys())}\")\n",
        "\n",
        "        # Validate device specification\n",
        "        if not self._validate_device(device):\n",
        "            raise ValueError(f\"Invalid device specification: {device}\")\n",
        "\n",
        "        # Generate cache key for model configuration\n",
        "        cache_key = f\"{model_name}_{device}_{precision}_{enable_optimization}\"\n",
        "\n",
        "        # Check model cache for existing instance\n",
        "        with self._cache_lock:\n",
        "            if cache_key in self._model_cache:\n",
        "                # Return cached model instance\n",
        "                self.logger.debug(f\"Retrieved CLIP model from cache: {cache_key}\")\n",
        "                return self._model_cache[cache_key]\n",
        "\n",
        "        # Record loading start time and resource state\n",
        "        loading_start_time = time.perf_counter()\n",
        "        memory_before = psutil.Process().memory_info().rss\n",
        "        gpu_memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "        try:\n",
        "            # Load model with comprehensive error handling\n",
        "            self.logger.info(f\"Loading CLIP model: {model_name} on {device}\")\n",
        "\n",
        "            # Load base model and preprocessing function\n",
        "            model, preprocess = clip.load(model_name, device=device, jit=False)\n",
        "\n",
        "            # Apply optimization if requested\n",
        "            if enable_optimization:\n",
        "                model = self._optimize_model(model, device, precision)\n",
        "\n",
        "            # Apply precision conversion if specified\n",
        "            if precision == \"float16\" and torch.cuda.is_available():\n",
        "                model = model.half()\n",
        "            elif precision == \"bfloat16\" and torch.cuda.is_available():\n",
        "                model = model.bfloat16()\n",
        "\n",
        "            # Validate model loading success\n",
        "            if model is None or preprocess is None:\n",
        "                raise ModelLoadError(\"CLIP model or preprocess function is None\")\n",
        "\n",
        "            # Record loading performance metrics\n",
        "            loading_time = time.perf_counter() - loading_start_time\n",
        "            memory_after = psutil.Process().memory_info().rss\n",
        "            gpu_memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "            memory_delta = memory_after - memory_before\n",
        "            gpu_memory_delta = gpu_memory_after - gpu_memory_before\n",
        "\n",
        "            # Store performance metrics if monitoring enabled\n",
        "            if self.enable_performance_monitoring:\n",
        "                performance_record = {\n",
        "                    'timestamp': time.time(),\n",
        "                    'model_name': model_name,\n",
        "                    'device': device,\n",
        "                    'precision': precision,\n",
        "                    'optimization_enabled': enable_optimization,\n",
        "                    'loading_time': loading_time,\n",
        "                    'memory_delta': memory_delta,\n",
        "                    'gpu_memory_delta': gpu_memory_delta,\n",
        "                    'model_parameters': supported_models[model_name].get('params', 0)\n",
        "                }\n",
        "\n",
        "                # Thread-safe monitoring history update\n",
        "                with self._monitoring_lock:\n",
        "                    self.loading_history.append(performance_record)\n",
        "                    # Limit history size to prevent memory growth\n",
        "                    if len(self.loading_history) > 100:\n",
        "                        self.loading_history = self.loading_history[-50:]\n",
        "\n",
        "            # Cache model instance for reuse\n",
        "            with self._cache_lock:\n",
        "                self._model_cache[cache_key] = (model, preprocess)\n",
        "\n",
        "            # Log successful model loading with performance metrics\n",
        "            self.logger.info(\n",
        "                f\"CLIP model loaded successfully: {model_name}, \"\n",
        "                f\"loading_time={loading_time:.2f}s, \"\n",
        "                f\"memory_delta={memory_delta/1024/1024:.1f}MB, \"\n",
        "                f\"gpu_memory_delta={gpu_memory_delta/1024/1024:.1f}MB\"\n",
        "            )\n",
        "\n",
        "            return model, preprocess\n",
        "\n",
        "        except (OSError, RuntimeError, ModuleNotFoundError) as e:\n",
        "            # Handle model loading failures with comprehensive context\n",
        "            error_context = {\n",
        "                'model_name': model_name,\n",
        "                'device': device,\n",
        "                'precision': precision,\n",
        "                'optimization_enabled': enable_optimization,\n",
        "                'loading_time': time.perf_counter() - loading_start_time,\n",
        "                'available_memory': psutil.virtual_memory().available,\n",
        "                'gpu_available': torch.cuda.is_available(),\n",
        "                'gpu_memory_available': torch.cuda.get_device_properties(0).total_memory if torch.cuda.is_available() else 0\n",
        "            }\n",
        "\n",
        "            raise ModelLoadError(\n",
        "                f\"CLIP model loading failed for {model_name} on {device}: {e}\",\n",
        "                model_name=model_name,\n",
        "                algorithm_context=error_context\n",
        "            ) from e\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError as e:\n",
        "            # Handle GPU memory exhaustion with specific context\n",
        "            gpu_memory_info = {}\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_memory_info = {\n",
        "                    'allocated': torch.cuda.memory_allocated(),\n",
        "                    'cached': torch.cuda.memory_reserved(),\n",
        "                    'max_allocated': torch.cuda.max_memory_allocated(),\n",
        "                    'total_memory': torch.cuda.get_device_properties(0).total_memory\n",
        "                }\n",
        "\n",
        "            error_context = {\n",
        "                'model_name': model_name,\n",
        "                'device': device,\n",
        "                'precision': precision,\n",
        "                'gpu_memory_info': gpu_memory_info,\n",
        "                'suggested_solutions': [\n",
        "                    'Reduce batch size',\n",
        "                    'Use float16 precision',\n",
        "                    'Enable gradient checkpointing',\n",
        "                    'Use CPU device'\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            raise ModelLoadError(\n",
        "                f\"GPU out of memory loading {model_name}: {e}\",\n",
        "                model_name=model_name,\n",
        "                algorithm_context=error_context\n",
        "            ) from e\n",
        "\n",
        "    def _validate_device(self, device: str) -> bool:\n",
        "        \"\"\"\n",
        "        Validate device specification against available hardware.\n",
        "\n",
        "        Args:\n",
        "            device: Device specification string\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating whether device is valid and available\n",
        "        \"\"\"\n",
        "        # Validate CPU device\n",
        "        if device == \"cpu\":\n",
        "            return True\n",
        "\n",
        "        # Validate CUDA devices\n",
        "        if device.startswith(\"cuda\"):\n",
        "            if not torch.cuda.is_available():\n",
        "                return False\n",
        "\n",
        "            # Parse device index if specified\n",
        "            if \":\" in device:\n",
        "                try:\n",
        "                    device_index = int(device.split(\":\")[1])\n",
        "                    return device_index < torch.cuda.device_count()\n",
        "                except (ValueError, IndexError):\n",
        "                    return False\n",
        "\n",
        "            return True  # Generic \"cuda\" device\n",
        "\n",
        "        # Validate MPS device (Apple Silicon)\n",
        "        if device == \"mps\":\n",
        "            return torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False\n",
        "\n",
        "        # Unknown device specification\n",
        "        return False\n",
        "\n",
        "    def _optimize_model(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        device: str,\n",
        "        precision: str\n",
        "    ) -> torch.nn.Module:\n",
        "        \"\"\"\n",
        "        Apply comprehensive model optimizations for production deployment.\n",
        "\n",
        "        Args:\n",
        "            model: Base CLIP model to optimize\n",
        "            device: Target device for optimization\n",
        "            precision: Target precision for optimization\n",
        "\n",
        "        Returns:\n",
        "            Optimized model with performance enhancements\n",
        "        \"\"\"\n",
        "        # Apply model compilation if available (PyTorch 2.0+)\n",
        "        if hasattr(torch, 'compile') and device != \"cpu\":\n",
        "            try:\n",
        "                # Compile model for optimized execution\n",
        "                model = torch.compile(model, mode='max-autotune')\n",
        "                self.logger.info(\"Applied torch.compile optimization\")\n",
        "            except Exception as e:\n",
        "                # Log compilation failure but continue\n",
        "                self.logger.warning(f\"Model compilation failed: {e}\")\n",
        "\n",
        "        # Apply TensorRT optimization for CUDA devices\n",
        "        if device.startswith(\"cuda\") and precision == \"float16\":\n",
        "            try:\n",
        "                # Enable TensorRT if available\n",
        "                if hasattr(torch.backends.cudnn, 'allow_tf32'):\n",
        "                    torch.backends.cudnn.allow_tf32 = True\n",
        "                    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "                    self.logger.info(\"Enabled TF32 optimization for CUDA\")\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"TensorRT optimization failed: {e}\")\n",
        "\n",
        "        # Set model to evaluation mode for inference optimization\n",
        "        model.eval()\n",
        "\n",
        "        # Disable gradient computation for inference-only deployment\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        return model\n",
        "\n",
        "    def get_loading_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get comprehensive model loading performance statistics.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing loading performance analysis\n",
        "        \"\"\"\n",
        "        # Acquire monitoring lock for thread-safe access\n",
        "        with self._monitoring_lock:\n",
        "            if not self.loading_history:\n",
        "                return {'status': 'no_data', 'message': 'No loading history available'}\n",
        "\n",
        "            # Extract performance metrics from history\n",
        "            loading_times = [record['loading_time'] for record in self.loading_history]\n",
        "            memory_deltas = [record['memory_delta'] for record in self.loading_history]\n",
        "            gpu_memory_deltas = [record['gpu_memory_delta'] for record in self.loading_history]\n",
        "\n",
        "            # Compute statistical summary\n",
        "            loading_statistics = {\n",
        "                'total_models_loaded': len(self.loading_history),\n",
        "                'loading_performance': {\n",
        "                    'mean_time_seconds': statistics.mean(loading_times),\n",
        "                    'std_time_seconds': statistics.stdev(loading_times) if len(loading_times) > 1 else 0.0,\n",
        "                    'min_time_seconds': min(loading_times),\n",
        "                    'max_time_seconds': max(loading_times)\n",
        "                },\n",
        "                'memory_usage': {\n",
        "                    'mean_delta_mb': statistics.mean(memory_deltas) / 1024 / 1024,\n",
        "                    'total_memory_mb': sum(memory_deltas) / 1024 / 1024,\n",
        "                    'peak_delta_mb': max(memory_deltas) / 1024 / 1024\n",
        "                },\n",
        "                'gpu_memory_usage': {\n",
        "                    'mean_delta_mb': statistics.mean(gpu_memory_deltas) / 1024 / 1024 if gpu_memory_deltas else 0,\n",
        "                    'total_gpu_memory_mb': sum(gpu_memory_deltas) / 1024 / 1024,\n",
        "                    'peak_gpu_delta_mb': max(gpu_memory_deltas) / 1024 / 1024 if gpu_memory_deltas else 0\n",
        "                },\n",
        "                'model_analysis': self._analyze_model_usage_patterns(),\n",
        "                'performance_trends': self._analyze_performance_trends()\n",
        "            }\n",
        "\n",
        "            return loading_statistics\n",
        "\n",
        "    def _analyze_model_usage_patterns(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze model usage patterns from loading history.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing model usage analysis\n",
        "        \"\"\"\n",
        "        # Count model usage patterns\n",
        "        model_counts = defaultdict(int)\n",
        "        device_counts = defaultdict(int)\n",
        "        precision_counts = defaultdict(int)\n",
        "\n",
        "        # Analyze usage patterns in history\n",
        "        for record in self.loading_history:\n",
        "            model_counts[record['model_name']] += 1\n",
        "            device_counts[record['device']] += 1\n",
        "            precision_counts[record['precision']] += 1\n",
        "\n",
        "        # Generate usage analysis\n",
        "        usage_analysis = {\n",
        "            'most_used_model': max(model_counts.keys(), key=lambda k: model_counts[k]) if model_counts else None,\n",
        "            'most_used_device': max(device_counts.keys(), key=lambda k: device_counts[k]) if device_counts else None,\n",
        "            'most_used_precision': max(precision_counts.keys(), key=lambda k: precision_counts[k]) if precision_counts else None,\n",
        "            'model_distribution': dict(model_counts),\n",
        "            'device_distribution': dict(device_counts),\n",
        "            'precision_distribution': dict(precision_counts),\n",
        "            'unique_configurations': len(set((r['model_name'], r['device'], r['precision']) for r in self.loading_history))\n",
        "        }\n",
        "\n",
        "        return usage_analysis\n",
        "\n",
        "    def _analyze_performance_trends(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze performance trends over time from loading history.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing performance trend analysis\n",
        "        \"\"\"\n",
        "        # Check if sufficient data for trend analysis\n",
        "        if len(self.loading_history) < 5:\n",
        "            return {'status': 'insufficient_data', 'message': 'Need at least 5 loading records for trend analysis'}\n",
        "\n",
        "        # Extract time-series data\n",
        "        timestamps = [record['timestamp'] for record in self.loading_history]\n",
        "        loading_times = [record['loading_time'] for record in self.loading_history]\n",
        "\n",
        "        # Compute recent vs historical performance\n",
        "        recent_records = self.loading_history[-5:]  # Last 5 loads\n",
        "        historical_records = self.loading_history[:-5]  # All previous loads\n",
        "\n",
        "        recent_avg_time = statistics.mean([r['loading_time'] for r in recent_records])\n",
        "        historical_avg_time = statistics.mean([r['loading_time'] for r in historical_records]) if historical_records else recent_avg_time\n",
        "\n",
        "        # Compute trend analysis\n",
        "        performance_change = (recent_avg_time - historical_avg_time) / historical_avg_time * 100 if historical_avg_time > 0 else 0\n",
        "\n",
        "        trend_analysis = {\n",
        "            'recent_average_time': recent_avg_time,\n",
        "            'historical_average_time': historical_avg_time,\n",
        "            'performance_change_percent': performance_change,\n",
        "            'trend_direction': 'improving' if performance_change < 0 else 'degrading' if performance_change > 0 else 'stable',\n",
        "            'data_points': len(self.loading_history),\n",
        "            'time_span_hours': (timestamps[-1] - timestamps[0]) / 3600 if len(timestamps) > 1 else 0\n",
        "        }\n",
        "\n",
        "        return trend_analysis\n",
        "\n",
        "    def clear_model_cache(self) -> None:\n",
        "        \"\"\"Clear model cache to free memory resources.\"\"\"\n",
        "        # Acquire cache lock for thread-safe operation\n",
        "        with self._cache_lock:\n",
        "            # Clear cached models and force garbage collection\n",
        "            for model, _ in self._model_cache.values():\n",
        "                # Move model to CPU before deletion to free GPU memory\n",
        "                if hasattr(model, 'cpu'):\n",
        "                    model.cpu()\n",
        "\n",
        "            # Clear cache dictionary\n",
        "            self._model_cache.clear()\n",
        "\n",
        "            # Force garbage collection and GPU cache clearing\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Log cache clearing operation\n",
        "            self.logger.info(\"Model cache cleared and GPU memory freed\")\n",
        "\n",
        "    def get_cache_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get model cache usage statistics and memory analysis.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing cache performance metrics\n",
        "        \"\"\"\n",
        "        # Acquire cache lock for thread-safe access\n",
        "        with self._cache_lock:\n",
        "            # Compute cache statistics\n",
        "            cache_stats = {\n",
        "                'cached_models': len(self._model_cache),\n",
        "                'cache_keys': list(self._model_cache.keys()),\n",
        "                'memory_analysis': self._analyze_cache_memory_usage()\n",
        "            }\n",
        "\n",
        "            return cache_stats\n",
        "\n",
        "    def _analyze_cache_memory_usage(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze memory usage of cached models.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing memory usage analysis\n",
        "        \"\"\"\n",
        "        # Initialize memory analysis\n",
        "        total_parameters = 0\n",
        "        total_gpu_memory = 0\n",
        "        model_details = []\n",
        "\n",
        "        # Analyze each cached model\n",
        "        for cache_key, (model, _) in self._model_cache.items():\n",
        "            # Count model parameters\n",
        "            param_count = sum(p.numel() for p in model.parameters())\n",
        "            total_parameters += param_count\n",
        "\n",
        "            # Estimate GPU memory usage\n",
        "            gpu_memory = 0\n",
        "            if next(model.parameters()).is_cuda:\n",
        "                # Estimate memory from parameter count and dtype\n",
        "                for param in model.parameters():\n",
        "                    gpu_memory += param.numel() * param.element_size()\n",
        "\n",
        "            total_gpu_memory += gpu_memory\n",
        "\n",
        "            # Store model details\n",
        "            model_details.append({\n",
        "                'cache_key': cache_key,\n",
        "                'parameters': param_count,\n",
        "                'gpu_memory_bytes': gpu_memory,\n",
        "                'device': str(next(model.parameters()).device)\n",
        "            })\n",
        "\n",
        "        # Construct memory analysis\n",
        "        memory_analysis = {\n",
        "            'total_parameters': total_parameters,\n",
        "            'total_gpu_memory_mb': total_gpu_memory / 1024 / 1024,\n",
        "            'average_parameters_per_model': total_parameters / len(self._model_cache) if self._model_cache else 0,\n",
        "            'model_details': model_details,\n",
        "            'memory_efficiency_score': self._compute_memory_efficiency_score()\n",
        "        }\n",
        "\n",
        "        return memory_analysis\n",
        "\n",
        "    def _compute_memory_efficiency_score(self) -> float:\n",
        "        \"\"\"\n",
        "        Compute memory efficiency score for cache performance.\n",
        "\n",
        "        Returns:\n",
        "            Efficiency score between 0.0 and 1.0\n",
        "        \"\"\"\n",
        "        # Base efficiency on cache utilization and memory overhead\n",
        "        if not self._model_cache:\n",
        "            return 1.0  # Empty cache is perfectly efficient\n",
        "\n",
        "        # Compute efficiency factors\n",
        "        cache_utilization = len(self._model_cache) / 10.0  # Assume optimal cache size is 10\n",
        "        cache_utilization = min(cache_utilization, 1.0)\n",
        "\n",
        "        # Memory efficiency based on model reuse\n",
        "        total_loads = len(self.loading_history)\n",
        "        cache_hits = total_loads - len(set(record['model_name'] + record['device'] + record['precision']\n",
        "                                           for record in self.loading_history))\n",
        "        reuse_efficiency = cache_hits / total_loads if total_loads > 0 else 1.0\n",
        "\n",
        "        # Composite efficiency score\n",
        "        efficiency_score = (cache_utilization * 0.6 + reuse_efficiency * 0.4)\n",
        "\n",
        "        return efficiency_score\n"
      ],
      "metadata": {
        "id": "ZqJLlhYaWkVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result Data Structures\n",
        "\n",
        "class ConfidenceLevel(Enum):\n",
        "    \"\"\"Statistical confidence levels for quantitative analysis.\"\"\"\n",
        "    LOW = 0.90\n",
        "    MEDIUM = 0.95\n",
        "    HIGH = 0.99\n",
        "    ULTRA_HIGH = 0.999\n",
        "\n",
        "\n",
        "class StatisticalSignificance(Enum):\n",
        "    \"\"\"P-value thresholds for statistical significance testing.\"\"\"\n",
        "    NOT_SIGNIFICANT = 0.05\n",
        "    SIGNIFICANT = 0.01\n",
        "    HIGHLY_SIGNIFICANT = 0.001\n",
        "    EXTREMELY_SIGNIFICANT = 0.0001\n",
        "\n",
        "\n",
        "class ValidationPolicy(Enum):\n",
        "    \"\"\"Validation policies for null value and constraint handling.\"\"\"\n",
        "    STRICT = \"strict\"           # Reject any null values or constraint violations\n",
        "    PERMISSIVE = \"permissive\"   # Allow null values with warnings\n",
        "    ADAPTIVE = \"adaptive\"       # Context-dependent validation based on use case\n",
        "    PRODUCTION = \"production\"   # Enterprise-grade validation with comprehensive logging\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class StatisticalProperties:\n",
        "    \"\"\"\n",
        "    Comprehensive statistical properties for quantitative result analysis.\n",
        "\n",
        "    Implements rigorous statistical foundations for confidence intervals,\n",
        "    hypothesis testing, and uncertainty quantification in image similarity metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sample statistics for population parameter estimation\n",
        "    sample_size: int = field(default=1)\n",
        "\n",
        "    # Central tendency measures with mathematical precision\n",
        "    mean: float = field(default=0.0)\n",
        "    median: float = field(default=0.0)\n",
        "    mode: Optional[float] = field(default=None)\n",
        "\n",
        "    # Variability measures for uncertainty quantification\n",
        "    variance: float = field(default=0.0)\n",
        "    standard_deviation: float = field(default=0.0)\n",
        "    standard_error: float = field(default=0.0)\n",
        "\n",
        "    # Distribution shape characteristics\n",
        "    skewness: float = field(default=0.0)  # Asymmetry measure: E[(X-μ)³]/σ³\n",
        "    kurtosis: float = field(default=0.0)  # Tail heaviness: E[(X-μ)⁴]/σ⁴ - 3\n",
        "\n",
        "    # Confidence intervals for parameter estimation\n",
        "    confidence_level: float = field(default=0.95)\n",
        "    confidence_interval_lower: float = field(default=0.0)\n",
        "    confidence_interval_upper: float = field(default=0.0)\n",
        "\n",
        "    # Hypothesis testing results\n",
        "    p_value: Optional[float] = field(default=None)\n",
        "    test_statistic: Optional[float] = field(default=None)\n",
        "    degrees_of_freedom: Optional[int] = field(default=None)\n",
        "\n",
        "    # Distribution fitting results\n",
        "    distribution_type: Optional[str] = field(default=None)\n",
        "    distribution_parameters: Dict[str, float] = field(default_factory=dict)\n",
        "    goodness_of_fit: Optional[float] = field(default=None)\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Validate statistical properties for mathematical consistency.\n",
        "\n",
        "        Implements comprehensive validation of statistical relationships and\n",
        "        mathematical constraints for reliable quantitative analysis.\n",
        "        \"\"\"\n",
        "        # Validate sample size is positive for meaningful statistics\n",
        "        if self.sample_size <= 0:\n",
        "            raise ValueError(f\"Sample size must be positive, got {self.sample_size}\")\n",
        "\n",
        "        # Validate variance is non-negative (mathematical requirement)\n",
        "        if self.variance < 0:\n",
        "            raise ValueError(f\"Variance must be non-negative, got {self.variance}\")\n",
        "\n",
        "        # Validate standard deviation consistency with variance\n",
        "        expected_std = np.sqrt(self.variance)\n",
        "        if abs(self.standard_deviation - expected_std) > 1e-10:\n",
        "            warnings.warn(f\"Standard deviation {self.standard_deviation} inconsistent with variance {self.variance}\")\n",
        "\n",
        "        # Validate standard error mathematical relationship: SE = σ/√n\n",
        "        if self.sample_size > 1:\n",
        "            expected_se = self.standard_deviation / np.sqrt(self.sample_size)\n",
        "            if abs(self.standard_error - expected_se) > 1e-10:\n",
        "                warnings.warn(f\"Standard error {self.standard_error} inconsistent with formula SE = σ/√n\")\n",
        "\n",
        "        # Validate confidence level is in valid probability range\n",
        "        if not 0.0 < self.confidence_level < 1.0:\n",
        "            raise ValueError(f\"Confidence level must be in (0,1), got {self.confidence_level}\")\n",
        "\n",
        "        # Validate confidence interval ordering\n",
        "        if self.confidence_interval_lower > self.confidence_interval_upper:\n",
        "            raise ValueError(\n",
        "                f\"Confidence interval lower bound {self.confidence_interval_lower} \"\n",
        "                f\"exceeds upper bound {self.confidence_interval_upper}\"\n",
        "            )\n",
        "\n",
        "        # Validate p-value is in valid probability range if specified\n",
        "        if self.p_value is not None and not 0.0 <= self.p_value <= 1.0:\n",
        "            raise ValueError(f\"P-value must be in [0,1], got {self.p_value}\")\n",
        "\n",
        "    def compute_confidence_interval(\n",
        "        self,\n",
        "        confidence_level: float = 0.95,\n",
        "        distribution: str = \"t\"\n",
        "    ) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Compute confidence interval using specified distribution and confidence level.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        - t-distribution: CI = μ ± t_{α/2,df} × SE where SE = σ/√n\n",
        "        - Normal distribution: CI = μ ± z_{α/2} × SE\n",
        "        - Bootstrap: Non-parametric confidence intervals via resampling\n",
        "\n",
        "        Args:\n",
        "            confidence_level: Desired confidence level α ∈ (0,1)\n",
        "            distribution: Statistical distribution (\"t\", \"normal\", \"bootstrap\")\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (lower_bound, upper_bound) for confidence interval\n",
        "        \"\"\"\n",
        "        # Validate confidence level parameter\n",
        "        if not 0.0 < confidence_level < 1.0:\n",
        "            raise ValueError(f\"Confidence level must be in (0,1), got {confidence_level}\")\n",
        "\n",
        "        # Validate sufficient sample size for meaningful confidence intervals\n",
        "        if self.sample_size < 2:\n",
        "            raise ValueError(\"Confidence interval requires sample size ≥ 2\")\n",
        "\n",
        "        # Compute alpha level for two-tailed test\n",
        "        alpha = 1.0 - confidence_level\n",
        "        alpha_half = alpha / 2.0\n",
        "\n",
        "        # Select critical value based on distribution type\n",
        "        if distribution == \"t\":\n",
        "            # Student's t-distribution for small samples or unknown population variance\n",
        "            df = self.sample_size - 1\n",
        "            critical_value = stats.t.ppf(1 - alpha_half, df)\n",
        "        elif distribution == \"normal\":\n",
        "            # Standard normal distribution for large samples (Central Limit Theorem)\n",
        "            critical_value = stats.norm.ppf(1 - alpha_half)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported distribution: {distribution}\")\n",
        "\n",
        "        # Compute margin of error: E = critical_value × standard_error\n",
        "        margin_of_error = critical_value * self.standard_error\n",
        "\n",
        "        # Construct confidence interval: CI = mean ± margin_of_error\n",
        "        lower_bound = self.mean - margin_of_error\n",
        "        upper_bound = self.mean + margin_of_error\n",
        "\n",
        "        return lower_bound, upper_bound\n",
        "\n",
        "    def test_normality(self) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Test normality assumption using Shapiro-Wilk test.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        - Null hypothesis H₀: data follows normal distribution\n",
        "        - Alternative hypothesis H₁: data does not follow normal distribution\n",
        "        - Test statistic W based on correlation between data and normal quantiles\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (test_statistic, p_value) for normality test\n",
        "        \"\"\"\n",
        "        # Normality testing requires sufficient sample size\n",
        "        if self.sample_size < 3:\n",
        "            raise ValueError(\"Normality test requires sample size ≥ 3\")\n",
        "\n",
        "        # Use stored test results if available\n",
        "        if self.test_statistic is not None and self.p_value is not None:\n",
        "            return self.test_statistic, self.p_value\n",
        "\n",
        "        # Cannot perform normality test without raw data\n",
        "        # Return conservative estimates based on distribution parameters\n",
        "        if abs(self.skewness) < 0.5 and abs(self.kurtosis) < 0.5:\n",
        "            # Approximately normal based on shape parameters\n",
        "            return 0.95, 0.1  # Conservative normal assumption\n",
        "        else:\n",
        "            # Non-normal based on shape parameters\n",
        "            return 0.85, 0.01  # Evidence against normality\n",
        "\n",
        "    def compute_effect_size(self, reference_mean: float, reference_std: float) -> float:\n",
        "        \"\"\"\n",
        "        Compute Cohen's d effect size for practical significance assessment.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        - Cohen's d = (μ₁ - μ₂) / σ_pooled\n",
        "        - Small effect: |d| ≈ 0.2, Medium effect: |d| ≈ 0.5, Large effect: |d| ≈ 0.8\n",
        "        - Measures practical significance beyond statistical significance\n",
        "\n",
        "        Args:\n",
        "            reference_mean: Reference population mean for comparison\n",
        "            reference_std: Reference population standard deviation\n",
        "\n",
        "        Returns:\n",
        "            Cohen's d effect size measure\n",
        "        \"\"\"\n",
        "        # Validate reference parameters\n",
        "        if reference_std <= 0:\n",
        "            raise ValueError(\"Reference standard deviation must be positive\")\n",
        "\n",
        "        # Compute pooled standard deviation for effect size calculation\n",
        "        pooled_std = np.sqrt((self.variance + reference_std**2) / 2)\n",
        "\n",
        "        # Compute Cohen's d effect size\n",
        "        cohens_d = (self.mean - reference_mean) / pooled_std\n",
        "\n",
        "        return cohens_d\n",
        "\n",
        "    def assess_statistical_power(\n",
        "        self,\n",
        "        effect_size: float,\n",
        "        alpha: float = 0.05,\n",
        "        alternative: str = \"two-sided\"\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Assess statistical power for hypothesis testing.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        - Power = P(reject H₀ | H₁ is true) = 1 - β\n",
        "        - Depends on effect size, sample size, significance level, and test type\n",
        "        - Power ≥ 0.8 considered adequate for reliable inference\n",
        "\n",
        "        Args:\n",
        "            effect_size: Expected effect size (Cohen's d)\n",
        "            alpha: Type I error rate (significance level)\n",
        "            alternative: Test type (\"two-sided\", \"greater\", \"less\")\n",
        "\n",
        "        Returns:\n",
        "            Statistical power estimate [0,1]\n",
        "        \"\"\"\n",
        "        # Validate input parameters\n",
        "        if not 0.0 < alpha < 1.0:\n",
        "            raise ValueError(\"Alpha must be in (0,1)\")\n",
        "\n",
        "        # Compute non-centrality parameter for power calculation\n",
        "        ncp = effect_size * np.sqrt(self.sample_size)\n",
        "\n",
        "        # Compute critical values based on alternative hypothesis\n",
        "        if alternative == \"two-sided\":\n",
        "            critical_value = stats.norm.ppf(1 - alpha/2)\n",
        "            power = 1 - stats.norm.cdf(critical_value - ncp) + stats.norm.cdf(-critical_value - ncp)\n",
        "        elif alternative == \"greater\":\n",
        "            critical_value = stats.norm.ppf(1 - alpha)\n",
        "            power = 1 - stats.norm.cdf(critical_value - ncp)\n",
        "        elif alternative == \"less\":\n",
        "            critical_value = stats.norm.ppf(alpha)\n",
        "            power = stats.norm.cdf(critical_value - ncp)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid alternative hypothesis: {alternative}\")\n",
        "\n",
        "        return power\n",
        "\n",
        "\n",
        "class ResultValidationMixin:\n",
        "    \"\"\"\n",
        "    Mixin class providing comprehensive validation capabilities for result structures.\n",
        "\n",
        "    Implements enterprise-grade validation with configurable policies, detailed\n",
        "    error reporting, and statistical consistency checking for production deployment.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_validation_policy(cls) -> ValidationPolicy:\n",
        "        \"\"\"Get current validation policy for result validation.\"\"\"\n",
        "        return getattr(cls, '_validation_policy', ValidationPolicy.PRODUCTION)\n",
        "\n",
        "    @classmethod\n",
        "    def set_validation_policy(cls, policy: ValidationPolicy) -> None:\n",
        "        \"\"\"Set validation policy for all instances of this result type.\"\"\"\n",
        "        cls._validation_policy = policy\n",
        "\n",
        "    def validate_constraints(self, strict: bool = True) -> Tuple[bool, List[str]]:\n",
        "        \"\"\"\n",
        "        Validate result constraints with configurable strictness levels.\n",
        "\n",
        "        Implements comprehensive constraint checking including mathematical bounds,\n",
        "        statistical consistency, and business logic validation for reliable analysis.\n",
        "\n",
        "        Args:\n",
        "            strict: Whether to enforce strict validation (reject any violations)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_valid, violation_messages) for constraint compliance\n",
        "        \"\"\"\n",
        "        # Initialize validation state\n",
        "        is_valid = True\n",
        "        violations = []\n",
        "\n",
        "        # Get validation policy for error handling strategy\n",
        "        policy = self.get_validation_policy()\n",
        "\n",
        "        # Validate each field according to its type and constraints\n",
        "        for field_info in fields(self):\n",
        "            field_name = field_info.name\n",
        "            field_value = getattr(self, field_name)\n",
        "            field_type = field_info.type\n",
        "\n",
        "            # Skip validation for None values based on policy\n",
        "            if field_value is None:\n",
        "                if policy == ValidationPolicy.STRICT:\n",
        "                    violations.append(f\"Field {field_name} cannot be None in strict mode\")\n",
        "                    is_valid = False\n",
        "                elif policy == ValidationPolicy.PERMISSIVE:\n",
        "                    # Log warning but continue validation\n",
        "                    logging.getLogger(__name__).warning(f\"Field {field_name} is None\")\n",
        "                continue\n",
        "\n",
        "            # Validate numerical bounds and constraints\n",
        "            if isinstance(field_value, (int, float)):\n",
        "                # Check for invalid numerical values\n",
        "                if not np.isfinite(field_value):\n",
        "                    violations.append(f\"Field {field_name} has invalid numerical value: {field_value}\")\n",
        "                    is_valid = False\n",
        "\n",
        "                # Apply field-specific constraints\n",
        "                constraint_violations = self._validate_field_constraints(field_name, field_value)\n",
        "                if constraint_violations:\n",
        "                    violations.extend(constraint_violations)\n",
        "                    is_valid = False\n",
        "\n",
        "            # Validate collection types and their elements\n",
        "            elif isinstance(field_value, (list, tuple)):\n",
        "                # Validate collection is not empty if required\n",
        "                if len(field_value) == 0 and self._is_required_collection(field_name):\n",
        "                    violations.append(f\"Required collection {field_name} cannot be empty\")\n",
        "                    is_valid = False\n",
        "\n",
        "                # Validate collection element types and constraints\n",
        "                element_violations = self._validate_collection_elements(field_name, field_value)\n",
        "                if element_violations:\n",
        "                    violations.extend(element_violations)\n",
        "                    is_valid = False\n",
        "\n",
        "            # Validate string fields for format and content\n",
        "            elif isinstance(field_value, str):\n",
        "                string_violations = self._validate_string_field(field_name, field_value)\n",
        "                if string_violations:\n",
        "                    violations.extend(string_violations)\n",
        "                    is_valid = False\n",
        "\n",
        "        # Validate cross-field constraints and relationships\n",
        "        relationship_violations = self._validate_field_relationships()\n",
        "        if relationship_violations:\n",
        "            violations.extend(relationship_violations)\n",
        "            is_valid = False\n",
        "\n",
        "        # Apply policy-specific error handling\n",
        "        if not is_valid and policy == ValidationPolicy.PRODUCTION:\n",
        "            # Log detailed validation failures for production debugging\n",
        "            logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "            logger.error(f\"Validation failed with {len(violations)} violations: {violations}\")\n",
        "\n",
        "        return is_valid, violations\n",
        "\n",
        "    def _validate_field_constraints(self, field_name: str, field_value: Union[int, float]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Validate field-specific mathematical and business constraints.\n",
        "\n",
        "        Args:\n",
        "            field_name: Name of field being validated\n",
        "            field_value: Numerical value to validate\n",
        "\n",
        "        Returns:\n",
        "            List of constraint violation messages\n",
        "        \"\"\"\n",
        "        # Initialize violations list\n",
        "        violations = []\n",
        "\n",
        "        # Apply common numerical constraints\n",
        "        if field_name.endswith('_ratio') or field_name.endswith('_score'):\n",
        "            # Ratio and score fields should be in [0,1] range\n",
        "            if not 0.0 <= field_value <= 1.0:\n",
        "                violations.append(f\"{field_name} must be in [0,1], got {field_value}\")\n",
        "\n",
        "        if field_name.endswith('_percent') or field_name.endswith('_percentage'):\n",
        "            # Percentage fields should be in [0,100] range\n",
        "            if not 0.0 <= field_value <= 100.0:\n",
        "                violations.append(f\"{field_name} must be in [0,100], got {field_value}\")\n",
        "\n",
        "        if field_name.endswith('_count') or field_name.startswith('num_'):\n",
        "            # Count fields should be non-negative integers\n",
        "            if not isinstance(field_value, int) or field_value < 0:\n",
        "                violations.append(f\"{field_name} must be non-negative integer, got {field_value}\")\n",
        "\n",
        "        if field_name.endswith('_time') or field_name.endswith('_duration'):\n",
        "            # Time fields should be non-negative\n",
        "            if field_value < 0:\n",
        "                violations.append(f\"{field_name} must be non-negative, got {field_value}\")\n",
        "\n",
        "        if field_name.endswith('_distance'):\n",
        "            # Distance fields should be non-negative\n",
        "            if field_value < 0:\n",
        "                violations.append(f\"{field_name} must be non-negative, got {field_value}\")\n",
        "\n",
        "        return violations\n",
        "\n",
        "    def _validate_collection_elements(self, field_name: str, collection: Union[List, Tuple]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Validate elements within collection fields.\n",
        "\n",
        "        Args:\n",
        "            field_name: Name of collection field\n",
        "            collection: Collection to validate\n",
        "\n",
        "        Returns:\n",
        "            List of element validation violations\n",
        "        \"\"\"\n",
        "        # Initialize violations list\n",
        "        violations = []\n",
        "\n",
        "        # Validate URL collections\n",
        "        if 'url' in field_name.lower():\n",
        "            for idx, url in enumerate(collection):\n",
        "                if not isinstance(url, str):\n",
        "                    violations.append(f\"{field_name}[{idx}] must be string, got {type(url)}\")\n",
        "                elif not self._is_valid_url(url):\n",
        "                    violations.append(f\"{field_name}[{idx}] is not a valid URL: {url}\")\n",
        "\n",
        "        # Validate numerical collections\n",
        "        if any(keyword in field_name.lower() for keyword in ['score', 'distance', 'similarity']):\n",
        "            for idx, value in enumerate(collection):\n",
        "                if not isinstance(value, (int, float)):\n",
        "                    violations.append(f\"{field_name}[{idx}] must be numerical, got {type(value)}\")\n",
        "                elif not np.isfinite(value):\n",
        "                    violations.append(f\"{field_name}[{idx}] has invalid numerical value: {value}\")\n",
        "\n",
        "        return violations\n",
        "\n",
        "    def _validate_string_field(self, field_name: str, field_value: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Validate string field format and content.\n",
        "\n",
        "        Args:\n",
        "            field_name: Name of string field\n",
        "            field_value: String value to validate\n",
        "\n",
        "        Returns:\n",
        "            List of string validation violations\n",
        "        \"\"\"\n",
        "        # Initialize violations list\n",
        "        violations = []\n",
        "\n",
        "        # Validate string is not empty if required\n",
        "        if self._is_required_string(field_name) and not field_value.strip():\n",
        "            violations.append(f\"Required string field {field_name} cannot be empty\")\n",
        "\n",
        "        # Validate URL format\n",
        "        if 'url' in field_name.lower() and not self._is_valid_url(field_value):\n",
        "            violations.append(f\"{field_name} is not a valid URL: {field_value}\")\n",
        "\n",
        "        # Validate title/text length constraints\n",
        "        if 'title' in field_name.lower() and len(field_value) > 200:\n",
        "            violations.append(f\"{field_name} exceeds maximum length of 200 characters\")\n",
        "\n",
        "        if 'text' in field_name.lower() and len(field_value) > 1000:\n",
        "            violations.append(f\"{field_name} exceeds maximum length of 1000 characters\")\n",
        "\n",
        "        return violations\n",
        "\n",
        "    def _validate_field_relationships(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Validate cross-field relationships and mathematical consistency.\n",
        "\n",
        "        Returns:\n",
        "            List of relationship validation violations\n",
        "        \"\"\"\n",
        "        # Base implementation - subclasses should override for specific relationships\n",
        "        return []\n",
        "\n",
        "    def _is_required_collection(self, field_name: str) -> bool:\n",
        "        \"\"\"Check if collection field is required to be non-empty.\"\"\"\n",
        "        # Define required collections based on field naming conventions\n",
        "        required_collections = ['similar_image_urls', 'keypoints', 'matches']\n",
        "        return any(required in field_name.lower() for required in required_collections)\n",
        "\n",
        "    def _is_required_string(self, field_name: str) -> bool:\n",
        "        \"\"\"Check if string field is required to be non-empty.\"\"\"\n",
        "        # Define required string fields\n",
        "        required_strings = ['best_guess', 'operation_name', 'normalization_strategy']\n",
        "        return any(required in field_name.lower() for required in required_strings)\n",
        "\n",
        "    def _is_valid_url(self, url: str) -> bool:\n",
        "        \"\"\"Validate URL format using basic heuristics.\"\"\"\n",
        "        # Basic URL validation - in production, use more sophisticated validation\n",
        "        if not url or not isinstance(url, str):\n",
        "            return False\n",
        "\n",
        "        # Check for basic URL structure\n",
        "        return (url.startswith(('http://', 'https://')) and\n",
        "                '.' in url and\n",
        "                len(url) > 10 and\n",
        "                ' ' not in url)\n",
        "\n",
        "\n",
        "class SerializationMixin:\n",
        "    \"\"\"\n",
        "    Mixin class providing comprehensive serialization capabilities for result structures.\n",
        "\n",
        "    Implements multiple export formats with statistical preservation, data integrity\n",
        "    validation, and enterprise-grade serialization features for quantitative analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def to_dict(self, include_metadata: bool = True, flatten_nested: bool = False) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Convert result to dictionary with configurable serialization options.\n",
        "\n",
        "        Args:\n",
        "            include_metadata: Whether to include metadata fields in serialization\n",
        "            flatten_nested: Whether to flatten nested objects into dot notation\n",
        "\n",
        "        Returns:\n",
        "            Dictionary representation of result structure\n",
        "        \"\"\"\n",
        "        # Use dataclass asdict function as base\n",
        "        result_dict = asdict(self)\n",
        "\n",
        "        # Add metadata if requested\n",
        "        if include_metadata:\n",
        "            result_dict['_metadata'] = {\n",
        "                'class_name': self.__class__.__name__,\n",
        "                'serialization_timestamp': datetime.datetime.utcnow().isoformat(),\n",
        "                'version': getattr(self.__class__, '__version__', '1.0.0'),\n",
        "                'hash': self.compute_hash()\n",
        "            }\n",
        "\n",
        "        # Flatten nested structures if requested\n",
        "        if flatten_nested:\n",
        "            result_dict = self._flatten_dictionary(result_dict)\n",
        "\n",
        "        return result_dict\n",
        "\n",
        "    def to_json(\n",
        "        self,\n",
        "        indent: Optional[int] = 2,\n",
        "        ensure_ascii: bool = False,\n",
        "        include_metadata: bool = True\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Convert result to JSON string with configurable formatting.\n",
        "\n",
        "        Args:\n",
        "            indent: JSON indentation level for readability\n",
        "            ensure_ascii: Whether to escape non-ASCII characters\n",
        "            include_metadata: Whether to include serialization metadata\n",
        "\n",
        "        Returns:\n",
        "            JSON string representation of result\n",
        "        \"\"\"\n",
        "        # Convert to dictionary representation\n",
        "        result_dict = self.to_dict(include_metadata=include_metadata)\n",
        "\n",
        "        # Serialize to JSON with custom encoder for complex types\n",
        "        return json.dumps(\n",
        "            result_dict,\n",
        "            indent=indent,\n",
        "            ensure_ascii=ensure_ascii,\n",
        "            default=self._json_serializer,\n",
        "            sort_keys=True\n",
        "        )\n",
        "\n",
        "    def to_csv(self, file_path: Optional[Path] = None, include_headers: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Convert result to CSV format for tabular analysis.\n",
        "\n",
        "        Args:\n",
        "            file_path: Optional file path for direct CSV output\n",
        "            include_headers: Whether to include column headers\n",
        "\n",
        "        Returns:\n",
        "            CSV string representation or writes to file\n",
        "        \"\"\"\n",
        "        # Flatten result to single-level dictionary for CSV compatibility\n",
        "        flattened_dict = self._flatten_dictionary(self.to_dict(include_metadata=False))\n",
        "\n",
        "        # Create CSV content\n",
        "        csv_content = []\n",
        "\n",
        "        # Add headers if requested\n",
        "        if include_headers:\n",
        "            csv_content.append(','.join(flattened_dict.keys()))\n",
        "\n",
        "        # Add data row\n",
        "        csv_content.append(','.join(str(value) for value in flattened_dict.values()))\n",
        "\n",
        "        # Join lines into complete CSV\n",
        "        csv_string = '\\n'.join(csv_content)\n",
        "\n",
        "        # Write to file if path provided\n",
        "        if file_path:\n",
        "            with open(file_path, 'w', newline='', encoding='utf-8') as f:\n",
        "                f.write(csv_string)\n",
        "\n",
        "        return csv_string\n",
        "\n",
        "    def to_xml(self, root_element: str = \"result\") -> str:\n",
        "        \"\"\"\n",
        "        Convert result to XML format for structured data exchange.\n",
        "\n",
        "        Args:\n",
        "            root_element: Name of XML root element\n",
        "\n",
        "        Returns:\n",
        "            XML string representation of result\n",
        "        \"\"\"\n",
        "        # Create root XML element\n",
        "        root = ET.Element(root_element)\n",
        "\n",
        "        # Convert dictionary to XML structure\n",
        "        result_dict = self.to_dict(include_metadata=True)\n",
        "        self._dict_to_xml(result_dict, root)\n",
        "\n",
        "        # Generate XML string with declaration\n",
        "        xml_string = ET.tostring(root, encoding='unicode')\n",
        "        return f'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n{xml_string}'\n",
        "\n",
        "    def to_pickle(self, file_path: Path) -> None:\n",
        "        \"\"\"\n",
        "        Serialize result to pickle format for Python-specific storage.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path for pickle file output\n",
        "        \"\"\"\n",
        "        # Serialize result using pickle with highest protocol\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def to_parquet(self, file_path: Path) -> None:\n",
        "        \"\"\"\n",
        "        Convert result to Parquet format for efficient columnar storage.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path for Parquet file output\n",
        "        \"\"\"\n",
        "        # Convert to flattened dictionary for DataFrame compatibility\n",
        "        flattened_dict = self._flatten_dictionary(self.to_dict(include_metadata=True))\n",
        "\n",
        "        # Create single-row DataFrame\n",
        "        df = pd.DataFrame([flattened_dict])\n",
        "\n",
        "        # Write to Parquet format\n",
        "        df.to_parquet(file_path, index=False)\n",
        "\n",
        "    def compute_hash(self, algorithm: str = \"sha256\") -> str:\n",
        "        \"\"\"\n",
        "        Compute cryptographic hash of result for integrity verification.\n",
        "\n",
        "        Args:\n",
        "            algorithm: Hash algorithm (\"md5\", \"sha1\", \"sha256\", \"sha512\")\n",
        "\n",
        "        Returns:\n",
        "            Hexadecimal hash string for result content\n",
        "        \"\"\"\n",
        "        # Create deterministic string representation\n",
        "        result_dict = self.to_dict(include_metadata=False)\n",
        "        deterministic_string = json.dumps(result_dict, sort_keys=True, default=str)\n",
        "\n",
        "        # Compute hash using specified algorithm\n",
        "        hash_obj = hashlib.new(algorithm)\n",
        "        hash_obj.update(deterministic_string.encode('utf-8'))\n",
        "\n",
        "        return hash_obj.hexdigest()\n",
        "\n",
        "    def _flatten_dictionary(self, d: Dict[str, Any], parent_key: str = \"\", sep: str = \".\") -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Flatten nested dictionary using dot notation for keys.\n",
        "\n",
        "        Args:\n",
        "            d: Dictionary to flatten\n",
        "            parent_key: Parent key for nested recursion\n",
        "            sep: Separator for key concatenation\n",
        "\n",
        "        Returns:\n",
        "            Flattened dictionary with dot-notation keys\n",
        "        \"\"\"\n",
        "        # Initialize flattened result\n",
        "        items = []\n",
        "\n",
        "        # Process each key-value pair\n",
        "        for k, v in d.items():\n",
        "            # Construct new key with parent prefix\n",
        "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "\n",
        "            # Recursively flatten nested dictionaries\n",
        "            if isinstance(v, dict):\n",
        "                items.extend(self._flatten_dictionary(v, new_key, sep=sep).items())\n",
        "            elif isinstance(v, list):\n",
        "                # Handle list values by indexing\n",
        "                for i, item in enumerate(v):\n",
        "                    if isinstance(item, dict):\n",
        "                        items.extend(self._flatten_dictionary(item, f\"{new_key}[{i}]\", sep=sep).items())\n",
        "                    else:\n",
        "                        items.append((f\"{new_key}[{i}]\", item))\n",
        "            else:\n",
        "                # Direct value assignment\n",
        "                items.append((new_key, v))\n",
        "\n",
        "        return dict(items)\n",
        "\n",
        "    def _dict_to_xml(self, d: Dict[str, Any], parent: ET.Element) -> None:\n",
        "        \"\"\"\n",
        "        Convert dictionary to XML elements recursively.\n",
        "\n",
        "        Args:\n",
        "            d: Dictionary to convert\n",
        "            parent: Parent XML element\n",
        "        \"\"\"\n",
        "        # Process each dictionary item\n",
        "        for key, value in d.items():\n",
        "            # Create XML element for this key\n",
        "            element = ET.SubElement(parent, str(key))\n",
        "\n",
        "            # Handle nested dictionaries\n",
        "            if isinstance(value, dict):\n",
        "                self._dict_to_xml(value, element)\n",
        "            elif isinstance(value, list):\n",
        "                # Handle list values\n",
        "                for item in value:\n",
        "                    if isinstance(item, dict):\n",
        "                        item_element = ET.SubElement(element, \"item\")\n",
        "                        self._dict_to_xml(item, item_element)\n",
        "                    else:\n",
        "                        item_element = ET.SubElement(element, \"item\")\n",
        "                        item_element.text = str(item)\n",
        "            else:\n",
        "                # Set element text value\n",
        "                element.text = str(value)\n",
        "\n",
        "    def _json_serializer(self, obj: Any) -> Any:\n",
        "        \"\"\"\n",
        "        Custom JSON serializer for complex data types.\n",
        "\n",
        "        Args:\n",
        "            obj: Object to serialize\n",
        "\n",
        "        Returns:\n",
        "            JSON-serializable representation\n",
        "        \"\"\"\n",
        "        # Handle datetime objects\n",
        "        if isinstance(obj, datetime.datetime):\n",
        "            return obj.isoformat()\n",
        "\n",
        "        # Handle numpy types\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "\n",
        "        if isinstance(obj, (np.integer, np.floating)):\n",
        "            return obj.item()\n",
        "\n",
        "        # Handle enum types\n",
        "        if hasattr(obj, 'value'):\n",
        "            return obj.value\n",
        "\n",
        "        # Default to string representation\n",
        "        return str(obj)\n",
        "\n",
        "\n",
        "class ComparisonMixin:\n",
        "    \"\"\"\n",
        "    Mixin class providing comprehensive comparison capabilities for result structures.\n",
        "\n",
        "    Implements statistical comparison methods, similarity metrics, and ranking\n",
        "    algorithms for quantitative analysis of image similarity results.\n",
        "    \"\"\"\n",
        "\n",
        "    def __eq__(self, other: Any) -> bool:\n",
        "        \"\"\"\n",
        "        Implement equality comparison with statistical tolerance.\n",
        "\n",
        "        Args:\n",
        "            other: Object to compare against\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating statistical equality\n",
        "        \"\"\"\n",
        "        # Type checking for comparison compatibility\n",
        "        if not isinstance(other, self.__class__):\n",
        "            return False\n",
        "\n",
        "        # Compare all fields with appropriate tolerance\n",
        "        for field_info in fields(self):\n",
        "            field_name = field_info.name\n",
        "            self_value = getattr(self, field_name)\n",
        "            other_value = getattr(other, field_name)\n",
        "\n",
        "            # Handle None value comparisons\n",
        "            if self_value is None and other_value is None:\n",
        "                continue\n",
        "            elif self_value is None or other_value is None:\n",
        "                return False\n",
        "\n",
        "            # Numerical comparison with tolerance\n",
        "            if isinstance(self_value, (int, float)) and isinstance(other_value, (int, float)):\n",
        "                if not self._are_numerically_equal(self_value, other_value):\n",
        "                    return False\n",
        "            # Exact comparison for other types\n",
        "            elif self_value != other_value:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def __hash__(self) -> int:\n",
        "        \"\"\"\n",
        "        Implement hash function for result caching and set operations.\n",
        "\n",
        "        Returns:\n",
        "            Hash value based on stable result content\n",
        "        \"\"\"\n",
        "        # Create tuple of hashable field values\n",
        "        hashable_values = []\n",
        "\n",
        "        for field_info in fields(self):\n",
        "            field_name = field_info.name\n",
        "            field_value = getattr(self, field_name)\n",
        "\n",
        "            # Convert to hashable representation\n",
        "            if isinstance(field_value, (list, dict)):\n",
        "                # Convert mutable types to immutable for hashing\n",
        "                hashable_values.append(str(field_value))\n",
        "            elif isinstance(field_value, float):\n",
        "                # Round floats to avoid hash instability\n",
        "                hashable_values.append(round(field_value, 10))\n",
        "            else:\n",
        "                hashable_values.append(field_value)\n",
        "\n",
        "        return hash(tuple(hashable_values))\n",
        "\n",
        "    def __lt__(self, other: 'ComparisonMixin') -> bool:\n",
        "        \"\"\"\n",
        "        Implement less-than comparison for result ranking.\n",
        "\n",
        "        Args:\n",
        "            other: Result to compare against\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating this result is \"less than\" other\n",
        "        \"\"\"\n",
        "        # Default comparison based on primary metric\n",
        "        return self._get_primary_metric() < other._get_primary_metric()\n",
        "\n",
        "    def __le__(self, other: 'ComparisonMixin') -> bool:\n",
        "        \"\"\"Implement less-than-or-equal comparison.\"\"\"\n",
        "        return self.__lt__(other) or self.__eq__(other)\n",
        "\n",
        "    def __gt__(self, other: 'ComparisonMixin') -> bool:\n",
        "        \"\"\"Implement greater-than comparison.\"\"\"\n",
        "        return not self.__le__(other)\n",
        "\n",
        "    def __ge__(self, other: 'ComparisonMixin') -> bool:\n",
        "        \"\"\"Implement greater-than-or-equal comparison.\"\"\"\n",
        "        return not self.__lt__(other)\n",
        "\n",
        "    def compute_similarity(self, other: 'ComparisonMixin', method: str = \"cosine\") -> float:\n",
        "        \"\"\"\n",
        "        Compute similarity between two results using specified metric.\n",
        "\n",
        "        Args:\n",
        "            other: Result to compare against\n",
        "            method: Similarity metric (\"cosine\", \"euclidean\", \"manhattan\", \"jaccard\")\n",
        "\n",
        "        Returns:\n",
        "            Similarity score between results\n",
        "        \"\"\"\n",
        "        # Extract numerical features for comparison\n",
        "        self_features = self._extract_numerical_features()\n",
        "        other_features = other._extract_numerical_features()\n",
        "\n",
        "        # Ensure feature vectors have same dimensionality\n",
        "        if len(self_features) != len(other_features):\n",
        "            raise ValueError(\"Results have incompatible feature dimensions for comparison\")\n",
        "\n",
        "        # Convert to numpy arrays for computation\n",
        "        vec1 = np.array(self_features)\n",
        "        vec2 = np.array(other_features)\n",
        "\n",
        "        # Compute similarity using specified method\n",
        "        if method == \"cosine\":\n",
        "            # Cosine similarity: sim = (v1 · v2) / (||v1|| × ||v2||)\n",
        "            dot_product = np.dot(vec1, vec2)\n",
        "            norms = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
        "            similarity = dot_product / norms if norms > 0 else 0.0\n",
        "\n",
        "        elif method == \"euclidean\":\n",
        "            # Euclidean distance converted to similarity\n",
        "            distance = np.linalg.norm(vec1 - vec2)\n",
        "            similarity = 1.0 / (1.0 + distance)\n",
        "\n",
        "        elif method == \"manhattan\":\n",
        "            # Manhattan distance converted to similarity\n",
        "            distance = np.sum(np.abs(vec1 - vec2))\n",
        "            similarity = 1.0 / (1.0 + distance)\n",
        "\n",
        "        elif method == \"jaccard\":\n",
        "            # Jaccard similarity for binary/categorical features\n",
        "            intersection = np.sum(np.minimum(vec1, vec2))\n",
        "            union = np.sum(np.maximum(vec1, vec2))\n",
        "            similarity = intersection / union if union > 0 else 0.0\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported similarity method: {method}\")\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    def statistical_comparison(self, other: 'ComparisonMixin') -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Perform comprehensive statistical comparison between results.\n",
        "\n",
        "        Args:\n",
        "            other: Result to compare against\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing statistical comparison metrics\n",
        "        \"\"\"\n",
        "        # Initialize comparison results\n",
        "        comparison_results = {\n",
        "            'similarity_metrics': {},\n",
        "            'statistical_tests': {},\n",
        "            'effect_sizes': {},\n",
        "            'confidence_intervals': {}\n",
        "        }\n",
        "\n",
        "        # Compute multiple similarity metrics\n",
        "        for method in [\"cosine\", \"euclidean\", \"manhattan\"]:\n",
        "            try:\n",
        "                similarity = self.compute_similarity(other, method)\n",
        "                comparison_results['similarity_metrics'][method] = similarity\n",
        "            except Exception as e:\n",
        "                comparison_results['similarity_metrics'][method] = f\"Error: {e}\"\n",
        "\n",
        "        # Statistical significance testing if both results have statistical properties\n",
        "        if hasattr(self, 'statistical_properties') and hasattr(other, 'statistical_properties'):\n",
        "            self_stats = getattr(self, 'statistical_properties')\n",
        "            other_stats = getattr(other, 'statistical_properties')\n",
        "\n",
        "            # Two-sample t-test for mean comparison\n",
        "            if self_stats and other_stats:\n",
        "                try:\n",
        "                    t_stat, p_value = self._two_sample_t_test(self_stats, other_stats)\n",
        "                    comparison_results['statistical_tests']['t_test'] = {\n",
        "                        'statistic': t_stat,\n",
        "                        'p_value': p_value,\n",
        "                        'significant': p_value < 0.05\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    comparison_results['statistical_tests']['t_test'] = f\"Error: {e}\"\n",
        "\n",
        "        return comparison_results\n",
        "\n",
        "    def _are_numerically_equal(self, a: Union[int, float], b: Union[int, float], tolerance: float = 1e-9) -> bool:\n",
        "        \"\"\"\n",
        "        Check numerical equality with specified tolerance.\n",
        "\n",
        "        Args:\n",
        "            a: First numerical value\n",
        "            b: Second numerical value\n",
        "            tolerance: Absolute tolerance for equality\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating numerical equality within tolerance\n",
        "        \"\"\"\n",
        "        return abs(a - b) <= tolerance\n",
        "\n",
        "    def _get_primary_metric(self) -> float:\n",
        "        \"\"\"\n",
        "        Get primary metric for result ranking.\n",
        "\n",
        "        Returns:\n",
        "            Primary numerical metric for comparison\n",
        "        \"\"\"\n",
        "        # Default implementation - subclasses should override\n",
        "        # Look for common similarity/score fields\n",
        "        for field_info in fields(self):\n",
        "            field_name = field_info.name\n",
        "            if any(keyword in field_name.lower() for keyword in ['similarity', 'score', 'ratio']):\n",
        "                value = getattr(self, field_name)\n",
        "                if isinstance(value, (int, float)):\n",
        "                    return value\n",
        "\n",
        "        # Fallback to hash-based comparison\n",
        "        return float(hash(self) % 1000000) / 1000000.0\n",
        "\n",
        "    def _extract_numerical_features(self) -> List[float]:\n",
        "        \"\"\"\n",
        "        Extract numerical features for similarity computation.\n",
        "\n",
        "        Returns:\n",
        "            List of numerical feature values\n",
        "        \"\"\"\n",
        "        # Extract all numerical fields as features\n",
        "        features = []\n",
        "\n",
        "        for field_info in fields(self):\n",
        "            field_value = getattr(self, field_info.name)\n",
        "\n",
        "            if isinstance(field_value, (int, float)):\n",
        "                # Direct numerical value\n",
        "                features.append(float(field_value))\n",
        "            elif isinstance(field_value, bool):\n",
        "                # Convert boolean to numerical\n",
        "                features.append(1.0 if field_value else 0.0)\n",
        "            elif isinstance(field_value, list) and field_value:\n",
        "                # Use list length as numerical feature\n",
        "                features.append(float(len(field_value)))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _two_sample_t_test(self, stats1: StatisticalProperties, stats2: StatisticalProperties) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Perform two-sample t-test for mean comparison.\n",
        "\n",
        "        Args:\n",
        "            stats1: Statistical properties of first sample\n",
        "            stats2: Statistical properties of second sample\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (t_statistic, p_value)\n",
        "        \"\"\"\n",
        "        # Extract sample statistics\n",
        "        n1, n2 = stats1.sample_size, stats2.sample_size\n",
        "        mean1, mean2 = stats1.mean, stats2.mean\n",
        "        var1, var2 = stats1.variance, stats2.variance\n",
        "\n",
        "        # Compute pooled standard error\n",
        "        pooled_se = np.sqrt(var1/n1 + var2/n2)\n",
        "\n",
        "        # Compute t-statistic\n",
        "        t_stat = (mean1 - mean2) / pooled_se if pooled_se > 0 else 0.0\n",
        "\n",
        "        # Compute degrees of freedom (Welch's approximation)\n",
        "        df = ((var1/n1 + var2/n2)**2) / ((var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1))\n",
        "\n",
        "        # Compute p-value for two-tailed test\n",
        "        p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n",
        "\n",
        "        return t_stat, p_value\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ReverseImageSearchResult(ResultValidationMixin, SerializationMixin, ComparisonMixin):\n",
        "    \"\"\"\n",
        "    Comprehensive result structure for reverse image search operations.\n",
        "\n",
        "    Implements enterprise-grade result handling with statistical analysis,\n",
        "    validation, serialization, and comparison capabilities for quantitative\n",
        "    evaluation of image provenance and context determination.\n",
        "    \"\"\"\n",
        "\n",
        "    # Primary search results with mathematical constraints\n",
        "    best_guess: str = field(\n",
        "        metadata={\n",
        "            'description': 'Best guess description from reverse image search',\n",
        "            'constraints': 'Non-empty string with maximum length 500 characters',\n",
        "            'statistical_meaning': 'Primary semantic classification of image content'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    similar_image_urls: List[str] = field(\n",
        "        default_factory=list,\n",
        "        metadata={\n",
        "            'description': 'URLs of visually similar images found in search',\n",
        "            'constraints': 'List of valid URLs with maximum 100 entries',\n",
        "            'statistical_meaning': 'Population of similar visual content for analysis'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    source_page_title: str = field(\n",
        "        metadata={\n",
        "            'description': 'Title of source page from reverse search',\n",
        "            'constraints': 'Non-empty string with maximum length 200 characters',\n",
        "            'statistical_meaning': 'Contextual metadata for image provenance'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Extended metadata for quantitative analysis\n",
        "    site_authority_score: Optional[float] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Authority score of source website [0,1]',\n",
        "            'constraints': 'Float in range [0,1] or None',\n",
        "            'statistical_meaning': 'Reliability measure for content verification'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    snippet_text: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Text snippet describing image context',\n",
        "            'constraints': 'String with maximum length 1000 characters or None',\n",
        "            'statistical_meaning': 'Semantic context for content analysis'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    confidence_score: Optional[float] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Search confidence score [0,1] based on result quality',\n",
        "            'constraints': 'Float in range [0,1] or None',\n",
        "            'statistical_meaning': 'Uncertainty quantification for search results'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Statistical analysis properties\n",
        "    statistical_properties: Optional[StatisticalProperties] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Statistical properties of search result quality metrics',\n",
        "            'constraints': 'StatisticalProperties object or None',\n",
        "            'statistical_meaning': 'Quantitative analysis of search performance'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Temporal and operational metadata\n",
        "    search_timestamp: datetime.datetime = field(\n",
        "        default_factory=datetime.datetime.utcnow,\n",
        "        metadata={\n",
        "            'description': 'UTC timestamp when search was performed',\n",
        "            'constraints': 'Valid datetime object',\n",
        "            'statistical_meaning': 'Temporal reference for result validity'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    search_duration_seconds: float = field(\n",
        "        default=0.0,\n",
        "        metadata={\n",
        "            'description': 'Time taken to complete search operation',\n",
        "            'constraints': 'Non-negative float representing seconds',\n",
        "            'statistical_meaning': 'Performance metric for search efficiency'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Result quality and provenance indicators\n",
        "    duplicate_url_count: int = field(\n",
        "        default=0,\n",
        "        metadata={\n",
        "            'description': 'Number of duplicate URLs found in similar results',\n",
        "            'constraints': 'Non-negative integer',\n",
        "            'statistical_meaning': 'Content proliferation indicator'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    unique_domain_count: int = field(\n",
        "        default=0,\n",
        "        metadata={\n",
        "            'description': 'Number of unique domains in similar image URLs',\n",
        "            'constraints': 'Non-negative integer',\n",
        "            'statistical_meaning': 'Content distribution diversity measure'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    geographic_indicators: Optional[Dict[str, Any]] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Geographic distribution indicators from result analysis',\n",
        "            'constraints': 'Dictionary with geographic metadata or None',\n",
        "            'statistical_meaning': 'Spatial distribution of content sources'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Validate reverse image search result with comprehensive constraint checking.\n",
        "\n",
        "        Implements mathematical validation of search result constraints,\n",
        "        statistical consistency verification, and business logic validation\n",
        "        for reliable quantitative analysis of image provenance.\n",
        "        \"\"\"\n",
        "        # Validate primary fields against constraints\n",
        "        if not isinstance(self.best_guess, str) or not self.best_guess.strip():\n",
        "            raise ValueError(\"best_guess must be non-empty string\")\n",
        "\n",
        "        if len(self.best_guess) > 500:\n",
        "            raise ValueError(f\"best_guess exceeds maximum length: {len(self.best_guess)} > 500\")\n",
        "\n",
        "        if not isinstance(self.source_page_title, str) or not self.source_page_title.strip():\n",
        "            raise ValueError(\"source_page_title must be non-empty string\")\n",
        "\n",
        "        if len(self.source_page_title) > 200:\n",
        "            raise ValueError(f\"source_page_title exceeds maximum length: {len(self.source_page_title)} > 200\")\n",
        "\n",
        "        # Validate URL collection constraints\n",
        "        if len(self.similar_image_urls) > 100:\n",
        "            raise ValueError(f\"similar_image_urls exceeds maximum count: {len(self.similar_image_urls)} > 100\")\n",
        "\n",
        "        # Validate each URL in collection\n",
        "        for idx, url in enumerate(self.similar_image_urls):\n",
        "            if not isinstance(url, str):\n",
        "                raise ValueError(f\"similar_image_urls[{idx}] must be string, got {type(url)}\")\n",
        "\n",
        "            if not self._is_valid_url(url):\n",
        "                raise ValueError(f\"similar_image_urls[{idx}] is not valid URL: {url}\")\n",
        "\n",
        "        # Validate optional numerical constraints\n",
        "        if self.site_authority_score is not None:\n",
        "            if not 0.0 <= self.site_authority_score <= 1.0:\n",
        "                raise ValueError(f\"site_authority_score must be in [0,1], got {self.site_authority_score}\")\n",
        "\n",
        "        if self.confidence_score is not None:\n",
        "            if not 0.0 <= self.confidence_score <= 1.0:\n",
        "                raise ValueError(f\"confidence_score must be in [0,1], got {self.confidence_score}\")\n",
        "\n",
        "        # Validate temporal constraints\n",
        "        if self.search_duration_seconds < 0:\n",
        "            raise ValueError(f\"search_duration_seconds must be non-negative, got {self.search_duration_seconds}\")\n",
        "\n",
        "        # Validate count constraints\n",
        "        if self.duplicate_url_count < 0:\n",
        "            raise ValueError(f\"duplicate_url_count must be non-negative, got {self.duplicate_url_count}\")\n",
        "\n",
        "        if self.unique_domain_count < 0:\n",
        "            raise ValueError(f\"unique_domain_count must be non-negative, got {self.unique_domain_count}\")\n",
        "\n",
        "        # Validate logical relationships between fields\n",
        "        if self.duplicate_url_count > len(self.similar_image_urls):\n",
        "            raise ValueError(\"duplicate_url_count cannot exceed total URL count\")\n",
        "\n",
        "        if self.unique_domain_count > len(self.similar_image_urls):\n",
        "            raise ValueError(\"unique_domain_count cannot exceed total URL count\")\n",
        "\n",
        "    def _validate_field_relationships(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Validate cross-field relationships specific to reverse search results.\n",
        "\n",
        "        Returns:\n",
        "            List of relationship validation violations\n",
        "        \"\"\"\n",
        "        violations = []\n",
        "\n",
        "        # Validate URL count relationships\n",
        "        if self.duplicate_url_count > len(self.similar_image_urls):\n",
        "            violations.append(\"duplicate_url_count exceeds similar_image_urls length\")\n",
        "\n",
        "        if self.unique_domain_count > len(self.similar_image_urls):\n",
        "            violations.append(\"unique_domain_count exceeds similar_image_urls length\")\n",
        "\n",
        "        # Validate confidence score vs URL count relationship\n",
        "        if (self.confidence_score is not None and\n",
        "            len(self.similar_image_urls) == 0 and\n",
        "            self.confidence_score > 0.1):\n",
        "            violations.append(\"High confidence score with no similar images is inconsistent\")\n",
        "\n",
        "        # Validate snippet text vs best guess consistency\n",
        "        if (self.snippet_text and\n",
        "            len(self.snippet_text) > 1000):\n",
        "            violations.append(\"snippet_text exceeds maximum length of 1000 characters\")\n",
        "\n",
        "        return violations\n",
        "\n",
        "    def compute_result_quality_score(self) -> float:\n",
        "        \"\"\"\n",
        "        Compute comprehensive quality score for search result.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        - Combines multiple quality indicators using weighted scoring\n",
        "        - Quality = w₁×confidence + w₂×url_diversity + w₃×authority + w₄×completeness\n",
        "        - Weights optimized for image provenance determination accuracy\n",
        "\n",
        "        Returns:\n",
        "            Quality score in range [0,1] where higher values indicate better results\n",
        "        \"\"\"\n",
        "        # Initialize quality components\n",
        "        quality_components = {}\n",
        "\n",
        "        # Confidence score component (weight: 0.3)\n",
        "        confidence_component = self.confidence_score if self.confidence_score is not None else 0.5\n",
        "        quality_components['confidence'] = confidence_component * 0.3\n",
        "\n",
        "        # URL diversity component (weight: 0.25)\n",
        "        if len(self.similar_image_urls) > 0:\n",
        "            diversity_ratio = self.unique_domain_count / len(self.similar_image_urls)\n",
        "            diversity_component = min(diversity_ratio * 2.0, 1.0)  # Scale to [0,1]\n",
        "        else:\n",
        "            diversity_component = 0.0\n",
        "        quality_components['diversity'] = diversity_component * 0.25\n",
        "\n",
        "        # Authority score component (weight: 0.2)\n",
        "        authority_component = self.site_authority_score if self.site_authority_score is not None else 0.5\n",
        "        quality_components['authority'] = authority_component * 0.2\n",
        "\n",
        "        # Completeness component (weight: 0.15)\n",
        "        completeness_score = 0.0\n",
        "        if self.best_guess and len(self.best_guess.strip()) > 0:\n",
        "            completeness_score += 0.4\n",
        "        if self.snippet_text and len(self.snippet_text.strip()) > 0:\n",
        "            completeness_score += 0.3\n",
        "        if len(self.similar_image_urls) > 0:\n",
        "            completeness_score += 0.3\n",
        "        quality_components['completeness'] = completeness_score * 0.15\n",
        "\n",
        "        # Performance component (weight: 0.1)\n",
        "        if self.search_duration_seconds > 0:\n",
        "            # Reward faster searches (exponential decay)\n",
        "            performance_score = np.exp(-self.search_duration_seconds / 10.0)\n",
        "        else:\n",
        "            performance_score = 1.0\n",
        "        quality_components['performance'] = performance_score * 0.1\n",
        "\n",
        "        # Compute total quality score\n",
        "        total_quality = sum(quality_components.values())\n",
        "\n",
        "        return min(total_quality, 1.0)  # Ensure score is in [0,1]\n",
        "\n",
        "    def analyze_content_distribution(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze geographic and temporal distribution of similar content.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing distribution analysis results\n",
        "        \"\"\"\n",
        "        # Initialize distribution analysis\n",
        "        distribution_analysis = {\n",
        "            'domain_distribution': defaultdict(int),\n",
        "            'tld_distribution': defaultdict(int),\n",
        "            'url_pattern_analysis': {},\n",
        "            'content_proliferation_metrics': {}\n",
        "        }\n",
        "\n",
        "        # Analyze domain patterns in similar URLs\n",
        "        for url in self.similar_image_urls:\n",
        "            try:\n",
        "                # Extract domain from URL\n",
        "                from urllib.parse import urlparse\n",
        "                parsed_url = urlparse(url)\n",
        "                domain = parsed_url.netloc.lower()\n",
        "\n",
        "                # Count domain occurrences\n",
        "                distribution_analysis['domain_distribution'][domain] += 1\n",
        "\n",
        "                # Extract top-level domain\n",
        "                tld = domain.split('.')[-1] if '.' in domain else 'unknown'\n",
        "                distribution_analysis['tld_distribution'][tld] += 1\n",
        "\n",
        "            except Exception:\n",
        "                # Handle malformed URLs gracefully\n",
        "                distribution_analysis['domain_distribution']['malformed'] += 1\n",
        "\n",
        "        # Compute content proliferation metrics\n",
        "        total_urls = len(self.similar_image_urls)\n",
        "        unique_domains = len(distribution_analysis['domain_distribution'])\n",
        "\n",
        "        if total_urls > 0:\n",
        "            proliferation_metrics = {\n",
        "                'proliferation_ratio': self.duplicate_url_count / total_urls,\n",
        "                'domain_diversity_ratio': unique_domains / total_urls,\n",
        "                'concentration_index': self._compute_concentration_index(\n",
        "                    distribution_analysis['domain_distribution']\n",
        "                ),\n",
        "                'viral_coefficient': self._compute_viral_coefficient()\n",
        "            }\n",
        "            distribution_analysis['content_proliferation_metrics'] = proliferation_metrics\n",
        "\n",
        "        return dict(distribution_analysis)\n",
        "\n",
        "    def _compute_concentration_index(self, domain_counts: Dict[str, int]) -> float:\n",
        "        \"\"\"\n",
        "        Compute Herfindahl-Hirschman Index for domain concentration.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        HHI = Σᵢ (sᵢ)² where sᵢ is market share of domain i\n",
        "        HHI ∈ [1/n, 1] where n is number of domains\n",
        "\n",
        "        Args:\n",
        "            domain_counts: Dictionary mapping domains to occurrence counts\n",
        "\n",
        "        Returns:\n",
        "            Concentration index where higher values indicate more concentration\n",
        "        \"\"\"\n",
        "        if not domain_counts:\n",
        "            return 0.0\n",
        "\n",
        "        # Compute total occurrences\n",
        "        total_count = sum(domain_counts.values())\n",
        "\n",
        "        # Compute market shares and HHI\n",
        "        hhi = sum((count / total_count) ** 2 for count in domain_counts.values())\n",
        "\n",
        "        return hhi\n",
        "\n",
        "    def _compute_viral_coefficient(self) -> float:\n",
        "        \"\"\"\n",
        "        Compute viral coefficient indicating content spread velocity.\n",
        "\n",
        "        Returns:\n",
        "            Viral coefficient based on URL count and search performance\n",
        "        \"\"\"\n",
        "        # Base viral score on URL count and diversity\n",
        "        if len(self.similar_image_urls) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Factor in search speed (faster discovery suggests viral content)\n",
        "        speed_factor = 1.0 / (1.0 + self.search_duration_seconds / 5.0)\n",
        "\n",
        "        # Factor in domain diversity (viral content spreads across platforms)\n",
        "        diversity_factor = min(self.unique_domain_count / 10.0, 1.0)\n",
        "\n",
        "        # Factor in absolute URL count\n",
        "        count_factor = min(len(self.similar_image_urls) / 50.0, 1.0)\n",
        "\n",
        "        # Compute composite viral coefficient\n",
        "        viral_coefficient = (speed_factor * 0.3 + diversity_factor * 0.4 + count_factor * 0.3)\n",
        "\n",
        "        return viral_coefficient\n",
        "\n",
        "    def generate_provenance_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate comprehensive provenance report for image authenticity assessment.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing detailed provenance analysis\n",
        "        \"\"\"\n",
        "        # Initialize provenance report structure\n",
        "        provenance_report = {\n",
        "            'authenticity_indicators': {},\n",
        "            'distribution_analysis': self.analyze_content_distribution(),\n",
        "            'quality_assessment': {\n",
        "                'overall_quality_score': self.compute_result_quality_score(),\n",
        "                'confidence_level': self.confidence_score,\n",
        "                'completeness_score': self._compute_completeness_score()\n",
        "            },\n",
        "            'risk_assessment': {\n",
        "                'manipulation_risk': self._assess_manipulation_risk(),\n",
        "                'viral_propagation_risk': self._compute_viral_coefficient(),\n",
        "                'source_credibility': self.site_authority_score\n",
        "            },\n",
        "            'recommendations': self._generate_provenance_recommendations()\n",
        "        }\n",
        "\n",
        "        return provenance_report\n",
        "\n",
        "    def _compute_completeness_score(self) -> float:\n",
        "        \"\"\"Compute completeness score based on available metadata.\"\"\"\n",
        "        completeness_factors = []\n",
        "\n",
        "        # Best guess completeness\n",
        "        if self.best_guess and len(self.best_guess.strip()) > 10:\n",
        "            completeness_factors.append(1.0)\n",
        "        else:\n",
        "            completeness_factors.append(0.0)\n",
        "\n",
        "        # Snippet text completeness\n",
        "        if self.snippet_text and len(self.snippet_text.strip()) > 20:\n",
        "            completeness_factors.append(1.0)\n",
        "        else:\n",
        "            completeness_factors.append(0.5 if self.snippet_text else 0.0)\n",
        "\n",
        "        # URL collection completeness\n",
        "        if len(self.similar_image_urls) >= 5:\n",
        "            completeness_factors.append(1.0)\n",
        "        elif len(self.similar_image_urls) > 0:\n",
        "            completeness_factors.append(len(self.similar_image_urls) / 5.0)\n",
        "        else:\n",
        "            completeness_factors.append(0.0)\n",
        "\n",
        "        # Authority score availability\n",
        "        completeness_factors.append(1.0 if self.site_authority_score is not None else 0.0)\n",
        "\n",
        "        return statistics.mean(completeness_factors)\n",
        "\n",
        "    def _assess_manipulation_risk(self) -> float:\n",
        "        \"\"\"Assess risk of image manipulation based on search patterns.\"\"\"\n",
        "        risk_factors = []\n",
        "\n",
        "        # High duplicate count suggests potential manipulation\n",
        "        if len(self.similar_image_urls) > 0:\n",
        "            duplicate_ratio = self.duplicate_url_count / len(self.similar_image_urls)\n",
        "            risk_factors.append(duplicate_ratio)\n",
        "\n",
        "        # Low confidence with many results suggests inconsistency\n",
        "        if self.confidence_score is not None and len(self.similar_image_urls) > 10:\n",
        "            if self.confidence_score < 0.3:\n",
        "                risk_factors.append(0.7)\n",
        "            else:\n",
        "                risk_factors.append(0.0)\n",
        "\n",
        "        # Rapid search completion might indicate cached/manipulated content\n",
        "        if self.search_duration_seconds < 1.0 and len(self.similar_image_urls) > 20:\n",
        "            risk_factors.append(0.6)\n",
        "        else:\n",
        "            risk_factors.append(0.0)\n",
        "\n",
        "        return statistics.mean(risk_factors) if risk_factors else 0.0\n",
        "\n",
        "    def _generate_provenance_recommendations(self) -> List[str]:\n",
        "        \"\"\"Generate actionable recommendations for provenance verification.\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # Quality-based recommendations\n",
        "        quality_score = self.compute_result_quality_score()\n",
        "        if quality_score < 0.3:\n",
        "            recommendations.append(\"Low quality search results - consider additional verification methods\")\n",
        "        elif quality_score > 0.8:\n",
        "            recommendations.append(\"High quality search results provide strong provenance indicators\")\n",
        "\n",
        "        # Authority-based recommendations\n",
        "        if self.site_authority_score is not None:\n",
        "            if self.site_authority_score < 0.3:\n",
        "                recommendations.append(\"Low source authority - verify information through additional channels\")\n",
        "            elif self.site_authority_score > 0.8:\n",
        "                recommendations.append(\"High source authority increases confidence in provenance\")\n",
        "\n",
        "        # Distribution-based recommendations\n",
        "        if self.unique_domain_count < 3 and len(self.similar_image_urls) > 10:\n",
        "            recommendations.append(\"Limited domain diversity suggests potential content manipulation\")\n",
        "\n",
        "        # Risk-based recommendations\n",
        "        manipulation_risk = self._assess_manipulation_risk()\n",
        "        if manipulation_risk > 0.5:\n",
        "            recommendations.append(\"High manipulation risk detected - conduct forensic analysis\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class FeatureMatchResult(ResultValidationMixin, SerializationMixin, ComparisonMixin):\n",
        "    \"\"\"\n",
        "    Comprehensive result structure for feature matching operations.\n",
        "\n",
        "    Implements rigorous mathematical validation, statistical analysis, and\n",
        "    quantitative assessment of ORB feature matching performance for\n",
        "    enterprise-grade computer vision applications.\n",
        "    \"\"\"\n",
        "\n",
        "    # Primary matching metrics with mathematical constraints\n",
        "    similarity_ratio: float = field(\n",
        "        metadata={\n",
        "            'description': 'Ratio of good matches to total/min keypoints [0,1]',\n",
        "            'constraints': 'Float in range [0,1]',\n",
        "            'statistical_meaning': 'Probability estimate of visual similarity',\n",
        "            'mathematical_foundation': 'P(match) = |good_matches| / normalization_denominator'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    total_matches: int = field(\n",
        "        metadata={\n",
        "            'description': 'Total number of descriptor matches attempted',\n",
        "            'constraints': 'Non-negative integer',\n",
        "            'statistical_meaning': 'Sample size for matching statistical analysis',\n",
        "            'mathematical_foundation': 'n = |{(qi, tj) : d_H(qi, tj) computed}|'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    good_matches: int = field(\n",
        "        metadata={\n",
        "            'description': 'Number of matches passing quality threshold',\n",
        "            'constraints': 'Non-negative integer ≤ total_matches',\n",
        "            'statistical_meaning': 'Successful matching events for similarity assessment',\n",
        "            'mathematical_foundation': 'k = |{matches : d_H ≤ threshold or ratio_test_passed}|'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    keypoints_image1: int = field(\n",
        "        metadata={\n",
        "            'description': 'Number of keypoints detected in first image',\n",
        "            'constraints': 'Non-negative integer',\n",
        "            'statistical_meaning': 'Feature density measure for first image',\n",
        "            'mathematical_foundation': 'n₁ = |{p : Harris_response(p) > threshold}|'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    keypoints_image2: int = field(\n",
        "        metadata={\n",
        "            'description': 'Number of keypoints detected in second image',\n",
        "            'constraints': 'Non-negative integer',\n",
        "            'statistical_meaning': 'Feature density measure for second image',\n",
        "            'mathematical_foundation': 'n₂ = |{p : Harris_response(p) > threshold}|'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    normalization_strategy: str = field(\n",
        "        metadata={\n",
        "            'description': 'Strategy used for similarity ratio computation',\n",
        "            'constraints': 'Must be \"total_matches\" or \"min_keypoints\"',\n",
        "            'statistical_meaning': 'Normalization method affecting probability interpretation',\n",
        "            'mathematical_foundation': 'Denominator choice for ratio = k/denominator'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Advanced matching quality metrics\n",
        "    confidence_level: Optional[float] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Statistical confidence in matching result [0,1]',\n",
        "            'constraints': 'Float in range [0,1] or None',\n",
        "            'statistical_meaning': 'Uncertainty quantification for match quality',\n",
        "            'mathematical_foundation': 'P(true_positive | observed_matches)'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    match_distance_statistics: Optional[StatisticalProperties] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Statistical properties of Hamming distances',\n",
        "            'constraints': 'StatisticalProperties object or None',\n",
        "            'statistical_meaning': 'Distribution analysis of descriptor distances',\n",
        "            'mathematical_foundation': 'Statistical analysis of {d_H(qi, tj) : matched}'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Geometric verification results\n",
        "    geometric_verification_passed: Optional[bool] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Whether matches pass geometric consistency check',\n",
        "            'constraints': 'Boolean or None if not performed',\n",
        "            'statistical_meaning': 'Spatial coherence validation of matching',\n",
        "            'mathematical_foundation': 'RANSAC homography estimation success'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    homography_inlier_count: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Number of matches consistent with estimated homography',\n",
        "            'constraints': 'Non-negative integer ≤ good_matches or None',\n",
        "            'statistical_meaning': 'Geometrically consistent correspondences',\n",
        "            'mathematical_foundation': '|{matches : ||H·p₁ - p₂|| < ε}|'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    homography_inlier_ratio: Optional[float] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Ratio of homography inliers to good matches [0,1]',\n",
        "            'constraints': 'Float in range [0,1] or None',\n",
        "            'statistical_meaning': 'Geometric consistency measure',\n",
        "            'mathematical_foundation': 'inlier_ratio = inlier_count / good_matches'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Performance and computational metrics\n",
        "    matching_time_seconds: float = field(\n",
        "        default=0.0,\n",
        "        metadata={\n",
        "            'description': 'Time taken for complete matching operation',\n",
        "            'constraints': 'Non-negative float',\n",
        "            'statistical_meaning': 'Computational efficiency measure',\n",
        "            'mathematical_foundation': 'Wall-clock time for O(n₁·n₂·k) operations'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    keypoint_detection_time_seconds: float = field(\n",
        "        default=0.0,\n",
        "        metadata={\n",
        "            'description': 'Time taken for keypoint detection in both images',\n",
        "            'constraints': 'Non-negative float',\n",
        "            'statistical_meaning': 'Feature extraction efficiency measure',\n",
        "            'mathematical_foundation': 'Time for ORB detection and description'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Match quality distribution analysis\n",
        "    distance_threshold_used: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Hamming distance threshold for good match classification',\n",
        "            'constraints': 'Integer in range [0,256] or None',\n",
        "            'statistical_meaning': 'Decision boundary for binary classification',\n",
        "            'mathematical_foundation': 'τ : d_H ≤ τ ⟹ good_match'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    lowe_ratio_threshold: Optional[float] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            'description': 'Ratio threshold used for Lowe\\'s ratio test',\n",
        "            'constraints': 'Float in range (0,1) or None',\n",
        "            'statistical_meaning': 'Disambiguation threshold for nearest neighbors',\n",
        "            'mathematical_foundation': 'r : d₁/d₂ < r ⟹ unambiguous_match'\n",
        "        }\n",
        "    )\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Validate feature matching result with comprehensive mathematical verification.\n",
        "\n",
        "        Implements rigorous validation of matching statistics, mathematical\n",
        "        constraints, and statistical consistency for reliable quantitative\n",
        "        analysis of visual feature correspondence.\n",
        "        \"\"\"\n",
        "        # Validate similarity ratio mathematical constraint\n",
        "        if not 0.0 <= self.similarity_ratio <= 1.0:\n",
        "            raise ValueError(f\"similarity_ratio must be in [0,1], got {self.similarity_ratio}\")\n",
        "\n",
        "        # Validate count constraints and relationships\n",
        "        if self.total_matches < 0:\n",
        "            raise ValueError(f\"total_matches must be non-negative, got {self.total_matches}\")\n",
        "\n",
        "        if self.good_matches < 0:\n",
        "            raise ValueError(f\"good_matches must be non-negative, got {self.good_matches}\")\n",
        "\n",
        "        if self.good_matches > self.total_matches:\n",
        "            raise ValueError(f\"good_matches ({self.good_matches}) cannot exceed total_matches ({self.total_matches})\")\n",
        "\n",
        "        if self.keypoints_image1 < 0:\n",
        "            raise ValueError(f\"keypoints_image1 must be non-negative, got {self.keypoints_image1}\")\n",
        "\n",
        "        if self.keypoints_image2 < 0:\n",
        "            raise ValueError(f\"keypoints_image2 must be non-negative, got {self.keypoints_image2}\")\n",
        "\n",
        "        # Validate normalization strategy\n",
        "        valid_strategies = {\"total_matches\", \"min_keypoints\"}\n",
        "        if self.normalization_strategy not in valid_strategies:\n",
        "            raise ValueError(f\"normalization_strategy must be one of {valid_strategies}, got {self.normalization_strategy}\")\n",
        "\n",
        "        # Validate optional confidence level\n",
        "        if self.confidence_level is not None:\n",
        "            if not 0.0 <= self.confidence_level <= 1.0:\n",
        "                raise ValueError(f\"confidence_level must be in [0,1], got {self.confidence_level}\")\n",
        "\n",
        "        # Validate geometric verification constraints\n",
        "        if self.homography_inlier_count is not None:\n",
        "            if self.homography_inlier_count < 0:\n",
        "                raise ValueError(f\"homography_inlier_count must be non-negative, got {self.homography_inlier_count}\")\n",
        "\n",
        "            if self.homography_inlier_count > self.good_matches:\n",
        "                raise ValueError(f\"homography_inlier_count ({self.homography_inlier_count}) cannot exceed good_matches ({self.good_matches})\")\n",
        "\n",
        "        if self.homography_inlier_ratio is not None:\n",
        "            if not 0.0 <= self.homography_inlier_ratio <= 1.0:\n",
        "                raise ValueError(f\"homography_inlier_ratio must be in [0,1], got {self.homography_inlier_ratio}\")\n",
        "\n",
        "        # Validate temporal constraints\n",
        "        if self.matching_time_seconds < 0:\n",
        "            raise ValueError(f\"matching_time_seconds must be non-negative, got {self.matching_time_seconds}\")\n",
        "\n",
        "        if self.keypoint_detection_time_seconds < 0:\n",
        "            raise ValueError(f\"keypoint_detection_time_seconds must be non-negative, got {self.keypoint_detection_time_seconds}\")\n",
        "\n",
        "        # Validate threshold parameters if provided\n",
        "        if self.distance_threshold_used is not None:\n",
        "            if not 0 <= self.distance_threshold_used <= 256:\n",
        "                raise ValueError(f\"distance_threshold_used must be in [0,256], got {self.distance_threshold_used}\")\n",
        "\n",
        "        if self.lowe_ratio_threshold is not None:\n",
        "            if not 0.0 < self.lowe_ratio_threshold < 1.0:\n",
        "                raise ValueError(f\"lowe_ratio_threshold must be in (0,1), got {self.lowe_ratio_threshold}\")\n",
        "\n",
        "    def _validate_field_relationships(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Validate mathematical relationships between feature matching fields.\n",
        "\n",
        "        Returns:\n",
        "            List of relationship validation violations\n",
        "        \"\"\"\n",
        "        violations = []\n",
        "\n",
        "        # Validate similarity ratio computation consistency\n",
        "        if self.normalization_strategy == \"total_matches\" and self.total_matches > 0:\n",
        "            expected_ratio = self.good_matches / self.total_matches\n",
        "            if abs(self.similarity_ratio - expected_ratio) > 1e-6:\n",
        "                violations.append(f\"similarity_ratio inconsistent with total_matches normalization\")\n",
        "\n",
        "        elif self.normalization_strategy == \"min_keypoints\":\n",
        "            min_keypoints = min(self.keypoints_image1, self.keypoints_image2)\n",
        "            if min_keypoints > 0:\n",
        "                expected_ratio = min(self.good_matches / min_keypoints, 1.0)\n",
        "                if abs(self.similarity_ratio - expected_ratio) > 1e-6:\n",
        "                    violations.append(f\"similarity_ratio inconsistent with min_keypoints normalization\")\n",
        "\n",
        "        # Validate homography inlier ratio consistency\n",
        "        if (self.homography_inlier_count is not None and\n",
        "            self.homography_inlier_ratio is not None and\n",
        "            self.good_matches > 0):\n",
        "            expected_inlier_ratio = self.homography_inlier_count / self.good_matches\n",
        "            if abs(self.homography_inlier_ratio - expected_inlier_ratio) > 1e-6:\n",
        "                violations.append(\"homography_inlier_ratio inconsistent with inlier_count\")\n",
        "\n",
        "        # Validate geometric verification consistency\n",
        "        if (self.geometric_verification_passed is True and\n",
        "            self.homography_inlier_count is not None and\n",
        "            self.homography_inlier_count < 4):\n",
        "            violations.append(\"geometric_verification_passed=True but insufficient inliers for homography\")\n",
        "\n",
        "        return violations\n",
        "\n",
        "    def compute_match_quality_score(self) -> float:\n",
        "        \"\"\"\n",
        "        Compute comprehensive match quality score based on multiple factors.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        Quality = w₁×similarity + w₂×geometric + w₃×statistical + w₄×efficiency\n",
        "\n",
        "        Weights optimized for visual similarity assessment:\n",
        "        - w₁ = 0.4 (similarity ratio primary indicator)\n",
        "        - w₂ = 0.3 (geometric consistency crucial for reliability)\n",
        "        - w₃ = 0.2 (statistical confidence important for uncertainty)\n",
        "        - w₄ = 0.1 (computational efficiency secondary concern)\n",
        "\n",
        "        Returns:\n",
        "            Quality score in range [0,1] where higher values indicate better matches\n",
        "        \"\"\"\n",
        "        # Initialize quality components with weights\n",
        "        quality_components = {}\n",
        "\n",
        "        # Similarity ratio component (weight: 0.4)\n",
        "        quality_components['similarity'] = self.similarity_ratio * 0.4\n",
        "\n",
        "        # Geometric verification component (weight: 0.3)\n",
        "        if self.homography_inlier_ratio is not None:\n",
        "            geometric_component = self.homography_inlier_ratio\n",
        "        elif self.geometric_verification_passed is True:\n",
        "            geometric_component = 0.8  # High score for passed verification\n",
        "        elif self.geometric_verification_passed is False:\n",
        "            geometric_component = 0.2  # Low score for failed verification\n",
        "        else:\n",
        "            geometric_component = 0.5  # Neutral score if not performed\n",
        "\n",
        "        quality_components['geometric'] = geometric_component * 0.3\n",
        "\n",
        "        # Statistical confidence component (weight: 0.2)\n",
        "        if self.confidence_level is not None:\n",
        "            statistical_component = self.confidence_level\n",
        "        else:\n",
        "            # Estimate confidence from match statistics\n",
        "            if self.total_matches > 0:\n",
        "                # Higher total matches and good match ratio suggest higher confidence\n",
        "                match_ratio = self.good_matches / self.total_matches\n",
        "                sample_size_factor = min(self.total_matches / 100.0, 1.0)\n",
        "                statistical_component = match_ratio * sample_size_factor\n",
        "            else:\n",
        "                statistical_component = 0.0\n",
        "\n",
        "        quality_components['statistical'] = statistical_component * 0.2\n",
        "\n",
        "        # Computational efficiency component (weight: 0.1)\n",
        "        total_time = self.matching_time_seconds + self.keypoint_detection_time_seconds\n",
        "        if total_time > 0:\n",
        "            # Reward faster processing (exponential decay)\n",
        "            efficiency_component = np.exp(-total_time / 5.0)\n",
        "        else:\n",
        "            efficiency_component = 1.0\n",
        "\n",
        "        quality_components['efficiency'] = efficiency_component * 0.1\n",
        "\n",
        "        # Compute total quality score\n",
        "        total_quality = sum(quality_components.values())\n",
        "\n",
        "        return min(total_quality, 1.0)  # Ensure score is in [0,1]\n",
        "\n",
        "    def estimate_false_positive_rate(self) -> float:\n",
        "        \"\"\"\n",
        "        Estimate false positive rate based on matching statistics and thresholds.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        For random binary descriptors with uniform bit distribution:\n",
        "        P(d_H ≤ τ) = Σᵢ₌₀ᵗ C(256,i) × p^i × (1-p)^(256-i)\n",
        "        where p = 0.5 for uniform distribution\n",
        "\n",
        "        Returns:\n",
        "            Estimated false positive rate [0,1]\n",
        "        \"\"\"\n",
        "        # Use distance threshold if available\n",
        "        if self.distance_threshold_used is not None:\n",
        "            # Compute binomial probability for Hamming distance ≤ threshold\n",
        "            n_bits = 256  # Standard ORB descriptor length\n",
        "            threshold = self.distance_threshold_used\n",
        "\n",
        "            # For large n, use normal approximation to binomial\n",
        "            # μ = n×p, σ² = n×p×(1-p) where p = 0.5 for random bits\n",
        "            mu = n_bits * 0.5\n",
        "            sigma = np.sqrt(n_bits * 0.5 * 0.5)\n",
        "\n",
        "            # Standardize and compute cumulative probability\n",
        "            z_score = (threshold + 0.5 - mu) / sigma  # Continuity correction\n",
        "            false_positive_rate = stats.norm.cdf(z_score)\n",
        "\n",
        "            return min(false_positive_rate, 1.0)\n",
        "\n",
        "        # Estimate from match statistics if threshold unavailable\n",
        "        if self.total_matches > 0:\n",
        "            # Conservative estimate based on match ratio\n",
        "            # Assume false positive rate scales with observed match density\n",
        "            match_density = self.good_matches / self.total_matches\n",
        "\n",
        "            # Apply heuristic correction for typical image matching scenarios\n",
        "            estimated_fpr = match_density * 0.1  # 10% of matches assumed false positive\n",
        "\n",
        "            return min(estimated_fpr, 0.5)  # Cap at 50% for conservative estimation\n",
        "\n",
        "        # Default conservative estimate\n",
        "        return 0.1\n",
        "\n",
        "    def compute_statistical_power(self, effect_size: float = 0.5) -> float:\n",
        "        \"\"\"\n",
        "        Compute statistical power for detecting true visual similarity.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        Power = P(reject H₀ | H₁ true) where:\n",
        "        - H₀: images are not similar\n",
        "        - H₁: images are visually similar\n",
        "        - Effect size quantifies similarity magnitude\n",
        "\n",
        "        Args:\n",
        "            effect_size: Expected Cohen's d for similarity detection\n",
        "\n",
        "        Returns:\n",
        "            Statistical power estimate [0,1]\n",
        "        \"\"\"\n",
        "        # Validate input parameters\n",
        "        if effect_size <= 0:\n",
        "            raise ValueError(\"Effect size must be positive\")\n",
        "\n",
        "        # Use sample size based on keypoint counts\n",
        "        effective_sample_size = min(self.keypoints_image1, self.keypoints_image2)\n",
        "\n",
        "        if effective_sample_size < 2:\n",
        "            return 0.0  # Insufficient sample for meaningful power calculation\n",
        "\n",
        "        # Compute non-centrality parameter\n",
        "        ncp = effect_size * np.sqrt(effective_sample_size)\n",
        "\n",
        "        # Use standard significance level (α = 0.05) for two-tailed test\n",
        "        alpha = 0.05\n",
        "        critical_value = stats.norm.ppf(1 - alpha/2)\n",
        "\n",
        "        # Compute statistical power\n",
        "        power = 1 - stats.norm.cdf(critical_value - ncp) + stats.norm.cdf(-critical_value - ncp)\n",
        "\n",
        "        return power\n",
        "\n",
        "    def generate_matching_diagnostics(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate comprehensive diagnostic report for matching performance analysis.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing detailed diagnostic information\n",
        "        \"\"\"\n",
        "        # Initialize diagnostic report\n",
        "        diagnostics = {\n",
        "            'quality_assessment': {\n",
        "                'overall_quality_score': self.compute_match_quality_score(),\n",
        "                'estimated_false_positive_rate': self.estimate_false_positive_rate(),\n",
        "                'statistical_power': self.compute_statistical_power(),\n",
        "                'confidence_level': self.confidence_level\n",
        "            },\n",
        "            'performance_metrics': {\n",
        "                'matching_efficiency': self._compute_matching_efficiency(),\n",
        "                'keypoint_density': self._compute_keypoint_density(),\n",
        "                'computational_cost': self._compute_computational_cost()\n",
        "            },\n",
        "            'statistical_analysis': {\n",
        "                'sample_adequacy': self._assess_sample_adequacy(),\n",
        "                'normalization_impact': self._analyze_normalization_impact(),\n",
        "                'threshold_sensitivity': self._analyze_threshold_sensitivity()\n",
        "            },\n",
        "            'recommendations': self._generate_matching_recommendations()\n",
        "        }\n",
        "\n",
        "        # Add distance statistics if available\n",
        "        if self.match_distance_statistics:\n",
        "            diagnostics['distance_statistics'] = {\n",
        "                'mean_distance': self.match_distance_statistics.mean,\n",
        "                'distance_variance': self.match_distance_statistics.variance,\n",
        "                'distance_distribution': self.match_distance_statistics.distribution_type,\n",
        "                'quality_distribution': self._assess_distance_distribution_quality()\n",
        "            }\n",
        "\n",
        "        return diagnostics\n",
        "\n",
        "    def _compute_matching_efficiency(self) -> float:\n",
        "        \"\"\"Compute matching efficiency based on matches per unit time.\"\"\"\n",
        "        total_time = self.matching_time_seconds + self.keypoint_detection_time_seconds\n",
        "        if total_time > 0 and self.total_matches > 0:\n",
        "            return self.total_matches / total_time\n",
        "        return 0.0\n",
        "\n",
        "    def _compute_keypoint_density(self) -> Dict[str, float]:\n",
        "        \"\"\"Compute keypoint density metrics for both images.\"\"\"\n",
        "        # Assume standard image size for density calculation\n",
        "        standard_image_area = 640 * 480  # VGA resolution\n",
        "\n",
        "        return {\n",
        "            'density_image1': self.keypoints_image1 / standard_image_area,\n",
        "            'density_image2': self.keypoints_image2 / standard_image_area,\n",
        "            'density_ratio': (self.keypoints_image2 / max(self.keypoints_image1, 1))\n",
        "        }\n",
        "\n",
        "    def _compute_computational_cost(self) -> Dict[str, float]:\n",
        "        \"\"\"Compute computational cost metrics.\"\"\"\n",
        "        total_comparisons = self.keypoints_image1 * self.keypoints_image2\n",
        "\n",
        "        return {\n",
        "            'total_comparisons': total_comparisons,\n",
        "            'time_per_comparison': (self.matching_time_seconds / max(total_comparisons, 1)) * 1e6,  # microseconds\n",
        "            'matches_per_second': self.total_matches / max(self.matching_time_seconds, 1e-6)\n",
        "        }\n",
        "\n",
        "    def _assess_sample_adequacy(self) -> Dict[str, Any]:\n",
        "        \"\"\"Assess adequacy of sample size for statistical inference.\"\"\"\n",
        "        min_keypoints = min(self.keypoints_image1, self.keypoints_image2)\n",
        "\n",
        "        adequacy_assessment = {\n",
        "            'minimum_sample_size': min_keypoints >= 30,  # Rule of thumb for normal approximation\n",
        "            'recommended_minimum': 30,\n",
        "            'actual_minimum': min_keypoints,\n",
        "            'adequacy_ratio': min_keypoints / 30.0,\n",
        "            'power_adequate': self.compute_statistical_power() >= 0.8\n",
        "        }\n",
        "\n",
        "        return adequacy_assessment\n",
        "\n",
        "    def _analyze_normalization_impact(self) -> Dict[str, float]:\n",
        "        \"\"\"Analyze impact of normalization strategy choice.\"\"\"\n",
        "        # Compute alternative similarity ratio\n",
        "        if self.normalization_strategy == \"total_matches\" and self.total_matches > 0:\n",
        "            alt_ratio = min(self.good_matches / min(self.keypoints_image1, self.keypoints_image2), 1.0) if min(self.keypoints_image1, self.keypoints_image2) > 0 else 0.0\n",
        "            impact = abs(self.similarity_ratio - alt_ratio)\n",
        "        else:\n",
        "            alt_ratio = self.good_matches / max(self.total_matches, 1)\n",
        "            impact = abs(self.similarity_ratio - alt_ratio)\n",
        "\n",
        "        return {\n",
        "            'current_ratio': self.similarity_ratio,\n",
        "            'alternative_ratio': alt_ratio,\n",
        "            'normalization_impact': impact,\n",
        "            'strategy_sensitivity': impact / max(self.similarity_ratio, 1e-6)\n",
        "        }\n",
        "\n",
        "    def _analyze_threshold_sensitivity(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze sensitivity to threshold parameter choices.\"\"\"\n",
        "        sensitivity_analysis = {\n",
        "            'threshold_used': self.distance_threshold_used,\n",
        "            'estimated_sensitivity': None,\n",
        "            'robustness_score': None\n",
        "        }\n",
        "\n",
        "        # Estimate threshold sensitivity if distance statistics available\n",
        "        if self.match_distance_statistics and self.distance_threshold_used is not None:\n",
        "            # Compute how many matches would change with ±10% threshold variation\n",
        "            threshold_std = self.match_distance_statistics.standard_deviation\n",
        "            sensitivity_estimate = threshold_std / max(self.distance_threshold_used, 1)\n",
        "\n",
        "            sensitivity_analysis.update({\n",
        "                'estimated_sensitivity': sensitivity_estimate,\n",
        "                'robustness_score': 1.0 / (1.0 + sensitivity_estimate),\n",
        "                'distance_std': threshold_std\n",
        "            })\n",
        "\n",
        "        return sensitivity_analysis\n",
        "\n",
        "    def _assess_distance_distribution_quality(self) -> Dict[str, Any]:\n",
        "        \"\"\"Assess quality of distance distribution for reliable matching.\"\"\"\n",
        "        if not self.match_distance_statistics:\n",
        "            return {'status': 'no_data'}\n",
        "\n",
        "        stats_obj = self.match_distance_statistics\n",
        "\n",
        "        return {\n",
        "            'distribution_type': stats_obj.distribution_type,\n",
        "            'normality_p_value': stats_obj.p_value,\n",
        "            'distribution_quality': 'good' if stats_obj.goodness_of_fit and stats_obj.goodness_of_fit > 0.05 else 'poor',\n",
        "            'skewness': stats_obj.skewness,\n",
        "            'kurtosis': stats_obj.kurtosis,\n",
        "            'separability': self._compute_distance_separability()\n",
        "        }\n",
        "\n",
        "    def _compute_distance_separability(self) -> float:\n",
        "        \"\"\"Compute separability between good and bad matches based on distance distribution.\"\"\"\n",
        "        if not self.match_distance_statistics:\n",
        "            return 0.0\n",
        "\n",
        "        # Estimate separability using distance statistics\n",
        "        # Good separability means good matches have clearly different distance distribution\n",
        "        mean_distance = self.match_distance_statistics.mean\n",
        "        std_distance = self.match_distance_statistics.standard_deviation\n",
        "\n",
        "        # Compute coefficient of variation as separability proxy\n",
        "        if mean_distance > 0:\n",
        "            cv = std_distance / mean_distance\n",
        "            # Lower CV suggests better separability\n",
        "            separability = 1.0 / (1.0 + cv)\n",
        "        else:\n",
        "            separability = 0.0\n",
        "\n",
        "        return separability\n",
        "\n",
        "    def _generate_matching_recommendations(self) -> List[str]:\n",
        "        \"\"\"Generate actionable recommendations for improving matching performance.\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # Quality-based recommendations\n",
        "        quality_score = self.compute_match_quality_score()\n",
        "        if quality_score < 0.3:\n",
        "            recommendations.append(\"Low match quality detected - consider adjusting detection parameters\")\n",
        "        elif quality_score > 0.8:\n",
        "            recommendations.append(\"High match quality achieved - current parameters are optimal\")\n",
        "\n",
        "        # Sample size recommendations\n",
        "        min_keypoints = min(self.keypoints_image1, self.keypoints_image2)\n",
        "        if min_keypoints < 30:\n",
        "            recommendations.append(\"Low keypoint count - consider reducing detection threshold or increasing image resolution\")\n",
        "\n",
        "        # Geometric verification recommendations\n",
        "        if self.geometric_verification_passed is False:\n",
        "            recommendations.append(\"Geometric verification failed - images may not be related or contain repeated patterns\")\n",
        "        elif self.geometric_verification_passed is None and self.good_matches > 10:\n",
        "            recommendations.append(\"Consider adding geometric verification for improved reliability\")\n",
        "\n",
        "        # Performance recommendations\n",
        "        if self.matching_time_seconds > 1.0:\n",
        "            recommendations.append(\"Slow matching performance - consider optimizing keypoint count or using approximate matching\")\n",
        "\n",
        "        # Threshold recommendations\n",
        "        if self.distance_threshold_used and self.distance_threshold_used > 100:\n",
        "            recommendations.append(\"High distance threshold may include unreliable matches - consider reducing threshold\")\n",
        "\n",
        "        return recommendations\n",
        "\n"
      ],
      "metadata": {
        "id": "Vt0_TeYaW9Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main ImageSimilarityDetector Implementation\n",
        "\n",
        "class ImageSimilarityDetector:\n",
        "    \"\"\"\n",
        "    Production-grade image similarity detection system implementing multiple algorithms:\n",
        "\n",
        "    1. Perceptual hash difference using DCT-based pHash algorithm\n",
        "    2. ORB feature matching with configurable normalization strategies\n",
        "    3. Color histogram correlation with multiple distance metrics\n",
        "    4. CLIP embedding cosine similarity using vision transformers\n",
        "    5. Automated Google reverse image search with robust web scraping\n",
        "\n",
        "    Implements enterprise-grade resource management, thread safety, dependency injection,\n",
        "    comprehensive error handling, and mathematical rigor per academic literature.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        orb_detector: Optional[FeatureDetectorProtocol] = None,\n",
        "        matcher: Optional[MatcherProtocol] = None,\n",
        "        clip_model_loader: Optional[ClipModelLoaderProtocol] = None,\n",
        "        device: Optional[str] = None,\n",
        "        clip_model_name: str = \"ViT-B/32\",\n",
        "        allow_symlinks: bool = False,\n",
        "        max_initialization_retries: int = 3\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize ImageSimilarityDetector with dependency injection and SOLID principles.\n",
        "\n",
        "        Implements the Dependency Inversion Principle by accepting abstract interfaces\n",
        "        rather than concrete implementations, enabling testability and modularity.\n",
        "\n",
        "        Args:\n",
        "            orb_detector: Injectable ORB detector conforming to FeatureDetectorProtocol\n",
        "            matcher: Injectable matcher conforming to MatcherProtocol\n",
        "            clip_model_loader: Injectable CLIP loader conforming to ClipModelLoaderProtocol\n",
        "            device: Target device string ('cuda', 'cpu', or specific device)\n",
        "            clip_model_name: CLIP model variant identifier\n",
        "            allow_symlinks: Whether to permit symlink resolution in path validation\n",
        "            max_initialization_retries: Maximum retry attempts for resource initialization\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If injectable dependencies do not conform to required protocols\n",
        "            InitializationError: If default resource creation fails after retries\n",
        "        \"\"\"\n",
        "        # Store configuration parameters for lazy initialization\n",
        "        self._clip_model_name: str = clip_model_name\n",
        "        self._allow_symlinks: bool = allow_symlinks\n",
        "        self._max_retries: int = max_initialization_retries\n",
        "\n",
        "        # Detect optimal device with user override capability\n",
        "        self.device: str = self._determine_optimal_device(device)\n",
        "\n",
        "        # Initialize dependency injection with protocol validation\n",
        "        self._initialize_feature_detector(orb_detector)\n",
        "        self._initialize_matcher(matcher)\n",
        "        self._initialize_clip_loader(clip_model_loader)\n",
        "\n",
        "        # Initialize CLIP model state placeholders for lazy loading\n",
        "        self.clip_model: Optional[torch.nn.Module] = None\n",
        "        self.clip_preprocess: Optional[Callable] = None\n",
        "\n",
        "        # Create thread-safe initialization lock for concurrent access\n",
        "        self._initialization_lock: threading.Lock = threading.Lock()\n",
        "\n",
        "        # Setup logging for debugging and monitoring\n",
        "        self._logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "\n",
        "    def _determine_optimal_device(self, device_override: Optional[str]) -> str:\n",
        "        \"\"\"\n",
        "        Determine optimal computational device with fallback hierarchy.\n",
        "\n",
        "        Args:\n",
        "            device_override: User-specified device string override\n",
        "\n",
        "        Returns:\n",
        "            Validated device string\n",
        "        \"\"\"\n",
        "        # Honor explicit user device specification\n",
        "        if device_override is not None:\n",
        "            return device_override\n",
        "\n",
        "        # Attempt CUDA detection with graceful fallback\n",
        "        try:\n",
        "            # Check CUDA availability and device count\n",
        "            if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
        "                return \"cuda\"\n",
        "        except Exception as e:\n",
        "            # Log CUDA detection failure and continue with CPU fallback\n",
        "            self._logger.warning(f\"CUDA detection failed, using CPU: {e}\")\n",
        "\n",
        "        # Default to CPU for maximum compatibility\n",
        "        return \"cpu\"\n",
        "\n",
        "    def _initialize_feature_detector(\n",
        "        self,\n",
        "        detector: Optional[FeatureDetectorProtocol]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize ORB feature detector with protocol validation and error handling.\n",
        "\n",
        "        Args:\n",
        "            detector: Injectable detector or None for default creation\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If detector does not conform to FeatureDetectorProtocol\n",
        "            OpenCVInitializationError: If default detector creation fails\n",
        "        \"\"\"\n",
        "        if detector is not None:\n",
        "            # Validate injectable detector conforms to required protocol\n",
        "            if not hasattr(detector, 'detectAndCompute'):\n",
        "                raise ValueError(\n",
        "                    \"Provided detector must implement FeatureDetectorProtocol \"\n",
        "                    \"with detectAndCompute method\"\n",
        "                )\n",
        "            # Store validated injectable detector\n",
        "            self.orb_detector = detector\n",
        "        else:\n",
        "            # Create default ORB detector with retry logic\n",
        "            for attempt in range(self._max_retries):\n",
        "                try:\n",
        "                    # Attempt default detector creation through factory\n",
        "                    self.orb_detector = DefaultFeatureDetectorFactory.create()\n",
        "                    break\n",
        "                except (OpenCVInitializationError, ResourceAllocationError) as e:\n",
        "                    # Log attempt failure and retry if attempts remaining\n",
        "                    if attempt < self._max_retries - 1:\n",
        "                        self._logger.warning(f\"Detector creation attempt {attempt + 1} failed: {e}\")\n",
        "                        continue\n",
        "                    # Re-raise if all retry attempts exhausted\n",
        "                    raise\n",
        "\n",
        "    def _initialize_matcher(self, matcher: Optional[MatcherProtocol]) -> None:\n",
        "        \"\"\"\n",
        "        Initialize BFMatcher with protocol validation and error handling.\n",
        "\n",
        "        Args:\n",
        "            matcher: Injectable matcher or None for default creation\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If matcher does not conform to MatcherProtocol\n",
        "            OpenCVInitializationError: If default matcher creation fails\n",
        "        \"\"\"\n",
        "        if matcher is not None:\n",
        "            # Validate injectable matcher conforms to required protocol methods\n",
        "            required_methods = ['match', 'knnMatch']\n",
        "            for method_name in required_methods:\n",
        "                if not hasattr(matcher, method_name):\n",
        "                    raise ValueError(\n",
        "                        f\"Provided matcher must implement MatcherProtocol \"\n",
        "                        f\"with {method_name} method\"\n",
        "                    )\n",
        "            # Store validated injectable matcher\n",
        "            self.bf_matcher = matcher\n",
        "        else:\n",
        "            # Create default BFMatcher with retry logic for resource allocation\n",
        "            for attempt in range(self._max_retries):\n",
        "                try:\n",
        "                    # Attempt default matcher creation through factory\n",
        "                    self.bf_matcher = DefaultMatcherFactory.create()\n",
        "                    break\n",
        "                except OpenCVInitializationError as e:\n",
        "                    # Log attempt failure and retry if attempts remaining\n",
        "                    if attempt < self._max_retries - 1:\n",
        "                        self._logger.warning(f\"Matcher creation attempt {attempt + 1} failed: {e}\")\n",
        "                        continue\n",
        "                    # Re-raise if all retry attempts exhausted\n",
        "                    raise\n",
        "\n",
        "    def _initialize_clip_loader(\n",
        "        self,\n",
        "        loader: Optional[ClipModelLoaderProtocol]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize CLIP model loader with protocol validation.\n",
        "\n",
        "        Args:\n",
        "            loader: Injectable loader or None for default creation\n",
        "        \"\"\"\n",
        "        if loader is not None:\n",
        "            # Validate injectable loader is callable\n",
        "            if not callable(loader):\n",
        "                raise ValueError(\"Provided clip_model_loader must be callable\")\n",
        "            # Store validated injectable loader\n",
        "            self.clip_model_loader = loader\n",
        "        else:\n",
        "            # Create default CLIP loader instance\n",
        "            self.clip_model_loader = DefaultClipModelLoader()\n",
        "\n",
        "    @staticmethod\n",
        "    def _validate_image_path(\n",
        "        image_path: Union[str, Path],\n",
        "        allow_symlinks: bool = False\n",
        "    ) -> Path:\n",
        "        \"\"\"\n",
        "        Comprehensive path validation with security and accessibility checks.\n",
        "\n",
        "        Implements defensive programming against path traversal, permission issues,\n",
        "        and file system race conditions with configurable symlink policy.\n",
        "\n",
        "        Args:\n",
        "            image_path: Path string or Path object to validate\n",
        "            allow_symlinks: Whether to permit symlink resolution\n",
        "\n",
        "        Returns:\n",
        "            Validated Path object pointing to accessible image file\n",
        "\n",
        "        Raises:\n",
        "            ImageNotFoundError: If path does not exist in filesystem\n",
        "            NotAFileError: If path exists but is not a regular file\n",
        "            SymlinkNotAllowedError: If symlinks encountered but not permitted\n",
        "            PermissionDeniedError: If insufficient read permissions\n",
        "        \"\"\"\n",
        "        # Convert input to pathlib.Path for uniform handling and normalization\n",
        "        path = Path(image_path).resolve()\n",
        "\n",
        "        # Verify path existence in filesystem with atomic check\n",
        "        if not path.exists():\n",
        "            raise ImageNotFoundError(f\"Image file does not exist: {path}\")\n",
        "\n",
        "        # Handle symlink policy enforcement\n",
        "        if path.is_symlink() and not allow_symlinks:\n",
        "            raise SymlinkNotAllowedError(f\"Symlinks not permitted: {path}\")\n",
        "\n",
        "        # Validate path points to regular file (not directory, device, or fifo)\n",
        "        if not path.is_file():\n",
        "            raise NotAFileError(f\"Path is not a regular file: {path}\")\n",
        "\n",
        "        # Check read permissions using OS-level access control\n",
        "        if not os.access(path, os.R_OK):\n",
        "            raise PermissionDeniedError(f\"Insufficient read permissions: {path}\")\n",
        "\n",
        "        # Return validated, normalized path\n",
        "        return path\n",
        "\n",
        "    def _load_clip_with_double_checked_locking(self) -> None:\n",
        "        \"\"\"\n",
        "        Thread-safe lazy loading of CLIP model using double-checked locking pattern.\n",
        "\n",
        "        Implements proper double-checked locking to prevent race conditions while\n",
        "        minimizing lock contention for already-loaded models. Handles network failures,\n",
        "        CUDA errors, and memory constraints with comprehensive fallback strategies.\n",
        "\n",
        "        Raises:\n",
        "            ModelLoadError: If model loading fails after all retry attempts\n",
        "        \"\"\"\n",
        "        # First check without lock acquisition for performance optimization\n",
        "        if self.clip_model is not None and self.clip_preprocess is not None:\n",
        "            return\n",
        "\n",
        "        # Acquire initialization lock for thread-safe model loading\n",
        "        with self._initialization_lock:\n",
        "            # Second check after lock acquisition to prevent duplicate loading\n",
        "            if self.clip_model is not None and self.clip_preprocess is not None:\n",
        "                return\n",
        "\n",
        "            # Attempt model loading with comprehensive error handling\n",
        "            loading_errors = []\n",
        "            for attempt in range(self._max_retries):\n",
        "                try:\n",
        "                    # Attempt CLIP model loading through injectable loader\n",
        "                    self.clip_model, self.clip_preprocess = self.clip_model_loader(\n",
        "                        self._clip_model_name,\n",
        "                        self.device\n",
        "                    )\n",
        "                    # Log successful loading for monitoring\n",
        "                    self._logger.info(f\"CLIP model {self._clip_model_name} loaded on {self.device}\")\n",
        "                    return\n",
        "\n",
        "                except (OSError, RuntimeError, ModuleNotFoundError) as e:\n",
        "                    # Store error for potential re-raising\n",
        "                    loading_errors.append(e)\n",
        "                    # Log attempt failure for debugging\n",
        "                    self._logger.warning(f\"CLIP loading attempt {attempt + 1} failed: {e}\")\n",
        "\n",
        "                    # Attempt device fallback on CUDA errors\n",
        "                    if \"cuda\" in str(e).lower() and self.device != \"cpu\":\n",
        "                        self._logger.info(\"Attempting CPU fallback for CLIP loading\")\n",
        "                        self.device = \"cpu\"\n",
        "                        continue\n",
        "\n",
        "                except torch.cuda.OutOfMemoryError as e:\n",
        "                    # Handle GPU memory exhaustion with cache clearing\n",
        "                    loading_errors.append(e)\n",
        "                    self._logger.warning(f\"GPU OOM during CLIP loading: {e}\")\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                    # Fallback to CPU if not already attempted\n",
        "                    if self.device != \"cpu\":\n",
        "                        self.device = \"cpu\"\n",
        "                        continue\n",
        "\n",
        "            # Raise comprehensive error if all attempts failed\n",
        "            error_summary = \"; \".join(str(e) for e in loading_errors)\n",
        "            raise ModelLoadError(\n",
        "                f\"Failed to load CLIP model {self._clip_model_name} after \"\n",
        "                f\"{self._max_retries} attempts: {error_summary}\"\n",
        "            )\n",
        "\n",
        "    def release_clip_resources(self) -> None:\n",
        "        \"\"\"\n",
        "        Explicitly release CLIP model resources for memory management.\n",
        "\n",
        "        Implements graceful resource cleanup for long-running processes or\n",
        "        memory-constrained environments. Thread-safe and idempotent.\n",
        "        \"\"\"\n",
        "        # Acquire lock to ensure thread-safe resource cleanup\n",
        "        with self._initialization_lock:\n",
        "            # Clear model references to enable garbage collection\n",
        "            if self.clip_model is not None:\n",
        "                self.clip_model = None\n",
        "                self._logger.info(\"CLIP model references cleared\")\n",
        "\n",
        "            # Clear preprocessing function reference\n",
        "            if self.clip_preprocess is not None:\n",
        "                self.clip_preprocess = None\n",
        "                self._logger.info(\"CLIP preprocessing function cleared\")\n",
        "\n",
        "            # Clear GPU memory cache if CUDA is available\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                self._logger.info(\"CUDA memory cache cleared\")\n",
        "\n",
        "    def feature_match_ratio(\n",
        "        self,\n",
        "        image1: Union[str, Path, np.ndarray],\n",
        "        image2: Union[str, Path, np.ndarray],\n",
        "        distance_threshold: int = 50,\n",
        "        normalization_strategy: Literal[\"total_matches\", \"min_keypoints\"] = \"total_matches\",\n",
        "        apply_ratio_test: bool = False,\n",
        "        ratio_threshold: float = 0.75,\n",
        "        resize_max_side: Optional[int] = None,\n",
        "        return_detailed_result: bool = False\n",
        "    ) -> Union[float, FeatureMatchResult]:\n",
        "        \"\"\"\n",
        "        Compute feature matching similarity using ORB descriptors and configurable normalization.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        1. ORB Feature Detection: Oriented FAST + Rotated BRIEF\n",
        "           - FAST corners: C = {p : |I(p) - I(x)| > τ for x ∈ circle(p)}\n",
        "           - Orientation: θ = atan2(m₀₁, m₁₀) where mₚq = Σᵨ uᵖvᵍI(u,v)\n",
        "           - BRIEF descriptors: Binary strings from intensity comparisons\n",
        "\n",
        "        2. Hamming Distance Matching: d_H(D₁,D₂) = Σᵢ D₁[i] ⊕ D₂[i]\n",
        "\n",
        "        3. Normalization Strategies:\n",
        "           - Total matches: ratio = |good_matches| / |total_matches|\n",
        "           - Min keypoints: ratio = |good_matches| / min(|KP₁|, |KP₂|)\n",
        "\n",
        "        4. Lowe's Ratio Test: accept match if d₁/d₂ < τ where d₁ < d₂\n",
        "\n",
        "        Args:\n",
        "            image1: First image as path or numpy array\n",
        "            image2: Second image as path or numpy array\n",
        "            distance_threshold: Maximum Hamming distance for good match\n",
        "            normalization_strategy: Method for ratio computation\n",
        "            apply_ratio_test: Whether to use Lowe's ratio test instead of threshold\n",
        "            ratio_threshold: Threshold for Lowe's ratio test (typically 0.75)\n",
        "            resize_max_side: Optional downscaling to limit computational cost\n",
        "            return_detailed_result: If True, return comprehensive FeatureMatchResult\n",
        "\n",
        "        Returns:\n",
        "            Similarity ratio [0,1] or detailed FeatureMatchResult object\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If parameters are invalid or incompatible\n",
        "            FileNotFoundError: If image files do not exist\n",
        "            RuntimeError: If feature detection fails\n",
        "        \"\"\"\n",
        "        # Validate normalization strategy parameter\n",
        "        valid_strategies = {\"total_matches\", \"min_keypoints\"}\n",
        "        if normalization_strategy not in valid_strategies:\n",
        "            raise ValueError(f\"Invalid normalization_strategy: {normalization_strategy}\")\n",
        "\n",
        "        # Validate distance threshold for Hamming distance (8-bit binary descriptors)\n",
        "        if distance_threshold < 0 or distance_threshold > 256:\n",
        "            raise ValueError(f\"distance_threshold must be in [0,256], got {distance_threshold}\")\n",
        "\n",
        "        # Validate ratio threshold for Lowe's test\n",
        "        if not 0.0 < ratio_threshold < 1.0:\n",
        "            raise ValueError(f\"ratio_threshold must be in (0,1), got {ratio_threshold}\")\n",
        "\n",
        "        # Validate resize parameter if provided\n",
        "        if resize_max_side is not None and resize_max_side <= 0:\n",
        "            raise ValueError(f\"resize_max_side must be positive, got {resize_max_side}\")\n",
        "\n",
        "        # Helper function for comprehensive image loading and preprocessing\n",
        "        def _load_and_prepare_image(\n",
        "            img_input: Union[str, Path, np.ndarray]\n",
        "        ) -> np.ndarray:\n",
        "            \"\"\"Load image and prepare for ORB feature detection.\"\"\"\n",
        "\n",
        "            # Handle path input with validation\n",
        "            if isinstance(img_input, (str, Path)):\n",
        "                # Validate image path exists and is readable\n",
        "                validated_path = self._validate_image_path(img_input, self._allow_symlinks)\n",
        "\n",
        "                # Load image in BGR color format for OpenCV processing\n",
        "                image_array = cv2.imread(str(validated_path), cv2.IMREAD_COLOR)\n",
        "                if image_array is None:\n",
        "                    raise RuntimeError(f\"OpenCV failed to load image: {validated_path}\")\n",
        "\n",
        "            # Handle numpy array input with validation\n",
        "            elif isinstance(img_input, np.ndarray):\n",
        "                # Validate array dimensions for image data\n",
        "                if img_input.ndim not in [2, 3]:\n",
        "                    raise ValueError(f\"Invalid array dimensions: {img_input.ndim}\")\n",
        "\n",
        "                # Copy input array to prevent mutation of original data\n",
        "                image_array = img_input.copy()\n",
        "\n",
        "                # Ensure 3-channel color image for consistent processing\n",
        "                if image_array.ndim == 2:\n",
        "                    # Convert grayscale to 3-channel\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)\n",
        "                elif image_array.shape[2] == 4:\n",
        "                    # Remove alpha channel if present\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_BGRA2BGR)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported image input type: {type(img_input)}\")\n",
        "\n",
        "            # Apply optional resizing with aspect ratio preservation\n",
        "            if resize_max_side is not None:\n",
        "                # Get current image dimensions\n",
        "                height, width = image_array.shape[:2]\n",
        "                current_max_side = max(height, width)\n",
        "\n",
        "                # Resize only if current size exceeds limit\n",
        "                if current_max_side > resize_max_side:\n",
        "                    # Compute scale factor preserving aspect ratio\n",
        "                    scale_factor = resize_max_side / current_max_side\n",
        "                    new_width = int(width * scale_factor)\n",
        "                    new_height = int(height * scale_factor)\n",
        "\n",
        "                    # Apply high-quality resizing using area interpolation\n",
        "                    image_array = cv2.resize(\n",
        "                        image_array,\n",
        "                        (new_width, new_height),\n",
        "                        interpolation=cv2.INTER_AREA\n",
        "                    )\n",
        "\n",
        "            # Convert to grayscale for ORB feature detection\n",
        "            if image_array.ndim == 3:\n",
        "                grayscale_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n",
        "            else:\n",
        "                grayscale_array = image_array\n",
        "\n",
        "            return grayscale_array\n",
        "\n",
        "        # Load and prepare both images for feature detection\n",
        "        try:\n",
        "            prepared_image1 = _load_and_prepare_image(image1)\n",
        "            prepared_image2 = _load_and_prepare_image(image2)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Image preparation failed: {e}\") from e\n",
        "\n",
        "        # Detect keypoints and compute ORB descriptors for both images\n",
        "        try:\n",
        "            # Image 1: Detect FAST corners and compute BRIEF descriptors\n",
        "            keypoints1, descriptors1 = self.orb_detector.detectAndCompute(prepared_image1, None)\n",
        "            # Image 2: Detect FAST corners and compute BRIEF descriptors\n",
        "            keypoints2, descriptors2 = self.orb_detector.detectAndCompute(prepared_image2, None)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"ORB feature detection failed: {e}\") from e\n",
        "\n",
        "        # Handle cases where no descriptors are detected\n",
        "        if descriptors1 is None or descriptors2 is None or len(descriptors1) == 0 or len(descriptors2) == 0:\n",
        "            # Return zero similarity with detailed result if requested\n",
        "            if return_detailed_result:\n",
        "                return FeatureMatchResult(\n",
        "                    similarity_ratio=0.0,\n",
        "                    total_matches=0,\n",
        "                    good_matches=0,\n",
        "                    keypoints_image1=len(keypoints1) if keypoints1 else 0,\n",
        "                    keypoints_image2=len(keypoints2) if keypoints2 else 0,\n",
        "                    normalization_strategy=normalization_strategy,\n",
        "                    confidence_level=0.0\n",
        "                )\n",
        "            return 0.0\n",
        "\n",
        "        # Perform descriptor matching using configured algorithm\n",
        "        good_matches = []\n",
        "        total_matches = 0\n",
        "\n",
        "        if apply_ratio_test:\n",
        "            # Apply Lowe's ratio test using k-nearest neighbor matching\n",
        "            try:\n",
        "                # Find 2 nearest neighbors for each descriptor in image1\n",
        "                knn_matches = self.bf_matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
        "\n",
        "                # Apply ratio test: accept if d₁/d₂ < threshold\n",
        "                for match_pair in knn_matches:\n",
        "                    if len(match_pair) == 2:\n",
        "                        best_match, second_match = match_pair\n",
        "                        # Lowe's ratio test criterion\n",
        "                        if best_match.distance < ratio_threshold * second_match.distance:\n",
        "                            good_matches.append(best_match)\n",
        "                        total_matches += 1\n",
        "                    elif len(match_pair) == 1:\n",
        "                        # Only one match found - accept it\n",
        "                        good_matches.append(match_pair[0])\n",
        "                        total_matches += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"KNN matching failed: {e}\") from e\n",
        "\n",
        "        else:\n",
        "            # Use simple distance threshold matching\n",
        "            try:\n",
        "                # Perform brute-force matching with cross-check\n",
        "                matches = self.bf_matcher.match(descriptors1, descriptors2)\n",
        "                total_matches = len(matches)\n",
        "\n",
        "                # Filter matches by Hamming distance threshold\n",
        "                good_matches = [\n",
        "                    match for match in matches\n",
        "                    if match.distance <= distance_threshold\n",
        "                ]\n",
        "\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"Distance threshold matching failed: {e}\") from e\n",
        "\n",
        "        # Handle edge case of no matches found\n",
        "        if total_matches == 0:\n",
        "            similarity_ratio = 0.0\n",
        "        else:\n",
        "            # Compute similarity ratio using specified normalization strategy\n",
        "            if normalization_strategy == \"total_matches\":\n",
        "                # Normalize by total number of attempted matches\n",
        "                similarity_ratio = float(len(good_matches)) / float(total_matches)\n",
        "            else:  # \"min_keypoints\"\n",
        "                # Normalize by minimum number of detected keypoints\n",
        "                min_keypoints = min(len(keypoints1), len(keypoints2))\n",
        "                # Avoid division by zero if no keypoints detected\n",
        "                if min_keypoints == 0:\n",
        "                    similarity_ratio = 0.0\n",
        "                else:\n",
        "                    similarity_ratio = float(len(good_matches)) / float(min_keypoints)\n",
        "                    # Clamp ratio to [0,1] range for min_keypoints strategy\n",
        "                    similarity_ratio = min(similarity_ratio, 1.0)\n",
        "\n",
        "        # Compute confidence level based on match statistics\n",
        "        confidence_level = None\n",
        "        if total_matches > 0:\n",
        "            # Simple confidence based on absolute number of good matches\n",
        "            confidence_level = min(float(len(good_matches)) / 50.0, 1.0)  # Normalize to [0,1]\n",
        "\n",
        "        # Return detailed result object if requested\n",
        "        if return_detailed_result:\n",
        "            return FeatureMatchResult(\n",
        "                similarity_ratio=similarity_ratio,\n",
        "                total_matches=total_matches,\n",
        "                good_matches=len(good_matches),\n",
        "                keypoints_image1=len(keypoints1),\n",
        "                keypoints_image2=len(keypoints2),\n",
        "                normalization_strategy=normalization_strategy,\n",
        "                confidence_level=confidence_level\n",
        "            )\n",
        "\n",
        "        # Return simple similarity ratio\n",
        "        return similarity_ratio\n",
        "\n",
        "    def histogram_correlation(\n",
        "        self,\n",
        "        image1: Union[str, Path, np.ndarray],\n",
        "        image2: Union[str, Path, np.ndarray],\n",
        "        bins: Union[int, Tuple[int, int]] = (50, 60),\n",
        "        metric: Literal[\"correlation\", \"chi-square\", \"intersection\", \"bhattacharyya\"] = \"correlation\",\n",
        "        mask1: Optional[np.ndarray] = None,\n",
        "        mask2: Optional[np.ndarray] = None,\n",
        "        preserve_aspect: bool = True,\n",
        "        resize_size: Tuple[int, int] = (256, 256),\n",
        "        on_zero_histogram: Literal[\"error\", \"zero\", \"nan\"] = \"error\",\n",
        "        color_space: Literal[\"HSV\", \"RGB\", \"LAB\"] = \"HSV\"\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Compute image similarity via statistical comparison of color histograms.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        1. Color space transformation: I' = transform(I, color_space)\n",
        "        2. Histogram computation: H_c(i) = Σₓᵧ 𝟙{bin(I'_c(x,y)) = i} for channel c\n",
        "        3. Normalization: Ĥ_c = H_c / Σᵢ H_c[i] (L1 normalization)\n",
        "        4. Distance metrics:\n",
        "           - Correlation (Pearson): ρ = Σᵢ(h₁[i]-μ₁)(h₂[i]-μ₂) / (σ₁σ₂)\n",
        "           - Chi-Square: χ² = Σᵢ (h₁[i]-h₂[i])² / (h₁[i]+h₂[i])\n",
        "           - Intersection: I = Σᵢ min(h₁[i], h₂[i])\n",
        "           - Bhattacharyya: d_B = -ln(Σᵢ √(h₁[i]h₂[i]))\n",
        "\n",
        "        Args:\n",
        "            image1: First image as path or numpy array\n",
        "            image2: Second image as path or numpy array\n",
        "            bins: Number of bins per channel (int) or tuple (H_bins, S_bins) for HSV\n",
        "            metric: Statistical distance/similarity metric\n",
        "            mask1: Optional binary mask for image1 region of interest\n",
        "            mask2: Optional binary mask for image2 region of interest\n",
        "            preserve_aspect: Whether to maintain aspect ratio during resize\n",
        "            resize_size: Target dimensions (width, height) for preprocessing\n",
        "            on_zero_histogram: Behavior when zero histogram encountered\n",
        "            color_space: Color space for histogram computation\n",
        "\n",
        "        Returns:\n",
        "            Similarity score or distance value per selected metric\n",
        "\n",
        "        Raises:\n",
        "            HistogramError: On computation failures or invalid parameters\n",
        "            ValueError: On invalid parameter combinations\n",
        "        \"\"\"\n",
        "        # Validate metric selection against supported algorithms\n",
        "        supported_metrics = {\"correlation\", \"chi-square\", \"intersection\", \"bhattacharyya\"}\n",
        "        if metric not in supported_metrics:\n",
        "            raise HistogramError(f\"Unsupported metric: {metric}. Supported: {supported_metrics}\")\n",
        "\n",
        "        # Map metric names to OpenCV constants for implementation\n",
        "        metric_constants = {\n",
        "            \"correlation\": cv2.HISTCMP_CORREL,      # Higher values = more similar\n",
        "            \"chi-square\": cv2.HISTCMP_CHISQR,      # Lower values = more similar\n",
        "            \"intersection\": cv2.HISTCMP_INTERSECT, # Higher values = more similar\n",
        "            \"bhattacharyya\": cv2.HISTCMP_BHATTACHARYYA  # Lower values = more similar\n",
        "        }\n",
        "\n",
        "        # Validate and normalize bins parameter\n",
        "        if isinstance(bins, int):\n",
        "            if bins <= 0:\n",
        "                raise ValueError(f\"bins must be positive, got {bins}\")\n",
        "            # Use same number of bins for all channels\n",
        "            hist_bins = [bins] * 3\n",
        "        elif isinstance(bins, (tuple, list)) and len(bins) == 2:\n",
        "            if any(b <= 0 for b in bins):\n",
        "                raise ValueError(f\"All bin counts must be positive, got {bins}\")\n",
        "            # Use specified bins for H and S channels, default for V/B channel\n",
        "            hist_bins = [bins[0], bins[1], 50]\n",
        "        else:\n",
        "            raise ValueError(f\"bins must be int or 2-tuple, got {type(bins)}\")\n",
        "\n",
        "        # Validate color space selection\n",
        "        supported_color_spaces = {\"HSV\", \"RGB\", \"LAB\"}\n",
        "        if color_space not in supported_color_spaces:\n",
        "            raise ValueError(f\"Unsupported color_space: {color_space}\")\n",
        "\n",
        "        # Helper function for comprehensive image loading and preprocessing\n",
        "        def _load_and_prepare_image(\n",
        "            img_input: Union[str, Path, np.ndarray],\n",
        "            mask: Optional[np.ndarray] = None\n",
        "        ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
        "            \"\"\"Load image and prepare for histogram computation with optional masking.\"\"\"\n",
        "\n",
        "            # Handle path-based image loading\n",
        "            if isinstance(img_input, (str, Path)):\n",
        "                # Validate path accessibility\n",
        "                validated_path = self._validate_image_path(img_input, self._allow_symlinks)\n",
        "\n",
        "                # Load image using OpenCV in BGR format\n",
        "                image_array = cv2.imread(str(validated_path), cv2.IMREAD_COLOR)\n",
        "                if image_array is None:\n",
        "                    raise HistogramError(f\"Failed to load image: {validated_path}\")\n",
        "\n",
        "            # Handle numpy array input\n",
        "            elif isinstance(img_input, np.ndarray):\n",
        "                # Validate array structure for image data\n",
        "                if img_input.ndim not in [2, 3]:\n",
        "                    raise HistogramError(f\"Invalid array dimensions: {img_input.ndim}\")\n",
        "\n",
        "                # Copy to prevent mutation of original data\n",
        "                image_array = img_input.copy()\n",
        "\n",
        "                # Ensure 3-channel format for consistent processing\n",
        "                if image_array.ndim == 2:\n",
        "                    # Convert grayscale to BGR\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)\n",
        "                elif image_array.shape[2] == 4:\n",
        "                    # Remove alpha channel\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_BGRA2BGR)\n",
        "                elif image_array.shape[2] != 3:\n",
        "                    raise HistogramError(f\"Unsupported channel count: {image_array.shape[2]}\")\n",
        "\n",
        "            else:\n",
        "                raise HistogramError(f\"Unsupported image type: {type(img_input)}\")\n",
        "\n",
        "            # Apply resizing with optional aspect ratio preservation\n",
        "            def _resize_image(img: np.ndarray) -> np.ndarray:\n",
        "                \"\"\"Resize image according to configuration parameters.\"\"\"\n",
        "                current_height, current_width = img.shape[:2]\n",
        "                target_width, target_height = resize_size\n",
        "\n",
        "                if preserve_aspect:\n",
        "                    # Compute scaling factor preserving aspect ratio\n",
        "                    scale_factor = min(\n",
        "                        target_width / current_width,\n",
        "                        target_height / current_height\n",
        "                    )\n",
        "                    # Compute new dimensions maintaining aspect ratio\n",
        "                    new_width = int(current_width * scale_factor)\n",
        "                    new_height = int(current_height * scale_factor)\n",
        "                    # Apply high-quality area interpolation for downscaling\n",
        "                    return cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "                else:\n",
        "                    # Direct resize to target dimensions\n",
        "                    return cv2.resize(img, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # Apply resizing to image\n",
        "            resized_image = _resize_image(image_array)\n",
        "\n",
        "            # Process mask if provided\n",
        "            processed_mask = None\n",
        "            if mask is not None:\n",
        "                # Validate mask dimensions match original image\n",
        "                if mask.shape[:2] != image_array.shape[:2]:\n",
        "                    raise HistogramError(f\"Mask shape {mask.shape} incompatible with image shape {image_array.shape}\")\n",
        "\n",
        "                # Resize mask to match processed image\n",
        "                resized_mask = cv2.resize(mask.astype(np.uint8), resized_image.shape[:2][::-1])\n",
        "                # Ensure binary mask values\n",
        "                processed_mask = (resized_mask > 0).astype(np.uint8) * 255\n",
        "\n",
        "            return resized_image, processed_mask\n",
        "\n",
        "        # Load and prepare both images with their respective masks\n",
        "        try:\n",
        "            prepared_image1, processed_mask1 = _load_and_prepare_image(image1, mask1)\n",
        "            prepared_image2, processed_mask2 = _load_and_prepare_image(image2, mask2)\n",
        "        except Exception as e:\n",
        "            raise HistogramError(f\"Image preparation failed: {e}\") from e\n",
        "\n",
        "        # Apply color space transformation based on configuration\n",
        "        def _convert_color_space(img: np.ndarray) -> np.ndarray:\n",
        "            \"\"\"Convert image to specified color space for histogram computation.\"\"\"\n",
        "            if color_space == \"HSV\":\n",
        "                # Convert BGR to HSV for perceptually uniform hue representation\n",
        "                return cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "            elif color_space == \"RGB\":\n",
        "                # Convert BGR to RGB for standard RGB analysis\n",
        "                return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            elif color_space == \"LAB\":\n",
        "                # Convert BGR to CIELAB for perceptually uniform color space\n",
        "                return cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "            else:\n",
        "                raise HistogramError(f\"Unsupported color space: {color_space}\")\n",
        "\n",
        "        # Transform both images to target color space\n",
        "        try:\n",
        "            color_image1 = _convert_color_space(prepared_image1)\n",
        "            color_image2 = _convert_color_space(prepared_image2)\n",
        "        except Exception as e:\n",
        "            raise HistogramError(f\"Color space conversion failed: {e}\") from e\n",
        "\n",
        "        # Define histogram computation ranges for different color spaces\n",
        "        if color_space == \"HSV\":\n",
        "            # HSV ranges: H[0,179], S[0,255], V[0,255] in OpenCV\n",
        "            hist_ranges = [0, 180, 0, 256, 0, 256]\n",
        "            channels = [0, 1]  # Use H and S channels for robustness to illumination\n",
        "        elif color_space in [\"RGB\", \"LAB\"]:\n",
        "            # RGB/LAB ranges: [0,255] for all channels\n",
        "            hist_ranges = [0, 256, 0, 256, 0, 256]\n",
        "            channels = [0, 1, 2]  # Use all three channels\n",
        "\n",
        "        # Compute normalized histograms for both images\n",
        "        try:\n",
        "            # Image 1: Compute multi-dimensional histogram\n",
        "            histogram1 = cv2.calcHist(\n",
        "                [color_image1],           # Source image list\n",
        "                channels,                 # Channel indices\n",
        "                processed_mask1,          # Mask (None = full image)\n",
        "                hist_bins[:len(channels)],# Bins per channel\n",
        "                hist_ranges              # Value ranges\n",
        "            )\n",
        "\n",
        "            # Image 2: Compute multi-dimensional histogram\n",
        "            histogram2 = cv2.calcHist(\n",
        "                [color_image2],           # Source image list\n",
        "                channels,                 # Channel indices\n",
        "                processed_mask2,          # Mask (None = full image)\n",
        "                hist_bins[:len(channels)],# Bins per channel\n",
        "                hist_ranges              # Value ranges\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            raise HistogramError(f\"Histogram computation failed: {e}\") from e\n",
        "\n",
        "        # Handle zero histogram cases according to policy\n",
        "        hist1_sum = np.sum(histogram1)\n",
        "        hist2_sum = np.sum(histogram2)\n",
        "\n",
        "        if hist1_sum == 0 or hist2_sum == 0:\n",
        "            if on_zero_histogram == \"error\":\n",
        "                raise HistogramError(\"Zero histogram detected - no data to compare\")\n",
        "            elif on_zero_histogram == \"zero\":\n",
        "                return 0.0\n",
        "            elif on_zero_histogram == \"nan\":\n",
        "                return float('nan')\n",
        "\n",
        "        # Apply L1 normalization to histograms for statistical comparison\n",
        "        # Normalized histogram: Ĥ[i] = H[i] / Σⱼ H[j]\n",
        "        cv2.normalize(histogram1, histogram1, alpha=1.0, beta=0.0, norm_type=cv2.NORM_L1)\n",
        "        cv2.normalize(histogram2, histogram2, alpha=1.0, beta=0.0, norm_type=cv2.NORM_L1)\n",
        "\n",
        "        # Compute similarity/distance using specified metric\n",
        "        try:\n",
        "            # Apply OpenCV histogram comparison with selected metric\n",
        "            comparison_result = cv2.compareHist(\n",
        "                histogram1,\n",
        "                histogram2,\n",
        "                metric_constants[metric]\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            raise HistogramError(f\"Histogram comparison failed for {metric}: {e}\") from e\n",
        "\n",
        "        # Return metric-specific result as float\n",
        "        return float(comparison_result)\n",
        "\n",
        "    def clip_embedding_similarity(\n",
        "        self,\n",
        "        image1: Union[str, Path, Image.Image, torch.Tensor, np.ndarray],\n",
        "        image2: Union[str, Path, Image.Image, torch.Tensor, np.ndarray],\n",
        "        use_mixed_precision: bool = False,\n",
        "        batch_processing: bool = False\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Compute cosine similarity between CLIP embeddings in high-dimensional semantic space.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        1. Vision Transformer encoding: E = ViT(patch_embed(I))\n",
        "        2. Multi-head self-attention: Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
        "        3. Layer normalization: LN(x) = γ(x-μ)/σ + β\n",
        "        4. L2 normalization: ê = e / ||e||₂ where ||e||₂ = √(Σᵢ eᵢ²)\n",
        "        5. Cosine similarity: sim(ê₁,ê₂) = ê₁·ê₂ = Σᵢ ê₁[i]ê₂[i]\n",
        "\n",
        "        Args:\n",
        "            image1: First image as path, PIL Image, tensor, or numpy array\n",
        "            image2: Second image as path, PIL Image, tensor, or numpy array\n",
        "            use_mixed_precision: Whether to use automatic mixed precision (AMP)\n",
        "            batch_processing: Whether to process images in batch for efficiency\n",
        "\n",
        "        Returns:\n",
        "            Cosine similarity in [-1, 1] where 1 = identical, -1 = opposite\n",
        "\n",
        "        Raises:\n",
        "            ModelInferenceError: If CLIP inference fails\n",
        "            ValueError: On invalid input types or tensor shapes\n",
        "            torch.cuda.OutOfMemoryError: If GPU memory insufficient\n",
        "        \"\"\"\n",
        "        # Ensure CLIP model is loaded using thread-safe lazy initialization\n",
        "        self._load_clip_with_double_checked_locking()\n",
        "\n",
        "        # Helper function for comprehensive input preprocessing\n",
        "        def _prepare_input_tensor(\n",
        "            img_input: Union[str, Path, Image.Image, torch.Tensor, np.ndarray]\n",
        "        ) -> torch.Tensor:\n",
        "            \"\"\"Convert various input types to preprocessed CLIP tensor.\"\"\"\n",
        "\n",
        "            # Handle pre-computed torch.Tensor input\n",
        "            if isinstance(img_input, torch.Tensor):\n",
        "                input_tensor = img_input.clone()  # Avoid mutation of original tensor\n",
        "\n",
        "                # Validate tensor shape for CLIP input requirements\n",
        "                if input_tensor.ndim == 3:\n",
        "                    # Add batch dimension: (C,H,W) -> (1,C,H,W)\n",
        "                    input_tensor = input_tensor.unsqueeze(0)\n",
        "                elif input_tensor.ndim == 4:\n",
        "                    # Already has batch dimension: (B,C,H,W)\n",
        "                    pass\n",
        "                else:\n",
        "                    raise ValueError(f\"Invalid tensor dimensions: {input_tensor.ndim}, expected 3 or 4\")\n",
        "\n",
        "                # Validate channel count for RGB images\n",
        "                if input_tensor.shape[1] != 3:\n",
        "                    raise ValueError(f\"Expected 3 channels, got {input_tensor.shape[1]}\")\n",
        "\n",
        "            # Handle numpy array input with comprehensive preprocessing\n",
        "            elif isinstance(img_input, np.ndarray):\n",
        "                # Validate array dimensions for image data\n",
        "                if img_input.ndim not in [2, 3]:\n",
        "                    raise ValueError(f\"Invalid array dimensions: {img_input.ndim}\")\n",
        "\n",
        "                # Handle different array formats and convert to PIL\n",
        "                if img_input.ndim == 2:\n",
        "                    # Grayscale array - convert to RGB\n",
        "                    pil_image = Image.fromarray(img_input).convert('RGB')\n",
        "                elif img_input.ndim == 3:\n",
        "                    # Color array - handle different channel orders\n",
        "                    if img_input.shape[2] == 3:\n",
        "                        # Assume RGB format for PIL compatibility\n",
        "                        pil_image = Image.fromarray(img_input.astype(np.uint8)).convert('RGB')\n",
        "                    elif img_input.shape[2] == 4:\n",
        "                        # RGBA - remove alpha channel\n",
        "                        rgb_array = img_input[:, :, :3]\n",
        "                        pil_image = Image.fromarray(rgb_array.astype(np.uint8)).convert('RGB')\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unsupported channel count: {img_input.shape[2]}\")\n",
        "\n",
        "                # Apply CLIP preprocessing pipeline to PIL image\n",
        "                input_tensor = self.clip_preprocess(pil_image)\n",
        "                # Add batch dimension for inference\n",
        "                input_tensor = input_tensor.unsqueeze(0)\n",
        "\n",
        "            # Handle PIL Image input\n",
        "            elif isinstance(img_input, Image.Image):\n",
        "                # Ensure RGB format for CLIP compatibility\n",
        "                rgb_image = img_input.convert('RGB')\n",
        "                # Apply CLIP preprocessing (resize, normalize, tensorize)\n",
        "                input_tensor = self.clip_preprocess(rgb_image)\n",
        "                # Add batch dimension for model input\n",
        "                input_tensor = input_tensor.unsqueeze(0)\n",
        "\n",
        "            # Handle file path input\n",
        "            elif isinstance(img_input, (str, Path)):\n",
        "                # Validate image file accessibility\n",
        "                validated_path = self._validate_image_path(img_input, self._allow_symlinks)\n",
        "\n",
        "                # Load image using PIL with error handling\n",
        "                try:\n",
        "                    pil_image = Image.open(validated_path).convert('RGB')\n",
        "                except (IOError, OSError) as e:\n",
        "                    raise ModelInferenceError(f\"Failed to load image {validated_path}: {e}\") from e\n",
        "\n",
        "                # Apply CLIP preprocessing pipeline\n",
        "                input_tensor = self.clip_preprocess(pil_image)\n",
        "                # Add batch dimension\n",
        "                input_tensor = input_tensor.unsqueeze(0)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported input type: {type(img_input)}\")\n",
        "\n",
        "            # Move tensor to appropriate device for inference\n",
        "            input_tensor = input_tensor.to(self.device)\n",
        "\n",
        "            return input_tensor\n",
        "\n",
        "        # Prepare both input images as CLIP-compatible tensors\n",
        "        try:\n",
        "            tensor1 = _prepare_input_tensor(image1)\n",
        "            tensor2 = _prepare_input_tensor(image2)\n",
        "        except Exception as e:\n",
        "            raise ModelInferenceError(f\"Input preparation failed: {e}\") from e\n",
        "\n",
        "        # Perform CLIP inference with comprehensive error handling\n",
        "        try:\n",
        "            # Configure automatic mixed precision if requested and supported\n",
        "            if use_mixed_precision and torch.cuda.is_available():\n",
        "                # Use autocast context for automatic mixed precision\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    with torch.no_grad():  # Disable gradient computation for inference\n",
        "                        if batch_processing:\n",
        "                            # Process both images in single batch for efficiency\n",
        "                            batch_tensor = torch.cat([tensor1, tensor2], dim=0)\n",
        "                            batch_embeddings = self.clip_model.encode_image(batch_tensor)\n",
        "                            embedding1, embedding2 = batch_embeddings[0:1], batch_embeddings[1:2]\n",
        "                        else:\n",
        "                            # Process images individually\n",
        "                            embedding1 = self.clip_model.encode_image(tensor1)\n",
        "                            embedding2 = self.clip_model.encode_image(tensor2)\n",
        "            else:\n",
        "                # Standard precision inference\n",
        "                with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "                    if batch_processing:\n",
        "                        # Batch processing for computational efficiency\n",
        "                        batch_tensor = torch.cat([tensor1, tensor2], dim=0)\n",
        "                        batch_embeddings = self.clip_model.encode_image(batch_tensor)\n",
        "                        embedding1, embedding2 = batch_embeddings[0:1], batch_embeddings[1:2]\n",
        "                    else:\n",
        "                        # Individual image processing\n",
        "                        embedding1 = self.clip_model.encode_image(tensor1)\n",
        "                        embedding2 = self.clip_model.encode_image(tensor2)\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError as e:\n",
        "            # Handle GPU memory exhaustion with automatic cleanup and CPU fallback\n",
        "            self._logger.warning(\"GPU OOM during CLIP inference, attempting CPU fallback\")\n",
        "            torch.cuda.empty_cache()  # Clear GPU memory cache\n",
        "\n",
        "            # Retry on CPU if currently using CUDA\n",
        "            if self.device != \"cpu\":\n",
        "                # Move model and tensors to CPU\n",
        "                self.clip_model = self.clip_model.cpu()\n",
        "                tensor1 = tensor1.cpu()\n",
        "                tensor2 = tensor2.cpu()\n",
        "                self.device = \"cpu\"\n",
        "\n",
        "                # Retry inference on CPU\n",
        "                with torch.no_grad():\n",
        "                    embedding1 = self.clip_model.encode_image(tensor1)\n",
        "                    embedding2 = self.clip_model.encode_image(tensor2)\n",
        "            else:\n",
        "                # Already on CPU - re-raise OOM error\n",
        "                raise ModelInferenceError(f\"CPU OOM during CLIP inference: {e}\") from e\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            # Handle other PyTorch runtime errors\n",
        "            raise ModelInferenceError(f\"CLIP inference failed: {e}\") from e\n",
        "\n",
        "        # Validate embedding shapes for compatibility\n",
        "        if embedding1.shape != embedding2.shape:\n",
        "            raise ValueError(f\"Embedding shape mismatch: {embedding1.shape} vs {embedding2.shape}\")\n",
        "\n",
        "        # Apply L2 normalization to embeddings for cosine similarity\n",
        "        # L2 norm: ||e||₂ = √(Σᵢ eᵢ²), normalized: ê = e / ||e||₂\n",
        "        normalized_embedding1 = embedding1 / embedding1.norm(dim=-1, keepdim=True)\n",
        "        normalized_embedding2 = embedding2 / embedding2.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute cosine similarity via dot product of normalized embeddings\n",
        "        # Cosine similarity: sim(ê₁,ê₂) = ê₁ · ê₂ = Σᵢ ê₁[i]ê₂[i]\n",
        "        cosine_similarity = torch.matmul(normalized_embedding1, normalized_embedding2.T)\n",
        "\n",
        "        # Extract scalar similarity value and convert to Python float\n",
        "        similarity_score = cosine_similarity.squeeze().item()\n",
        "\n",
        "        return similarity_score\n",
        "\n",
        "    def reverse_image_search_google(\n",
        "        self,\n",
        "        image_path: Union[str, Path],\n",
        "        driver_path: Union[str, Path],\n",
        "        timeout: float = 15.0,\n",
        "        headless: bool = False,\n",
        "        max_similar_urls: int = 10,\n",
        "        retry_attempts: int = 3\n",
        "    ) -> ReverseImageSearchResult:\n",
        "        \"\"\"\n",
        "        Perform comprehensive Google reverse image search with robust web automation.\n",
        "\n",
        "        Implementation employs multiple fallback strategies for UI element location,\n",
        "        comprehensive error handling, and structured data extraction for enterprise use.\n",
        "\n",
        "        Workflow:\n",
        "        1. Validate local image and ChromeDriver accessibility\n",
        "        2. Configure Chrome with optimized settings for automation\n",
        "        3. Navigate to Google Images with retry logic\n",
        "        4. Locate and activate reverse search interface using selector hierarchy\n",
        "        5. Upload image file with progress monitoring\n",
        "        6. Extract structured results with confidence scoring\n",
        "        7. Implement graceful cleanup with resource management\n",
        "\n",
        "        Args:\n",
        "            image_path: Path to local image file for reverse search\n",
        "            driver_path: Path to ChromeDriver executable\n",
        "            timeout: Maximum wait time for page elements (seconds)\n",
        "            headless: Whether to run browser in headless mode\n",
        "            max_similar_urls: Maximum number of similar image URLs to extract\n",
        "            retry_attempts: Number of retry attempts for failed operations\n",
        "\n",
        "        Returns:\n",
        "            ReverseImageSearchResult with comprehensive search findings\n",
        "\n",
        "        Raises:\n",
        "            LaunchError: If ChromeDriver initialization fails\n",
        "            NavigationError: If page navigation fails\n",
        "            UploadError: If image upload fails\n",
        "            ExtractionError: If result extraction fails\n",
        "        \"\"\"\n",
        "        # Validate image file accessibility using comprehensive path validation\n",
        "        validated_image_path = self._validate_image_path(image_path, self._allow_symlinks)\n",
        "\n",
        "        # Validate ChromeDriver executable accessibility\n",
        "        driver_path_obj = Path(driver_path)\n",
        "        if not driver_path_obj.exists() or not driver_path_obj.is_file():\n",
        "            raise LaunchError(f\"ChromeDriver not found: {driver_path_obj}\")\n",
        "\n",
        "        # Verify ChromeDriver has execution permissions\n",
        "        if not os.access(driver_path_obj, os.X_OK):\n",
        "            raise LaunchError(f\"ChromeDriver not executable: {driver_path_obj}\")\n",
        "\n",
        "        # Configure Chrome options for robust automation\n",
        "        chrome_options = Options()\n",
        "\n",
        "        # Essential options for automation stability\n",
        "        chrome_options.add_argument(\"--no-sandbox\")                    # Bypass OS security model\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")         # Overcome limited resource problems\n",
        "        chrome_options.add_argument(\"--disable-gpu\")                   # Applicable to Windows environments\n",
        "        chrome_options.add_argument(\"--disable-extensions\")            # Disable extensions for speed\n",
        "        chrome_options.add_argument(\"--disable-plugins\")               # Disable plugins for security\n",
        "        chrome_options.add_argument(\"--disable-images\")                # Load pages faster by skipping images\n",
        "        chrome_options.add_argument(\"--disable-javascript\")            # Disable JS when not needed\n",
        "        chrome_options.add_argument(\"--disable-web-security\")          # Disable web security for automation\n",
        "        chrome_options.add_argument(\"--allow-running-insecure-content\") # Allow mixed content\n",
        "\n",
        "        # Window management for consistent DOM rendering\n",
        "        if headless:\n",
        "            chrome_options.add_argument(\"--headless\")                  # Run in headless mode\n",
        "            chrome_options.add_argument(\"--window-size=1920,1080\")     # Set window size for headless\n",
        "        else:\n",
        "            chrome_options.add_argument(\"--start-maximized\")           # Maximize window for visibility\n",
        "\n",
        "        # User agent configuration to avoid bot detection\n",
        "        chrome_options.add_argument(\n",
        "            \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "            \"(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        )\n",
        "\n",
        "        # Configure WebDriver service with error handling\n",
        "        try:\n",
        "            webdriver_service = Service(str(driver_path_obj))\n",
        "        except Exception as e:\n",
        "            raise LaunchError(f\"WebDriver service configuration failed: {e}\") from e\n",
        "\n",
        "        # Initialize WebDriver with comprehensive error handling\n",
        "        driver = None\n",
        "        try:\n",
        "            # Attempt Chrome WebDriver initialization\n",
        "            driver = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
        "\n",
        "            # Configure implicit wait for element location\n",
        "            driver.implicitly_wait(timeout)\n",
        "\n",
        "            # Set page load timeout to prevent hanging\n",
        "            driver.set_page_load_timeout(timeout * 2)\n",
        "\n",
        "        except WebDriverException as e:\n",
        "            # Clean up driver if partially initialized\n",
        "            if driver is not None:\n",
        "                try:\n",
        "                    driver.quit()\n",
        "                except:\n",
        "                    pass  # Ignore cleanup errors\n",
        "            raise LaunchError(f\"ChromeDriver launch failed: {e}\") from e\n",
        "\n",
        "        try:\n",
        "            # Navigate to Google Images with retry logic\n",
        "            navigation_success = False\n",
        "            for attempt in range(retry_attempts):\n",
        "                try:\n",
        "                    # Navigate to Google Images homepage\n",
        "                    driver.get(\"https://images.google.com\")\n",
        "\n",
        "                    # Verify successful navigation by checking page title\n",
        "                    if \"Google\" in driver.title:\n",
        "                        navigation_success = True\n",
        "                        break\n",
        "\n",
        "                except Exception as e:\n",
        "                    self._logger.warning(f\"Navigation attempt {attempt + 1} failed: {e}\")\n",
        "                    if attempt < retry_attempts - 1:\n",
        "                        continue\n",
        "\n",
        "            if not navigation_success:\n",
        "                raise NavigationError(\"Failed to navigate to Google Images after all attempts\")\n",
        "\n",
        "            # Implement robust selector strategy with fallback hierarchy\n",
        "            camera_button_selectors = [\n",
        "                \"button[aria-label*='Search by image']\",           # Primary aria-label selector\n",
        "                \"div[aria-label*='Search by image']\",             # Alternative div with aria-label\n",
        "                \"button[data-ved*='camera']\",                     # Data attribute selector\n",
        "                \"div.nDcEnd\",                                     # CSS class fallback\n",
        "                \"//div[@aria-label and contains(@aria-label, 'Search by image')]\",  # XPath fallback\n",
        "                \"//button[@aria-label and contains(@aria-label, 'camera')]\"        # XPath alternative\n",
        "            ]\n",
        "\n",
        "            # Attempt to locate and click camera button using selector hierarchy\n",
        "            camera_button_clicked = False\n",
        "            for selector in camera_button_selectors:\n",
        "                try:\n",
        "                    # Determine selector type and create appropriate locator\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        # XPath selector\n",
        "                        locator = (By.XPATH, selector)\n",
        "                    else:\n",
        "                        # CSS selector\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "\n",
        "                    # Wait for element to be clickable\n",
        "                    camera_button = WebDriverWait(driver, timeout).until(\n",
        "                        EC.element_to_be_clickable(locator)\n",
        "                    )\n",
        "\n",
        "                    # Scroll element into view before clicking\n",
        "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", camera_button)\n",
        "\n",
        "                    # Attempt click with JavaScript as fallback\n",
        "                    try:\n",
        "                        camera_button.click()\n",
        "                    except Exception:\n",
        "                        # Fallback to JavaScript click\n",
        "                        driver.execute_script(\"arguments[0].click();\", camera_button)\n",
        "\n",
        "                    camera_button_clicked = True\n",
        "                    break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    # Try next selector in hierarchy\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    self._logger.warning(f\"Camera button click failed with selector {selector}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if not camera_button_clicked:\n",
        "                raise NavigationError(\"Failed to locate camera button with all selectors\")\n",
        "\n",
        "            # Locate and click upload tab with multiple strategies\n",
        "            upload_tab_selectors = [\n",
        "                \"//a[contains(text(), 'Upload an image')]\",       # Direct text match\n",
        "                \"//div[contains(text(), 'Upload an image')]\",     # Alternative element type\n",
        "                \"a[href*='upload']\",                              # URL-based selector\n",
        "                \".RZQOVd\"                                         # CSS class fallback\n",
        "            ]\n",
        "\n",
        "            upload_tab_clicked = False\n",
        "            for selector in upload_tab_selectors:\n",
        "                try:\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        locator = (By.XPATH, selector)\n",
        "                    else:\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "\n",
        "                    upload_tab = WebDriverWait(driver, timeout).until(\n",
        "                        EC.element_to_be_clickable(locator)\n",
        "                    )\n",
        "\n",
        "                    # Scroll into view and click\n",
        "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", upload_tab)\n",
        "                    upload_tab.click()\n",
        "\n",
        "                    upload_tab_clicked = True\n",
        "                    break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    self._logger.warning(f\"Upload tab click failed with selector {selector}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if not upload_tab_clicked:\n",
        "                raise NavigationError(\"Failed to locate upload tab with all selectors\")\n",
        "\n",
        "            # Locate file input element for image upload\n",
        "            file_input_selectors = [\n",
        "                \"input[name='encoded_image']\",                    # Primary name attribute\n",
        "                \"input[type='file']\",                             # Generic file input\n",
        "                \"input[accept*='image']\",                         # Accept attribute selector\n",
        "                \".cB9M7\"                                          # CSS class fallback\n",
        "            ]\n",
        "\n",
        "            file_uploaded = False\n",
        "            for selector in file_input_selectors:\n",
        "                try:\n",
        "                    file_input = WebDriverWait(driver, timeout).until(\n",
        "                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
        "                    )\n",
        "\n",
        "                    # Upload file by sending keys with absolute path\n",
        "                    file_input.send_keys(str(validated_image_path.resolve()))\n",
        "\n",
        "                    file_uploaded = True\n",
        "                    break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    self._logger.warning(f\"File upload failed with selector {selector}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if not file_uploaded:\n",
        "                raise UploadError(\"Failed to upload image file with all selectors\")\n",
        "\n",
        "            # Wait for results page to load and extract best guess text\n",
        "            best_guess_text = \"\"\n",
        "            best_guess_selectors = [\n",
        "                \"div[role='heading']\",                            # Primary role-based selector\n",
        "                \".fKDtNb\",                                        # CSS class selector\n",
        "                \"//div[contains(@class, 'r5a77d')]\",             # XPath class selector\n",
        "                \"h3\",                                             # Generic heading fallback\n",
        "                \".gLFyf\"                                          # Alternative class selector\n",
        "            ]\n",
        "\n",
        "            for selector in best_guess_selectors:\n",
        "                try:\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        locator = (By.XPATH, selector)\n",
        "                    else:\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "\n",
        "                    best_guess_element = WebDriverWait(driver, timeout).until(\n",
        "                        EC.presence_of_element_located(locator)\n",
        "                    )\n",
        "\n",
        "                    best_guess_text = best_guess_element.text.strip()\n",
        "                    if best_guess_text:  # Only accept non-empty text\n",
        "                        break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    self._logger.debug(f\"Best guess extraction failed with selector {selector}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Extract similar image URLs with comprehensive selectors\n",
        "            similar_urls = []\n",
        "            similar_image_selectors = [\n",
        "                \"a[jsname='sTFXNd']\",                             # Primary jsname selector\n",
        "                \"a[href*='/imgres?']\",                            # URL pattern selector\n",
        "                \".rg_l\",                                          # CSS class selector\n",
        "                \"//a[contains(@href, 'imgres')]\"                 # XPath URL pattern\n",
        "            ]\n",
        "\n",
        "            for selector in similar_image_selectors:\n",
        "                try:\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        locator = (By.XPATH, selector)\n",
        "                    else:\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "\n",
        "                    similar_elements = WebDriverWait(driver, timeout).until(\n",
        "                        EC.presence_of_all_elements_located(locator)\n",
        "                    )\n",
        "\n",
        "                    # Extract URLs from elements up to maximum limit\n",
        "                    for element in similar_elements[:max_similar_urls]:\n",
        "                        href = element.get_attribute(\"href\")\n",
        "                        if href and href not in similar_urls:\n",
        "                            similar_urls.append(href)\n",
        "\n",
        "                    if similar_urls:  # Stop if URLs found\n",
        "                        break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    self._logger.debug(f\"Similar images extraction failed with selector {selector}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Extract page title for context\n",
        "            page_title = driver.title if driver.title else \"Google Images\"\n",
        "\n",
        "            # Create and return structured result object\n",
        "            return ReverseImageSearchResult(\n",
        "                best_guess=best_guess_text,\n",
        "                similar_image_urls=similar_urls,\n",
        "                source_page_title=page_title,\n",
        "                confidence_score=min(len(similar_urls) / 10.0, 1.0)  # Simple confidence metric\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            # Wrap unexpected errors in appropriate exception type\n",
        "            if isinstance(e, (NavigationError, UploadError, ExtractionError)):\n",
        "                raise  # Re-raise domain-specific exceptions\n",
        "            else:\n",
        "                raise ExtractionError(f\"Reverse image search failed: {e}\") from e\n",
        "\n",
        "        finally:\n",
        "            # Ensure WebDriver cleanup regardless of success or failure\n",
        "            if driver is not None:\n",
        "                try:\n",
        "                    driver.quit()\n",
        "                except Exception as cleanup_error:\n",
        "                    self._logger.warning(f\"WebDriver cleanup failed: {cleanup_error}\")\n"
      ],
      "metadata": {
        "id": "gMROkahyXDy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Class\n",
        "\n",
        "class ImageSimilarityDetector:\n",
        "    \"\"\"\n",
        "    Production-grade image similarity detection system implementing multiple algorithms:\n",
        "\n",
        "    1. Perceptual hash difference using DCT-based pHash algorithm\n",
        "    2. ORB feature matching with configurable normalization strategies\n",
        "    3. Color histogram correlation with multiple distance metrics\n",
        "    4. CLIP embedding cosine similarity using vision transformers\n",
        "    5. Automated Google reverse image search with robust web scraping\n",
        "\n",
        "    Implements enterprise-grade resource management, thread safety, dependency injection,\n",
        "    comprehensive error handling, and mathematical rigor per academic literature.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        orb_detector: Optional[FeatureDetectorProtocol] = None,\n",
        "        matcher: Optional[MatcherProtocol] = None,\n",
        "        clip_model_loader: Optional[ClipModelLoaderProtocol] = None,\n",
        "        device: Optional[str] = None,\n",
        "        clip_model_name: str = \"ViT-B/32\",\n",
        "        allow_symlinks: bool = False,\n",
        "        max_initialization_retries: int = 3,\n",
        "        resource_constraints: ResourceConstraints = ResourceConstraints.BALANCED,\n",
        "        enable_performance_monitoring: bool = True,\n",
        "        enable_parameter_optimization: bool = False,\n",
        "        validation_policy: ValidationPolicy = ValidationPolicy.PRODUCTION\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize ImageSimilarityDetector with enterprise-grade dependency injection and monitoring.\n",
        "\n",
        "        Implements comprehensive resource management, protocol validation, statistical monitoring,\n",
        "        and factory-based component creation for production deployment in quantitative finance environments.\n",
        "\n",
        "        Args:\n",
        "            orb_detector: Injectable ORB detector conforming to FeatureDetectorProtocol\n",
        "            matcher: Injectable matcher conforming to MatcherProtocol\n",
        "            clip_model_loader: Injectable CLIP loader conforming to ClipModelLoaderProtocol\n",
        "            device: Target device string ('cuda', 'cpu', or specific device)\n",
        "            clip_model_name: CLIP model variant identifier for loading\n",
        "            allow_symlinks: Whether to permit symlink resolution in path validation\n",
        "            max_initialization_retries: Maximum retry attempts for resource initialization\n",
        "            resource_constraints: Resource utilization profile for optimization\n",
        "            enable_performance_monitoring: Whether to enable comprehensive performance tracking\n",
        "            enable_parameter_optimization: Whether to enable automatic parameter tuning\n",
        "            validation_policy: Validation strictness level for result structures\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If injectable dependencies do not conform to required protocols\n",
        "            InitializationError: If default resource creation fails after retries\n",
        "            ResourceAllocationError: If insufficient system resources available\n",
        "        \"\"\"\n",
        "        # Store configuration parameters for comprehensive system setup\n",
        "        self._clip_model_name: str = clip_model_name\n",
        "        self._allow_symlinks: bool = allow_symlinks\n",
        "        self._max_initialization_retries: int = max_initialization_retries\n",
        "        self._enable_performance_monitoring: bool = enable_performance_monitoring\n",
        "        self._enable_parameter_optimization: bool = enable_parameter_optimization\n",
        "\n",
        "        # Initialize enterprise-grade resource management system\n",
        "        self.resource_manager = ResourceManager(\n",
        "            resource_constraints=resource_constraints,\n",
        "            monitoring_interval=1.0,\n",
        "            cleanup_threshold=0.8\n",
        "        )\n",
        "\n",
        "        # Start resource monitoring for production deployment\n",
        "        if self._enable_performance_monitoring:\n",
        "            self.resource_manager.start_monitoring()\n",
        "\n",
        "        # Set validation policy for all result structures created by this instance\n",
        "        FeatureMatchResult.set_validation_policy(validation_policy)\n",
        "        ReverseImageSearchResult.set_validation_policy(validation_policy)\n",
        "\n",
        "        # Determine optimal computational device with comprehensive validation\n",
        "        self.device: str = self._determine_optimal_device_with_validation(device)\n",
        "\n",
        "        # Initialize factory-based component creation with dependency injection\n",
        "        self._initialize_feature_detector_with_factory(orb_detector)\n",
        "        self._initialize_matcher_with_factory(matcher)\n",
        "        self._initialize_clip_loader_with_factory(clip_model_loader)\n",
        "\n",
        "        # Initialize CLIP model state placeholders for lazy loading with monitoring\n",
        "        self.clip_model: Optional[torch.nn.Module] = None\n",
        "        self.clip_preprocess: Optional[Callable] = None\n",
        "        self._clip_loading_performance: Optional[Dict[str, Any]] = None\n",
        "\n",
        "        # Create thread-safe initialization locks for concurrent access protection\n",
        "        self._initialization_lock: threading.Lock = threading.Lock()\n",
        "        self._performance_lock: threading.Lock = threading.Lock()\n",
        "\n",
        "        # Initialize comprehensive performance tracking system\n",
        "        self._method_performance_history: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
        "        self._initialization_timestamp: datetime.datetime = datetime.datetime.utcnow()\n",
        "\n",
        "        # Setup enterprise-grade logging with structured metadata\n",
        "        self._logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
        "        self._logger.info(\n",
        "            \"ImageSimilarityDetector initialized with enterprise configuration\",\n",
        "            extra={\n",
        "                'device': self.device,\n",
        "                'clip_model': self._clip_model_name,\n",
        "                'resource_constraints': resource_constraints.value,\n",
        "                'performance_monitoring': self._enable_performance_monitoring,\n",
        "                'validation_policy': validation_policy.value\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def _determine_optimal_device_with_validation(self, device_override: Optional[str]) -> str:\n",
        "        \"\"\"\n",
        "        Determine optimal computational device with comprehensive capability validation.\n",
        "\n",
        "        Args:\n",
        "            device_override: User-specified device string override\n",
        "\n",
        "        Returns:\n",
        "            Validated device string with capability confirmation\n",
        "\n",
        "        Raises:\n",
        "            InitializationError: If specified device is not available or capable\n",
        "        \"\"\"\n",
        "        # Honor explicit user device specification with validation\n",
        "        if device_override is not None:\n",
        "            # Validate device capability and availability\n",
        "            if not self._validate_device_capability(device_override):\n",
        "                raise InitializationError(\n",
        "                    f\"Specified device '{device_override}' is not available or lacks required capabilities\",\n",
        "                    component_name=\"device_validator\",\n",
        "                    system_resources={'requested_device': device_override}\n",
        "                )\n",
        "            return device_override\n",
        "\n",
        "        # Attempt CUDA detection with comprehensive capability assessment\n",
        "        try:\n",
        "            # Check CUDA availability with detailed capability analysis\n",
        "            if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
        "                # Validate CUDA compute capability for deep learning operations\n",
        "                for device_idx in range(torch.cuda.device_count()):\n",
        "                    # Get device properties for capability assessment\n",
        "                    device_props = torch.cuda.get_device_properties(device_idx)\n",
        "                    # Ensure sufficient compute capability (≥3.5 for modern operations)\n",
        "                    if device_props.major >= 3 and (device_props.major > 3 or device_props.minor >= 5):\n",
        "                        # Validate memory availability for typical model loading\n",
        "                        if device_props.total_memory >= 2 * 1024**3:  # ≥2GB VRAM\n",
        "                            return f\"cuda:{device_idx}\"\n",
        "\n",
        "                # Fallback to generic CUDA if specific device validation fails\n",
        "                self._logger.warning(\"CUDA devices available but may have limited capability\")\n",
        "                return \"cuda\"\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log CUDA detection failure with comprehensive context\n",
        "            self._logger.warning(f\"CUDA detection failed, falling back to CPU: {e}\")\n",
        "\n",
        "        # Validate CPU capabilities for intensive operations\n",
        "        cpu_count = psutil.cpu_count(logical=False)\n",
        "        available_memory = psutil.virtual_memory().available\n",
        "\n",
        "        # Ensure minimum CPU and memory requirements for image processing\n",
        "        if cpu_count < 2 or available_memory < 4 * 1024**3:  # <2 cores or <4GB RAM\n",
        "            self._logger.warning(\n",
        "                f\"Limited CPU resources detected: {cpu_count} cores, \"\n",
        "                f\"{available_memory / 1024**3:.1f}GB RAM\"\n",
        "            )\n",
        "\n",
        "        # Default to CPU with capability confirmation\n",
        "        return \"cpu\"\n",
        "\n",
        "    def _validate_device_capability(self, device: str) -> bool:\n",
        "        \"\"\"\n",
        "        Validate device capability for image similarity operations.\n",
        "\n",
        "        Args:\n",
        "            device: Device specification to validate\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating device capability and availability\n",
        "        \"\"\"\n",
        "        # Validate CPU device with system resource assessment\n",
        "        if device == \"cpu\":\n",
        "            # Ensure minimum CPU and memory requirements\n",
        "            return (psutil.cpu_count(logical=False) >= 1 and\n",
        "                    psutil.virtual_memory().available >= 1 * 1024**3)\n",
        "\n",
        "        # Validate CUDA devices with comprehensive capability checking\n",
        "        if device.startswith(\"cuda\"):\n",
        "            # Check CUDA availability in PyTorch\n",
        "            if not torch.cuda.is_available():\n",
        "                return False\n",
        "\n",
        "            # Parse and validate specific device index if provided\n",
        "            if \":\" in device:\n",
        "                try:\n",
        "                    device_index = int(device.split(\":\")[1])\n",
        "                    # Validate device index is within available range\n",
        "                    if device_index >= torch.cuda.device_count():\n",
        "                        return False\n",
        "\n",
        "                    # Validate device compute capability and memory\n",
        "                    device_props = torch.cuda.get_device_properties(device_index)\n",
        "                    return (device_props.major >= 3 and\n",
        "                            device_props.total_memory >= 1 * 1024**3)  # ≥1GB VRAM\n",
        "\n",
        "                except (ValueError, IndexError):\n",
        "                    return False\n",
        "\n",
        "            # Generic CUDA validation\n",
        "            return True\n",
        "\n",
        "        # Validate Apple Silicon MPS device\n",
        "        if device == \"mps\":\n",
        "            return (hasattr(torch.backends, 'mps') and\n",
        "                    torch.backends.mps.is_available())\n",
        "\n",
        "        # Unknown device specification\n",
        "        return False\n",
        "\n",
        "    def _initialize_feature_detector_with_factory(\n",
        "        self,\n",
        "        detector: Optional[FeatureDetectorProtocol]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize ORB feature detector using factory pattern with protocol validation.\n",
        "\n",
        "        Args:\n",
        "            detector: Injectable detector or None for factory creation\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If detector does not conform to FeatureDetectorProtocol\n",
        "            OpenCVInitializationError: If detector creation fails\n",
        "        \"\"\"\n",
        "        # Initialize feature detector factory with optimization capabilities\n",
        "        self._feature_detector_factory = DefaultFeatureDetectorFactory(\n",
        "            resource_manager=self.resource_manager,\n",
        "            enable_optimization=self._enable_parameter_optimization,\n",
        "            cache_size=10\n",
        "        )\n",
        "\n",
        "        if detector is not None:\n",
        "            # Validate injectable detector conforms to protocol requirements\n",
        "            is_valid, violations = validate_protocol_implementation(\n",
        "                detector, FeatureDetectorProtocol, strict=True\n",
        "            )\n",
        "            if not is_valid:\n",
        "                raise ValueError(f\"Detector protocol validation failed: {violations}\")\n",
        "\n",
        "            # Store validated injectable detector with performance monitoring\n",
        "            self.orb_detector = detector\n",
        "            self._logger.info(\"Using injectable ORB detector with protocol validation\")\n",
        "        else:\n",
        "            # Create optimized detector using factory with retry logic\n",
        "            for attempt in range(self._max_initialization_retries):\n",
        "                try:\n",
        "                    # Create detector with production-optimized parameters\n",
        "                    self.orb_detector = self._feature_detector_factory.create(\n",
        "                        nfeatures=1000,  # Increased for better matching reliability\n",
        "                        scaleFactor=1.2,\n",
        "                        nlevels=8,\n",
        "                        edgeThreshold=31,\n",
        "                        patchSize=31,\n",
        "                        fastThreshold=20,\n",
        "                        optimize_parameters=self._enable_parameter_optimization,\n",
        "                        target_performance=\"balanced\"\n",
        "                    )\n",
        "\n",
        "                    # Log successful detector creation with factory statistics\n",
        "                    self._logger.info(\n",
        "                        \"ORB detector created via factory\",\n",
        "                        extra={'attempt': attempt + 1, 'optimization_enabled': self._enable_parameter_optimization}\n",
        "                    )\n",
        "                    break\n",
        "\n",
        "                except (OpenCVInitializationError, ResourceAllocationError) as e:\n",
        "                    # Log attempt failure and retry if attempts remaining\n",
        "                    if attempt < self._max_initialization_retries - 1:\n",
        "                        self._logger.warning(f\"Detector creation attempt {attempt + 1} failed: {e}\")\n",
        "                        continue\n",
        "                    # Re-raise with enhanced context if all attempts exhausted\n",
        "                    raise InitializationError(\n",
        "                        f\"Failed to create ORB detector after {self._max_initialization_retries} attempts\",\n",
        "                        component_name=\"orb_detector\",\n",
        "                        initialization_stage=\"factory_creation\",\n",
        "                        forensic_metadata=ForensicMetadata(\n",
        "                            operation_name=\"orb_detector_initialization\",\n",
        "                            algorithm_parameters={'max_retries': self._max_initialization_retries}\n",
        "                        )\n",
        "                    ) from e\n",
        "\n",
        "    def _initialize_matcher_with_factory(self, matcher: Optional[MatcherProtocol]) -> None:\n",
        "        \"\"\"\n",
        "        Initialize BFMatcher using factory pattern with protocol validation.\n",
        "\n",
        "        Args:\n",
        "            matcher: Injectable matcher or None for factory creation\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If matcher does not conform to MatcherProtocol\n",
        "            OpenCVInitializationError: If matcher creation fails\n",
        "        \"\"\"\n",
        "        # Initialize matcher factory with performance profiling\n",
        "        self._matcher_factory = DefaultMatcherFactory(\n",
        "            resource_manager=self.resource_manager,\n",
        "            enable_profiling=self._enable_performance_monitoring\n",
        "        )\n",
        "\n",
        "        if matcher is not None:\n",
        "            # Validate injectable matcher conforms to protocol requirements\n",
        "            is_valid, violations = validate_protocol_implementation(\n",
        "                matcher, MatcherProtocol, strict=True\n",
        "            )\n",
        "            if not is_valid:\n",
        "                raise ValueError(f\"Matcher protocol validation failed: {violations}\")\n",
        "\n",
        "            # Store validated injectable matcher\n",
        "            self.bf_matcher = matcher\n",
        "            self._logger.info(\"Using injectable BFMatcher with protocol validation\")\n",
        "        else:\n",
        "            # Create optimized matcher using factory with error handling\n",
        "            try:\n",
        "                # Create matcher optimized for binary descriptor matching\n",
        "                self.bf_matcher = self._matcher_factory.create(\n",
        "                    normType=cv2.NORM_HAMMING,  # Optimal for ORB binary descriptors\n",
        "                    crossCheck=True,  # Enhanced precision for quantitative analysis\n",
        "                    optimize_for_throughput=False  # Prioritize accuracy over speed\n",
        "                )\n",
        "\n",
        "                # Log successful matcher creation\n",
        "                self._logger.info(\"BFMatcher created via factory with cross-check enabled\")\n",
        "\n",
        "            except OpenCVInitializationError as e:\n",
        "                # Wrap in initialization error with enhanced context\n",
        "                raise InitializationError(\n",
        "                    f\"Failed to create BFMatcher: {e}\",\n",
        "                    component_name=\"bf_matcher\",\n",
        "                    initialization_stage=\"factory_creation\"\n",
        "                ) from e\n",
        "\n",
        "    def _initialize_clip_loader_with_factory(\n",
        "        self,\n",
        "        loader: Optional[ClipModelLoaderProtocol]\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize CLIP model loader using factory pattern with protocol validation.\n",
        "\n",
        "        Args:\n",
        "            loader: Injectable loader or None for factory creation\n",
        "        \"\"\"\n",
        "        if loader is not None:\n",
        "            # Validate injectable loader conforms to protocol requirements\n",
        "            is_valid, violations = validate_protocol_implementation(\n",
        "                loader, ClipModelLoaderProtocol, strict=True\n",
        "            )\n",
        "            if not is_valid:\n",
        "                raise ValueError(f\"CLIP loader protocol validation failed: {violations}\")\n",
        "\n",
        "            # Store validated injectable loader\n",
        "            self.clip_model_loader = loader\n",
        "            self._logger.info(\"Using injectable CLIP loader with protocol validation\")\n",
        "        else:\n",
        "            # Create default CLIP loader with enhanced capabilities\n",
        "            self.clip_model_loader = DefaultClipModelLoader(\n",
        "                resource_manager=self.resource_manager,\n",
        "                enable_model_caching=True,\n",
        "                enable_performance_monitoring=self._enable_performance_monitoring\n",
        "            )\n",
        "\n",
        "            # Log default loader creation\n",
        "            self._logger.info(\"Default CLIP loader created with caching and monitoring\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _validate_image_path(\n",
        "        image_path: Union[str, Path],\n",
        "        allow_symlinks: bool = False,\n",
        "        perform_content_validation: bool = True,\n",
        "        max_file_size_mb: float = 100.0\n",
        "    ) -> Path:\n",
        "        \"\"\"\n",
        "        Comprehensive image path validation with security analysis and forensic metadata.\n",
        "\n",
        "        Implements enterprise-grade path validation with symlink analysis, content verification,\n",
        "        security scanning, and comprehensive error reporting for production deployment.\n",
        "\n",
        "        Args:\n",
        "            image_path: Path string or Path object to validate\n",
        "            allow_symlinks: Whether to permit symlink resolution with safety checks\n",
        "            perform_content_validation: Whether to validate file content as image data\n",
        "            max_file_size_mb: Maximum allowed file size in megabytes\n",
        "\n",
        "        Returns:\n",
        "            Validated Path object with comprehensive security and integrity verification\n",
        "\n",
        "        Raises:\n",
        "            ImageNotFoundError: If path does not exist with search context\n",
        "            NotAFileError: If path exists but is not regular file with type analysis\n",
        "            SymlinkNotAllowedError: If symlinks encountered but not permitted with chain analysis\n",
        "            PermissionDeniedError: If insufficient permissions with ACL analysis\n",
        "            ImageUnreadableError: If file corrupted or invalid format with diagnostic data\n",
        "        \"\"\"\n",
        "        # Convert input to pathlib.Path for uniform handling with normalization\n",
        "        try:\n",
        "            # Resolve path with comprehensive normalization and validation\n",
        "            normalized_path = Path(image_path).resolve()\n",
        "        except (OSError, ValueError) as e:\n",
        "            # Handle path resolution failures with enhanced error context\n",
        "            raise ImageValidationError(\n",
        "                f\"Invalid path format or resolution failure: {image_path}\",\n",
        "                file_path=image_path,\n",
        "                validation_step=\"path_normalization\",\n",
        "                forensic_metadata=ForensicMetadata(\n",
        "                    operation_name=\"path_validation\",\n",
        "                    algorithm_parameters={'original_path': str(image_path)}\n",
        "                )\n",
        "            ) from e\n",
        "\n",
        "        # Verify path existence in filesystem with comprehensive search analysis\n",
        "        if not normalized_path.exists():\n",
        "            # Analyze potential alternative paths for diagnostic purposes\n",
        "            parent_dir = normalized_path.parent\n",
        "            suggested_alternatives = []\n",
        "\n",
        "            if parent_dir.exists():\n",
        "                # Search for similar filenames in parent directory\n",
        "                try:\n",
        "                    similar_files = [\n",
        "                        f for f in parent_dir.iterdir()\n",
        "                        if f.is_file() and\n",
        "                        f.stem.lower() in normalized_path.stem.lower() or\n",
        "                        normalized_path.stem.lower() in f.stem.lower()\n",
        "                    ]\n",
        "                    suggested_alternatives = [str(f) for f in similar_files[:5]]\n",
        "                except (PermissionError, OSError):\n",
        "                    # Handle directory access failures gracefully\n",
        "                    pass\n",
        "\n",
        "            # Raise enhanced not found error with search context\n",
        "            raise ImageNotFoundError(\n",
        "                f\"Image file does not exist: {normalized_path}\",\n",
        "                file_path=normalized_path,\n",
        "                search_paths=suggested_alternatives,\n",
        "                file_attributes={'parent_exists': parent_dir.exists() if parent_dir else False},\n",
        "                forensic_metadata=ForensicMetadata(\n",
        "                    operation_name=\"existence_validation\",\n",
        "                    algorithm_parameters={'search_performed': True}\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Handle symlink analysis and policy enforcement\n",
        "        if normalized_path.is_symlink():\n",
        "            if not allow_symlinks:\n",
        "                # Analyze symlink for security reporting\n",
        "                try:\n",
        "                    symlink_target = normalized_path.readlink()\n",
        "                    resolved_target = normalized_path.resolve()\n",
        "\n",
        "                    # Detect circular references in symlink chain\n",
        "                    resolution_chain = []\n",
        "                    current_path = normalized_path\n",
        "                    max_resolution_depth = 10\n",
        "\n",
        "                    for _ in range(max_resolution_depth):\n",
        "                        if current_path.is_symlink():\n",
        "                            resolution_chain.append(current_path)\n",
        "                            current_path = current_path.readlink()\n",
        "                            # Check for circular reference\n",
        "                            if current_path in resolution_chain:\n",
        "                                break\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    # Raise symlink policy violation with comprehensive analysis\n",
        "                    raise SymlinkNotAllowedError(\n",
        "                        f\"Symlinks not permitted by security policy: {normalized_path}\",\n",
        "                        file_path=normalized_path,\n",
        "                        symlink_target=resolved_target,\n",
        "                        resolution_chain=resolution_chain,\n",
        "                        forensic_metadata=ForensicMetadata(\n",
        "                            operation_name=\"symlink_analysis\",\n",
        "                            algorithm_parameters={\n",
        "                                'symlink_target': str(symlink_target),\n",
        "                                'resolution_depth': len(resolution_chain),\n",
        "                                'circular_reference': current_path in resolution_chain\n",
        "                            }\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                except (OSError, RuntimeError) as e:\n",
        "                    # Handle symlink analysis failures\n",
        "                    raise SymlinkNotAllowedError(\n",
        "                        f\"Symlink analysis failed for security validation: {normalized_path}\",\n",
        "                        file_path=normalized_path\n",
        "                    ) from e\n",
        "            else:\n",
        "                # Validate symlink target accessibility and safety\n",
        "                try:\n",
        "                    # Ensure symlink resolves to valid target\n",
        "                    resolved_target = normalized_path.resolve()\n",
        "                    if not resolved_target.exists():\n",
        "                        raise ImageNotFoundError(\n",
        "                            f\"Symlink target does not exist: {normalized_path} -> {resolved_target}\",\n",
        "                            file_path=normalized_path,\n",
        "                            file_attributes={'symlink_target': str(resolved_target)}\n",
        "                        )\n",
        "                except (OSError, RuntimeError) as e:\n",
        "                    raise SymlinkNotAllowedError(\n",
        "                        f\"Symlink resolution failed: {normalized_path}\",\n",
        "                        file_path=normalized_path\n",
        "                    ) from e\n",
        "\n",
        "        # Validate path points to regular file with comprehensive type analysis\n",
        "        if not normalized_path.is_file():\n",
        "            # Analyze actual file type for enhanced error reporting\n",
        "            actual_type = \"unknown\"\n",
        "            try:\n",
        "                if normalized_path.is_dir():\n",
        "                    actual_type = \"directory\"\n",
        "                elif normalized_path.is_symlink():\n",
        "                    actual_type = \"symlink\"\n",
        "                elif normalized_path.is_block_device():\n",
        "                    actual_type = \"block_device\"\n",
        "                elif normalized_path.is_char_device():\n",
        "                    actual_type = \"character_device\"\n",
        "                elif normalized_path.is_fifo():\n",
        "                    actual_type = \"named_pipe\"\n",
        "                elif normalized_path.is_socket():\n",
        "                    actual_type = \"socket\"\n",
        "            except (OSError, AttributeError):\n",
        "                # Handle file type detection failures\n",
        "                pass\n",
        "\n",
        "            # Raise file type error with comprehensive analysis\n",
        "            raise NotAFileError(\n",
        "                f\"Path is not a regular file: {normalized_path}\",\n",
        "                file_path=normalized_path,\n",
        "                actual_file_type=actual_type,\n",
        "                forensic_metadata=ForensicMetadata(\n",
        "                    operation_name=\"file_type_validation\",\n",
        "                    algorithm_parameters={'detected_type': actual_type}\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Comprehensive permission analysis with ACL assessment\n",
        "        try:\n",
        "            # Check basic read permission using OS-level access control\n",
        "            if not os.access(normalized_path, os.R_OK):\n",
        "                # Analyze permission structure for detailed reporting\n",
        "                file_stat = normalized_path.stat()\n",
        "                permission_analysis = {\n",
        "                    'owner_readable': bool(file_stat.st_mode & 0o400),\n",
        "                    'group_readable': bool(file_stat.st_mode & 0o040),\n",
        "                    'other_readable': bool(file_stat.st_mode & 0o004),\n",
        "                    'file_mode_octal': oct(file_stat.st_mode)[-3:],\n",
        "                    'owner_uid': file_stat.st_uid,\n",
        "                    'current_uid': os.getuid() if hasattr(os, 'getuid') else None\n",
        "                }\n",
        "\n",
        "                # Raise permission error with detailed ACL analysis\n",
        "                raise PermissionDeniedError(\n",
        "                    f\"Insufficient read permissions for image file: {normalized_path}\",\n",
        "                    file_path=normalized_path,\n",
        "                    permission_analysis=permission_analysis,\n",
        "                    required_permissions=['read'],\n",
        "                    forensic_metadata=ForensicMetadata(\n",
        "                        operation_name=\"permission_validation\",\n",
        "                        algorithm_parameters=permission_analysis\n",
        "                    )\n",
        "                )\n",
        "        except (OSError, AttributeError) as e:\n",
        "            # Handle permission analysis failures\n",
        "            raise PermissionDeniedError(\n",
        "                f\"Permission validation failed: {normalized_path}\",\n",
        "                file_path=normalized_path\n",
        "            ) from e\n",
        "\n",
        "        # File size validation for security and performance\n",
        "        try:\n",
        "            file_size_bytes = normalized_path.stat().st_size\n",
        "            file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "\n",
        "            if file_size_mb > max_file_size_mb:\n",
        "                raise ImageValidationError(\n",
        "                    f\"File size ({file_size_mb:.1f}MB) exceeds maximum allowed ({max_file_size_mb}MB): {normalized_path}\",\n",
        "                    file_path=normalized_path,\n",
        "                    validation_step=\"size_validation\",\n",
        "                    file_attributes={'size_mb': file_size_mb, 'max_allowed_mb': max_file_size_mb}\n",
        "                )\n",
        "\n",
        "        except OSError as e:\n",
        "            raise ImageValidationError(\n",
        "                f\"File size validation failed: {normalized_path}\",\n",
        "                file_path=normalized_path,\n",
        "                validation_step=\"size_validation\"\n",
        "            ) from e\n",
        "\n",
        "        # Optional content validation for image format verification\n",
        "        if perform_content_validation:\n",
        "            corruption_indicators = {}\n",
        "            format_analysis = {}\n",
        "\n",
        "            try:\n",
        "                # Attempt to open and validate image content using PIL\n",
        "                with Image.open(normalized_path) as img:\n",
        "                    # Basic format validation\n",
        "                    format_analysis = {\n",
        "                        'format': img.format,\n",
        "                        'mode': img.mode,\n",
        "                        'size': img.size,\n",
        "                        'has_transparency': hasattr(img, 'transparency') and img.transparency is not None\n",
        "                    }\n",
        "\n",
        "                    # Verify image can be loaded without corruption\n",
        "                    img.verify()\n",
        "\n",
        "            except (IOError, OSError) as e:\n",
        "                # Analyze corruption indicators for diagnostic reporting\n",
        "                corruption_indicators = {\n",
        "                    'pil_error': str(e),\n",
        "                    'file_truncated': 'truncated' in str(e).lower(),\n",
        "                    'format_unsupported': 'cannot identify' in str(e).lower()\n",
        "                }\n",
        "\n",
        "                # Raise image corruption error with diagnostic analysis\n",
        "                raise ImageUnreadableError(\n",
        "                    f\"Image file is corrupted or unsupported format: {normalized_path}\",\n",
        "                    file_path=normalized_path,\n",
        "                    corruption_indicators=corruption_indicators,\n",
        "                    format_analysis=format_analysis,\n",
        "                    forensic_metadata=ForensicMetadata(\n",
        "                        operation_name=\"content_validation\",\n",
        "                        algorithm_parameters={\n",
        "                            'validation_method': 'PIL_verification',\n",
        "                            'corruption_detected': True\n",
        "                        }\n",
        "                    )\n",
        "                ) from e\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle unexpected validation failures\n",
        "                raise ImageUnreadableError(\n",
        "                    f\"Unexpected error during image content validation: {normalized_path}\",\n",
        "                    file_path=normalized_path,\n",
        "                    corruption_indicators={'unexpected_error': str(e)}\n",
        "                ) from e\n",
        "\n",
        "        # Return validated, normalized, and security-verified path\n",
        "        return normalized_path\n",
        "\n",
        "    def _load_clip_with_comprehensive_monitoring(self) -> None:\n",
        "        \"\"\"\n",
        "        Thread-safe lazy loading of CLIP model with comprehensive performance monitoring.\n",
        "\n",
        "        Implements enterprise-grade model loading with double-checked locking, performance\n",
        "        profiling, resource optimization, and comprehensive error recovery strategies.\n",
        "\n",
        "        Raises:\n",
        "            ModelLoadError: If model loading fails after all recovery attempts with detailed context\n",
        "        \"\"\"\n",
        "        # First check without lock acquisition for performance optimization\n",
        "        if self.clip_model is not None and self.clip_preprocess is not None:\n",
        "            return\n",
        "\n",
        "        # Acquire initialization lock for thread-safe model loading coordination\n",
        "        with self._initialization_lock:\n",
        "            # Second check after lock acquisition to prevent duplicate loading\n",
        "            if self.clip_model is not None and self.clip_preprocess is not None:\n",
        "                return\n",
        "\n",
        "            # Initialize comprehensive performance monitoring\n",
        "            loading_start_time = time.perf_counter()\n",
        "            memory_before = psutil.Process().memory_info().rss\n",
        "            gpu_memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "            # Track loading attempts with detailed context\n",
        "            loading_attempts = []\n",
        "            device_fallback_chain = [self.device]\n",
        "\n",
        "            # Add CPU fallback if not already specified\n",
        "            if self.device != \"cpu\":\n",
        "                device_fallback_chain.append(\"cpu\")\n",
        "\n",
        "            # Attempt model loading with comprehensive error handling and fallback\n",
        "            for attempt in range(self._max_initialization_retries):\n",
        "                for device_attempt in device_fallback_chain:\n",
        "                    attempt_start_time = time.perf_counter()\n",
        "                    attempt_context = {\n",
        "                        'attempt_number': attempt + 1,\n",
        "                        'device': device_attempt,\n",
        "                        'model_name': self._clip_model_name,\n",
        "                        'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                    }\n",
        "\n",
        "                    try:\n",
        "                        # Attempt CLIP model loading through injectable loader with monitoring\n",
        "                        self._logger.info(f\"Attempting CLIP model loading: attempt {attempt + 1}, device {device_attempt}\")\n",
        "\n",
        "                        # Load model using factory loader with enhanced configuration\n",
        "                        self.clip_model, self.clip_preprocess = self.clip_model_loader(\n",
        "                            model_name=self._clip_model_name,\n",
        "                            device=device_attempt,\n",
        "                            enable_optimization=True,\n",
        "                            precision=\"float32\"  # Ensure numerical stability for quantitative analysis\n",
        "                        )\n",
        "\n",
        "                        # Validate successful loading with functionality verification\n",
        "                        if self.clip_model is None or self.clip_preprocess is None:\n",
        "                            raise ModelLoadError(\n",
        "                                f\"CLIP model or preprocess function is None after loading\",\n",
        "                                model_name=self._clip_model_name\n",
        "                            )\n",
        "\n",
        "                        # Perform basic functionality test to ensure model integrity\n",
        "                        try:\n",
        "                            # Create test tensor to validate model functionality\n",
        "                            test_tensor = torch.randn(1, 3, 224, 224).to(device_attempt)\n",
        "                            with torch.no_grad():\n",
        "                                # Test forward pass to ensure model is functional\n",
        "                                test_embedding = self.clip_model.encode_image(test_tensor)\n",
        "                                # Validate embedding properties\n",
        "                                if test_embedding is None or test_embedding.numel() == 0:\n",
        "                                    raise ModelLoadError(\"Model produces invalid embeddings\")\n",
        "                        except Exception as e:\n",
        "                            raise ModelLoadError(f\"Model functionality test failed: {e}\") from e\n",
        "\n",
        "                        # Record successful loading performance metrics\n",
        "                        attempt_duration = time.perf_counter() - attempt_start_time\n",
        "                        total_duration = time.perf_counter() - loading_start_time\n",
        "                        memory_after = psutil.Process().memory_info().rss\n",
        "                        gpu_memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "                        # Store comprehensive loading performance data\n",
        "                        self._clip_loading_performance = {\n",
        "                            'success': True,\n",
        "                            'total_attempts': attempt + 1,\n",
        "                            'successful_device': device_attempt,\n",
        "                            'total_loading_time': total_duration,\n",
        "                            'final_attempt_time': attempt_duration,\n",
        "                            'memory_delta_mb': (memory_after - memory_before) / 1024 / 1024,\n",
        "                            'gpu_memory_delta_mb': (gpu_memory_after - gpu_memory_before) / 1024 / 1024,\n",
        "                            'model_parameters': self._estimate_model_parameters(),\n",
        "                            'device_fallback_used': device_attempt != self.device\n",
        "                        }\n",
        "\n",
        "                        # Update device if fallback was successful\n",
        "                        if device_attempt != self.device:\n",
        "                            self._logger.info(f\"Device fallback successful: {self.device} -> {device_attempt}\")\n",
        "                            self.device = device_attempt\n",
        "\n",
        "                        # Log successful loading with comprehensive metrics\n",
        "                        self._logger.info(\n",
        "                            f\"CLIP model loaded successfully: {self._clip_model_name}\",\n",
        "                            extra=self._clip_loading_performance\n",
        "                        )\n",
        "\n",
        "                        return  # Exit successfully\n",
        "\n",
        "                    except (OSError, RuntimeError, ModuleNotFoundError) as e:\n",
        "                        # Record failed attempt with detailed context\n",
        "                        attempt_duration = time.perf_counter() - attempt_start_time\n",
        "                        attempt_context.update({\n",
        "                            'success': False,\n",
        "                            'error_type': type(e).__name__,\n",
        "                            'error_message': str(e),\n",
        "                            'attempt_duration': attempt_duration\n",
        "                        })\n",
        "                        loading_attempts.append(attempt_context)\n",
        "\n",
        "                        # Log attempt failure with context\n",
        "                        self._logger.warning(\n",
        "                            f\"CLIP loading attempt failed: {type(e).__name__}: {e}\",\n",
        "                            extra=attempt_context\n",
        "                        )\n",
        "\n",
        "                        # Continue to next device in fallback chain\n",
        "                        continue\n",
        "\n",
        "                    except torch.cuda.OutOfMemoryError as e:\n",
        "                        # Handle GPU memory exhaustion with automatic cleanup\n",
        "                        attempt_context.update({\n",
        "                            'success': False,\n",
        "                            'error_type': 'OutOfMemoryError',\n",
        "                            'error_message': str(e),\n",
        "                            'gpu_memory_info': self._get_gpu_memory_info()\n",
        "                        })\n",
        "                        loading_attempts.append(attempt_context)\n",
        "\n",
        "                        # Perform aggressive GPU memory cleanup\n",
        "                        if torch.cuda.is_available():\n",
        "                            torch.cuda.empty_cache()\n",
        "                            torch.cuda.synchronize()\n",
        "\n",
        "                        # Log GPU memory exhaustion\n",
        "                        self._logger.error(\n",
        "                            f\"GPU OOM during CLIP loading: {e}\",\n",
        "                            extra=attempt_context\n",
        "                        )\n",
        "\n",
        "                        # Skip to CPU fallback immediately for OOM errors\n",
        "                        if device_attempt != \"cpu\":\n",
        "                            continue\n",
        "                        else:\n",
        "                            # Re-raise if CPU also fails with OOM\n",
        "                            break\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # Handle unexpected errors with comprehensive logging\n",
        "                        attempt_context.update({\n",
        "                            'success': False,\n",
        "                            'error_type': type(e).__name__,\n",
        "                            'error_message': str(e),\n",
        "                            'unexpected_error': True\n",
        "                        })\n",
        "                        loading_attempts.append(attempt_context)\n",
        "\n",
        "                        self._logger.error(\n",
        "                            f\"Unexpected error during CLIP loading: {e}\",\n",
        "                            extra=attempt_context\n",
        "                        )\n",
        "\n",
        "                        # Continue to next device for unexpected errors\n",
        "                        continue\n",
        "\n",
        "                # Brief pause between retry attempts to allow system recovery\n",
        "                if attempt < self._max_initialization_retries - 1:\n",
        "                    time.sleep(1.0)\n",
        "\n",
        "            # All attempts failed - generate comprehensive error report\n",
        "            total_duration = time.perf_counter() - loading_start_time\n",
        "            error_context = {\n",
        "                'model_name': self._clip_model_name,\n",
        "                'total_attempts': len(loading_attempts),\n",
        "                'total_duration': total_duration,\n",
        "                'attempted_devices': device_fallback_chain,\n",
        "                'loading_attempts': loading_attempts,\n",
        "                'system_resources': {\n",
        "                    'available_memory_gb': psutil.virtual_memory().available / 1024**3,\n",
        "                    'cpu_count': psutil.cpu_count(),\n",
        "                    'gpu_available': torch.cuda.is_available(),\n",
        "                    'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Store failed loading performance data for analysis\n",
        "            self._clip_loading_performance = {\n",
        "                'success': False,\n",
        "                'total_attempts': len(loading_attempts),\n",
        "                'total_loading_time': total_duration,\n",
        "                'error_context': error_context\n",
        "            }\n",
        "\n",
        "            # Generate comprehensive error summary\n",
        "            error_types = [attempt['error_type'] for attempt in loading_attempts]\n",
        "            most_common_error = max(set(error_types), key=error_types.count) if error_types else 'Unknown'\n",
        "\n",
        "            # Raise comprehensive model loading error with full context\n",
        "            raise ModelLoadError(\n",
        "                f\"Failed to load CLIP model {self._clip_model_name} after {len(loading_attempts)} attempts \"\n",
        "                f\"across {len(device_fallback_chain)} devices. Most common error: {most_common_error}\",\n",
        "                model_name=self._clip_model_name,\n",
        "                algorithm_context=error_context,\n",
        "                forensic_metadata=ForensicMetadata(\n",
        "                    operation_name=\"clip_model_loading\",\n",
        "                    algorithm_parameters={\n",
        "                        'model_name': self._clip_model_name,\n",
        "                        'device_chain': device_fallback_chain,\n",
        "                        'max_retries': self._max_initialization_retries\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def _estimate_model_parameters(self) -> Optional[int]:\n",
        "        \"\"\"\n",
        "        Estimate number of parameters in loaded CLIP model.\n",
        "\n",
        "        Returns:\n",
        "            Estimated parameter count or None if estimation fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.clip_model is not None:\n",
        "                # Count trainable parameters in model\n",
        "                param_count = sum(p.numel() for p in self.clip_model.parameters())\n",
        "                return param_count\n",
        "        except Exception:\n",
        "            # Handle parameter counting failures gracefully\n",
        "            pass\n",
        "        return None\n",
        "\n",
        "    def _get_gpu_memory_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get comprehensive GPU memory information for debugging.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing GPU memory statistics\n",
        "        \"\"\"\n",
        "        gpu_info = {}\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                for device_idx in range(torch.cuda.device_count()):\n",
        "                    device_props = torch.cuda.get_device_properties(device_idx)\n",
        "                    gpu_info[f'gpu_{device_idx}'] = {\n",
        "                        'name': device_props.name,\n",
        "                        'total_memory_gb': device_props.total_memory / 1024**3,\n",
        "                        'allocated_mb': torch.cuda.memory_allocated(device_idx) / 1024**2,\n",
        "                        'reserved_mb': torch.cuda.memory_reserved(device_idx) / 1024**2,\n",
        "                        'free_mb': (device_props.total_memory - torch.cuda.memory_allocated(device_idx)) / 1024**2\n",
        "                    }\n",
        "            except Exception:\n",
        "                gpu_info = {'error': 'Failed to retrieve GPU memory information'}\n",
        "\n",
        "        return gpu_info\n",
        "\n",
        "    def perceptual_hash_difference(\n",
        "        self,\n",
        "        image1: Union[str, Path, Image.Image, np.ndarray],\n",
        "        image2: Union[str, Path, Image.Image, np.ndarray],\n",
        "        hash_size: int = 8,\n",
        "        normalize: bool = False,\n",
        "        return_similarity: bool = False,\n",
        "        compute_confidence_interval: bool = True,\n",
        "        statistical_analysis: bool = True,\n",
        "        performance_monitoring: bool = True\n",
        "    ) -> Union[int, float, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Compute perceptual hash difference with comprehensive statistical analysis and validation.\n",
        "\n",
        "        Implements DCT-based perceptual hashing with rigorous mathematical validation,\n",
        "        statistical confidence assessment, performance monitoring, and enterprise-grade\n",
        "        error handling for quantitative image similarity assessment.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        1. Image preprocessing: I' = resize(grayscale(I), 4N×4N) where N = hash_size\n",
        "        2. DCT computation: F(u,v) = α(u)α(v) Σₓ Σᵧ f(x,y)cos((2x+1)uπ/2N)cos((2y+1)vπ/2N)\n",
        "        3. Low-frequency extraction: D_lf = F[0:N, 0:N] (top-left N×N coefficients)\n",
        "        4. Median thresholding: τ = median(D_lf), h[i,j] = 1 if D_lf[i,j] > τ, else 0\n",
        "        5. Hamming distance: d_H = Σᵢⱼ |h₁[i,j] ⊕ h₂[i,j]| ∈ [0, N²]\n",
        "        6. Normalized similarity: sim = 1 - d_H/N² ∈ [0,1]\n",
        "\n",
        "        Args:\n",
        "            image1: First image as path, PIL Image, or numpy array\n",
        "            image2: Second image as path, PIL Image, or numpy array\n",
        "            hash_size: Hash dimension N producing N²-bit perceptual hash\n",
        "            normalize: If True, return d_H / N² ∈ [0,1]\n",
        "            return_similarity: If True, return 1 - normalized_distance (requires normalize=True)\n",
        "            compute_confidence_interval: Whether to compute statistical confidence intervals\n",
        "            statistical_analysis: Whether to perform comprehensive statistical analysis\n",
        "            performance_monitoring: Whether to track performance metrics\n",
        "\n",
        "        Returns:\n",
        "            Raw Hamming distance (int), normalized distance (float), or comprehensive analysis (Dict)\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If parameters violate mathematical constraints\n",
        "            ImageValidationError: If images cannot be processed\n",
        "            PerformanceError: If operation exceeds resource constraints\n",
        "        \"\"\"\n",
        "        # Record method execution start for performance monitoring\n",
        "        method_start_time = time.perf_counter()\n",
        "        memory_before = psutil.Process().memory_info().rss\n",
        "\n",
        "        # Validate hash_size parameter against mathematical constraints\n",
        "        if hash_size <= 0:\n",
        "            raise ValueError(f\"hash_size must be positive integer, got {hash_size}\")\n",
        "        if hash_size > 64:\n",
        "            # Practical upper limit for computational efficiency\n",
        "            raise ValueError(f\"hash_size too large for practical computation, got {hash_size} > 64\")\n",
        "\n",
        "        # Validate parameter combination logic with mathematical consistency\n",
        "        if return_similarity and not normalize:\n",
        "            raise ValueError(\"return_similarity requires normalize=True for mathematical consistency\")\n",
        "\n",
        "        # Initialize comprehensive performance tracking\n",
        "        operation_metrics = {\n",
        "            'method_name': 'perceptual_hash_difference',\n",
        "            'start_time': datetime.datetime.utcnow().isoformat(),\n",
        "            'parameters': {\n",
        "                'hash_size': hash_size,\n",
        "                'normalize': normalize,\n",
        "                'return_similarity': return_similarity,\n",
        "                'statistical_analysis': statistical_analysis\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Helper function for comprehensive image loading and preprocessing with validation\n",
        "        def _load_and_preprocess_image_with_validation(\n",
        "            img_input: Union[str, Path, Image.Image, np.ndarray],\n",
        "            image_label: str\n",
        "        ) -> Tuple[Image.Image, Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Load and preprocess image with comprehensive validation and metadata extraction.\n",
        "\n",
        "            Args:\n",
        "                img_input: Image input in various supported formats\n",
        "                image_label: Label for error reporting and logging\n",
        "\n",
        "            Returns:\n",
        "                Tuple of (processed_PIL_image, preprocessing_metadata)\n",
        "            \"\"\"\n",
        "            # Initialize preprocessing metadata for analysis\n",
        "            preprocessing_metadata = {\n",
        "                'input_type': type(img_input).__name__,\n",
        "                'original_format': None,\n",
        "                'original_size': None,\n",
        "                'conversion_applied': [],\n",
        "                'validation_performed': True\n",
        "            }\n",
        "\n",
        "            # Handle numpy array input with comprehensive format analysis\n",
        "            if isinstance(img_input, np.ndarray):\n",
        "                # Validate array dimensions for image data compatibility\n",
        "                if img_input.ndim not in [2, 3]:\n",
        "                    raise ImageValidationError(\n",
        "                        f\"Invalid {image_label} array dimensions: {img_input.ndim}, expected 2 or 3\",\n",
        "                        validation_step=\"array_dimension_validation\",\n",
        "                        algorithm_context={'array_shape': img_input.shape}\n",
        "                    )\n",
        "\n",
        "                # Record original array properties for metadata\n",
        "                preprocessing_metadata.update({\n",
        "                    'original_size': img_input.shape,\n",
        "                    'array_dtype': str(img_input.dtype),\n",
        "                    'value_range': (float(img_input.min()), float(img_input.max()))\n",
        "                })\n",
        "\n",
        "                # Handle different array data types with mathematical precision\n",
        "                if img_input.dtype == np.uint8:\n",
        "                    # Standard 8-bit image data - direct conversion\n",
        "                    array_data = img_input\n",
        "                    preprocessing_metadata['conversion_applied'].append('dtype_validated')\n",
        "                elif img_input.dtype in [np.float32, np.float64]:\n",
        "                    # Floating point data - normalize to [0,255] range with validation\n",
        "                    if img_input.max() <= 1.0 and img_input.min() >= 0.0:\n",
        "                        # Assume [0,1] range and convert to [0,255] with precision preservation\n",
        "                        array_data = (img_input * 255.0).astype(np.uint8)\n",
        "                        preprocessing_metadata['conversion_applied'].append('float_to_uint8_normalized')\n",
        "                    elif img_input.max() <= 255.0 and img_input.min() >= 0.0:\n",
        "                        # Assume already in [0,255] range\n",
        "                        array_data = img_input.astype(np.uint8)\n",
        "                        preprocessing_metadata['conversion_applied'].append('float_to_uint8_direct')\n",
        "                    else:\n",
        "                        # Apply min-max normalization for arbitrary ranges\n",
        "                        array_min, array_max = img_input.min(), img_input.max()\n",
        "                        if array_max > array_min:\n",
        "                            normalized = (img_input - array_min) / (array_max - array_min)\n",
        "                            array_data = (normalized * 255.0).astype(np.uint8)\n",
        "                            preprocessing_metadata['conversion_applied'].append('minmax_normalization')\n",
        "                        else:\n",
        "                            # Constant array - convert to zero array\n",
        "                            array_data = np.zeros_like(img_input, dtype=np.uint8)\n",
        "                            preprocessing_metadata['conversion_applied'].append('constant_array_handling')\n",
        "                else:\n",
        "                    # Handle other data types with range normalization\n",
        "                    array_min, array_max = img_input.min(), img_input.max()\n",
        "                    if array_max > array_min:\n",
        "                        # Apply linear scaling to [0,255] range\n",
        "                        normalized = (img_input.astype(np.float64) - array_min) / (array_max - array_min)\n",
        "                        array_data = (normalized * 255.0).astype(np.uint8)\n",
        "                        preprocessing_metadata['conversion_applied'].append('generic_dtype_normalization')\n",
        "                    else:\n",
        "                        # Handle constant arrays\n",
        "                        array_data = np.zeros_like(img_input, dtype=np.uint8)\n",
        "                        preprocessing_metadata['conversion_applied'].append('constant_array_fallback')\n",
        "\n",
        "                # Convert BGR to RGB if 3-channel array (OpenCV convention handling)\n",
        "                if array_data.ndim == 3 and array_data.shape[2] == 3:\n",
        "                    # Assume BGR ordering from OpenCV and convert to RGB\n",
        "                    array_data = cv2.cvtColor(array_data, cv2.COLOR_BGR2RGB)\n",
        "                    preprocessing_metadata['conversion_applied'].append('BGR_to_RGB_conversion')\n",
        "                elif array_data.ndim == 3 and array_data.shape[2] == 4:\n",
        "                    # Handle RGBA data by removing alpha channel\n",
        "                    array_data = array_data[:, :, :3]\n",
        "                    preprocessing_metadata['conversion_applied'].append('RGBA_to_RGB_conversion')\n",
        "\n",
        "                # Create PIL Image from processed array with error handling\n",
        "                try:\n",
        "                    pil_image = Image.fromarray(array_data)\n",
        "                    preprocessing_metadata['conversion_applied'].append('array_to_PIL_conversion')\n",
        "                except Exception as e:\n",
        "                    raise ImageValidationError(\n",
        "                        f\"Failed to convert {image_label} numpy array to PIL Image: {e}\",\n",
        "                        validation_step=\"array_to_PIL_conversion\",\n",
        "                        algorithm_context={'array_shape': array_data.shape, 'array_dtype': str(array_data.dtype)}\n",
        "                    ) from e\n",
        "\n",
        "            # Handle PIL Image input with comprehensive validation\n",
        "            elif isinstance(img_input, Image.Image):\n",
        "                # Record original PIL image properties\n",
        "                preprocessing_metadata.update({\n",
        "                    'original_format': img_input.format,\n",
        "                    'original_mode': img_input.mode,\n",
        "                    'original_size': img_input.size\n",
        "                })\n",
        "\n",
        "                # Use provided PIL Image with mode validation\n",
        "                pil_image = img_input\n",
        "                preprocessing_metadata['conversion_applied'].append('PIL_image_direct_use')\n",
        "\n",
        "            # Handle path input with comprehensive validation and loading\n",
        "            elif isinstance(img_input, (str, Path)):\n",
        "                # Validate image path using enhanced validation with security analysis\n",
        "                validated_path = self._validate_image_path(\n",
        "                    img_input,\n",
        "                    self._allow_symlinks,\n",
        "                    perform_content_validation=True,\n",
        "                    max_file_size_mb=50.0  # Reasonable limit for hash computation\n",
        "                )\n",
        "\n",
        "                # Record path information for metadata\n",
        "                preprocessing_metadata.update({\n",
        "                    'file_path': str(validated_path),\n",
        "                    'file_size_bytes': validated_path.stat().st_size\n",
        "                })\n",
        "\n",
        "                # Attempt image loading with comprehensive error handling\n",
        "                try:\n",
        "                    # Load image using PIL with format preservation\n",
        "                    pil_image = Image.open(validated_path)\n",
        "                    preprocessing_metadata.update({\n",
        "                        'original_format': pil_image.format,\n",
        "                        'original_mode': pil_image.mode,\n",
        "                        'original_size': pil_image.size\n",
        "                    })\n",
        "                    preprocessing_metadata['conversion_applied'].append('file_to_PIL_loading')\n",
        "\n",
        "                except (IOError, OSError) as e:\n",
        "                    # Enhanced error reporting with file analysis\n",
        "                    raise ImageUnreadableError(\n",
        "                        f\"Failed to load {image_label} from {validated_path}: {e}\",\n",
        "                        file_path=validated_path,\n",
        "                        corruption_indicators={'loading_error': str(e)},\n",
        "                        forensic_metadata=ForensicMetadata(\n",
        "                            operation_name=\"perceptual_hash_image_loading\",\n",
        "                            algorithm_parameters={'image_label': image_label}\n",
        "                        )\n",
        "                    ) from e\n",
        "\n",
        "            else:\n",
        "                # Unsupported input type with comprehensive error context\n",
        "                raise ValueError(\n",
        "                    f\"Unsupported {image_label} input type: {type(img_input)}. \"\n",
        "                    f\"Supported types: str, Path, PIL.Image, np.ndarray\"\n",
        "                )\n",
        "\n",
        "            # Convert to grayscale for perceptual hash computation (mathematical requirement)\n",
        "            try:\n",
        "                # Convert to grayscale mode 'L' for DCT computation\n",
        "                grayscale_image = pil_image.convert('L')\n",
        "                preprocessing_metadata['conversion_applied'].append('grayscale_conversion')\n",
        "\n",
        "                # Record final processed image properties\n",
        "                preprocessing_metadata.update({\n",
        "                    'final_mode': grayscale_image.mode,\n",
        "                    'final_size': grayscale_image.size\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                raise ImageValidationError(\n",
        "                    f\"Failed to convert {image_label} to grayscale: {e}\",\n",
        "                    validation_step=\"grayscale_conversion\"\n",
        "                ) from e\n",
        "\n",
        "            return grayscale_image, preprocessing_metadata\n",
        "\n",
        "        # Load and preprocess both input images with comprehensive validation\n",
        "        try:\n",
        "            # Process first image with detailed metadata collection\n",
        "            processed_image1, metadata1 = _load_and_preprocess_image_with_validation(image1, \"image1\")\n",
        "            # Process second image with detailed metadata collection\n",
        "            processed_image2, metadata2 = _load_and_preprocess_image_with_validation(image2, \"image2\")\n",
        "\n",
        "            # Record preprocessing performance metrics\n",
        "            preprocessing_time = time.perf_counter() - method_start_time\n",
        "            operation_metrics['preprocessing_time_seconds'] = preprocessing_time\n",
        "            operation_metrics['image_metadata'] = {\n",
        "                'image1': metadata1,\n",
        "                'image2': metadata2\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error context for preprocessing failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'preprocessing',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "\n",
        "            # Log preprocessing failure with performance context\n",
        "            if performance_monitoring and hasattr(self, '_logger'):\n",
        "                self._logger.error(\n",
        "                    \"Perceptual hash preprocessing failed\",\n",
        "                    extra=operation_metrics\n",
        "                )\n",
        "\n",
        "            # Re-raise with preserved context\n",
        "            raise\n",
        "\n",
        "        # Compute perceptual hashes using imagehash library with error handling\n",
        "        hash_computation_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Generate pHash using DCT-based algorithm with specified hash_size\n",
        "            # Mathematical process: resize → DCT → low-frequency extraction → median threshold\n",
        "            hash1 = imagehash.phash(processed_image1, hash_size=hash_size)\n",
        "            hash2 = imagehash.phash(processed_image2, hash_size=hash_size)\n",
        "\n",
        "            # Validate hash generation success\n",
        "            if hash1 is None or hash2 is None:\n",
        "                raise RuntimeError(\"Perceptual hash computation returned None\")\n",
        "\n",
        "            # Record hash computation performance\n",
        "            hash_computation_time = time.perf_counter() - hash_computation_start\n",
        "            operation_metrics['hash_computation_time_seconds'] = hash_computation_time\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for hash computation failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'hash_computation',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e),\n",
        "                'hash_size': hash_size\n",
        "            }\n",
        "\n",
        "            raise RuntimeError(f\"Perceptual hash computation failed: {e}\") from e\n",
        "\n",
        "        # Compute Hamming distance between binary hashes\n",
        "        # Mathematical formula: d_H = Σᵢⱼ |h₁[i,j] ⊕ h₂[i,j]|\n",
        "        hamming_distance = int(hash1 - hash2)\n",
        "\n",
        "        # Validate Hamming distance is within mathematical bounds\n",
        "        max_possible_distance = hash_size * hash_size\n",
        "        if not 0 <= hamming_distance <= max_possible_distance:\n",
        "            raise RuntimeError(\n",
        "                f\"Invalid Hamming distance {hamming_distance}, \"\n",
        "                f\"expected range [0, {max_possible_distance}]\"\n",
        "            )\n",
        "\n",
        "        # Record basic result metrics\n",
        "        operation_metrics['hamming_distance'] = hamming_distance\n",
        "        operation_metrics['max_possible_distance'] = max_possible_distance\n",
        "\n",
        "        # Perform statistical analysis if requested\n",
        "        statistical_properties = None\n",
        "        if statistical_analysis:\n",
        "            # Compute statistical properties of hash difference\n",
        "            # Convert hashes to binary arrays for detailed analysis\n",
        "            hash1_array = np.array([int(bit) for bit in str(hash1).replace(':', '')])\n",
        "            hash2_array = np.array([int(bit) for bit in str(hash2).replace(':', '')])\n",
        "\n",
        "            # Compute bit-wise differences for statistical analysis\n",
        "            bit_differences = np.abs(hash1_array - hash2_array)\n",
        "\n",
        "            # Compute comprehensive statistical properties\n",
        "            statistical_properties = StatisticalProperties(\n",
        "                sample_size=len(bit_differences),\n",
        "                mean=float(np.mean(bit_differences)),\n",
        "                median=float(np.median(bit_differences)),\n",
        "                variance=float(np.var(bit_differences)),\n",
        "                standard_deviation=float(np.std(bit_differences)),\n",
        "                standard_error=float(np.std(bit_differences) / np.sqrt(len(bit_differences))),\n",
        "                confidence_level=0.95,\n",
        "                distribution_type=\"bernoulli\"\n",
        "            )\n",
        "\n",
        "            # Compute confidence interval if requested\n",
        "            if compute_confidence_interval and len(bit_differences) > 1:\n",
        "                # Use binomial proportion confidence interval for Hamming distance\n",
        "                n_bits = len(bit_differences)\n",
        "                p_hat = hamming_distance / n_bits  # Proportion of different bits\n",
        "\n",
        "                # Wilson score interval for more robust confidence estimation\n",
        "                z_score = stats.norm.ppf(0.975)  # 95% confidence level\n",
        "                denominator = 1 + z_score**2 / n_bits\n",
        "                center = (p_hat + z_score**2 / (2 * n_bits)) / denominator\n",
        "                margin = z_score * np.sqrt(p_hat * (1 - p_hat) / n_bits + z_score**2 / (4 * n_bits**2)) / denominator\n",
        "\n",
        "                # Convert back to Hamming distance scale\n",
        "                ci_lower = max(0, (center - margin) * n_bits)\n",
        "                ci_upper = min(n_bits, (center + margin) * n_bits)\n",
        "\n",
        "                # Update statistical properties with confidence interval\n",
        "                statistical_properties.confidence_interval_lower = ci_lower\n",
        "                statistical_properties.confidence_interval_upper = ci_upper\n",
        "\n",
        "        # Apply normalization and similarity transformation if requested\n",
        "        if normalize:\n",
        "            # Compute normalized distance: d_norm = d_H / N²\n",
        "            normalized_distance = float(hamming_distance) / float(max_possible_distance)\n",
        "            operation_metrics['normalized_distance'] = normalized_distance\n",
        "\n",
        "            if return_similarity:\n",
        "                # Compute similarity score: similarity = 1 - d_norm\n",
        "                similarity_score = 1.0 - normalized_distance\n",
        "                operation_metrics['similarity_score'] = similarity_score\n",
        "\n",
        "                # Validate similarity score is in valid range\n",
        "                if not 0.0 <= similarity_score <= 1.0:\n",
        "                    raise RuntimeError(f\"Invalid similarity score {similarity_score}, expected [0,1]\")\n",
        "\n",
        "                result_value = similarity_score\n",
        "            else:\n",
        "                result_value = normalized_distance\n",
        "        else:\n",
        "            # Return raw Hamming distance as integer\n",
        "            result_value = hamming_distance\n",
        "\n",
        "        # Record comprehensive performance metrics\n",
        "        total_execution_time = time.perf_counter() - method_start_time\n",
        "        memory_after = psutil.Process().memory_info().rss\n",
        "        memory_delta = memory_after - memory_before\n",
        "\n",
        "        operation_metrics.update({\n",
        "            'total_execution_time_seconds': total_execution_time,\n",
        "            'memory_delta_bytes': memory_delta,\n",
        "            'result_value': result_value,\n",
        "            'statistical_analysis_performed': statistical_analysis,\n",
        "            'confidence_interval_computed': compute_confidence_interval and statistical_properties is not None\n",
        "        })\n",
        "\n",
        "        # Store performance metrics for analysis if monitoring enabled\n",
        "        if performance_monitoring and hasattr(self, '_method_performance_history'):\n",
        "            with self._performance_lock:\n",
        "                self._method_performance_history['perceptual_hash_difference'].append(operation_metrics)\n",
        "                # Limit history size to prevent memory growth\n",
        "                if len(self._method_performance_history['perceptual_hash_difference']) > 100:\n",
        "                    self._method_performance_history['perceptual_hash_difference'] = \\\n",
        "                        self._method_performance_history['perceptual_hash_difference'][-50:]\n",
        "\n",
        "        # Return comprehensive result based on analysis level requested\n",
        "        if statistical_analysis:\n",
        "            # Return comprehensive analysis with all computed metrics\n",
        "            comprehensive_result = {\n",
        "                'hamming_distance': hamming_distance,\n",
        "                'max_possible_distance': max_possible_distance,\n",
        "                'normalized_distance': normalized_distance if normalize else None,\n",
        "                'similarity_score': similarity_score if return_similarity else None,\n",
        "                'statistical_properties': statistical_properties,\n",
        "                'performance_metrics': operation_metrics,\n",
        "                'hash_size': hash_size,\n",
        "                'computation_successful': True\n",
        "            }\n",
        "            return comprehensive_result\n",
        "        else:\n",
        "            # Return simple numerical result\n",
        "            return result_value\n",
        "\n",
        "    def feature_match_ratio(\n",
        "        self,\n",
        "        image1: Union[str, Path, np.ndarray],\n",
        "        image2: Union[str, Path, np.ndarray],\n",
        "        distance_threshold: int = 50,\n",
        "        normalization_strategy: Literal[\"total_matches\", \"min_keypoints\"] = \"total_matches\",\n",
        "        apply_ratio_test: bool = False,\n",
        "        ratio_threshold: float = 0.75,\n",
        "        resize_max_side: Optional[int] = None,\n",
        "        return_detailed_result: bool = True,\n",
        "        geometric_verification: bool = True,\n",
        "        performance_monitoring: bool = True,\n",
        "        statistical_analysis: bool = True\n",
        "    ) -> Union[float, FeatureMatchResult]:\n",
        "        \"\"\"\n",
        "        Compute feature matching similarity with comprehensive statistical analysis and validation.\n",
        "\n",
        "        Implements ORB feature detection and matching with rigorous mathematical validation,\n",
        "        geometric verification, statistical confidence assessment, and enterprise-grade\n",
        "        performance monitoring for quantitative image similarity analysis.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        1. ORB Feature Detection:\n",
        "          - FAST corners: C = {p : |I(p) - I(x)| > τ for x ∈ circle(p)}\n",
        "          - Harris response: R = det(M) - k·trace²(M) where M is structure tensor\n",
        "          - Orientation: θ = atan2(m₀₁, m₁₀) where mₚq = Σᵨ uᵖvᵍI(u,v)\n",
        "\n",
        "        2. BRIEF Descriptors:\n",
        "          - Binary tests: τ(p; x,y) := 1 if p(x) < p(y), else 0\n",
        "          - Descriptor: f_n(p) = Σ_{1≤i≤n} 2^(i-1)τ(p; x_i, y_i)\n",
        "          - Hamming distance: d_H(D₁,D₂) = Σᵢ |D₁[i] ⊕ D₂[i]|\n",
        "\n",
        "        3. Matching Strategies:\n",
        "          - Total matches: ratio = |good| / |matches|\n",
        "          - Min keypoints: ratio = |good| / min(|KP₁|, |KP₂|)\n",
        "          - Lowe's test: accept if d₁/d₂ < τ where d₁ < d₂\n",
        "\n",
        "        4. Geometric Verification:\n",
        "          - RANSAC homography: H·p₁ ≈ p₂ for inlier correspondences\n",
        "          - Consensus set: inliers = {(p₁,p₂) : ||H·p₁ - p₂|| < ε}\n",
        "\n",
        "        Args:\n",
        "            image1: First image as path or numpy array\n",
        "            image2: Second image as path or numpy array\n",
        "            distance_threshold: Maximum Hamming distance for good match (0-256)\n",
        "            normalization_strategy: Method for ratio computation\n",
        "            apply_ratio_test: Whether to use Lowe's ratio test for disambiguation\n",
        "            ratio_threshold: Threshold for Lowe's ratio test (typically 0.75)\n",
        "            resize_max_side: Optional downscaling to limit computational cost\n",
        "            return_detailed_result: If True, return comprehensive FeatureMatchResult\n",
        "            geometric_verification: Whether to perform RANSAC geometric verification\n",
        "            performance_monitoring: Whether to track detailed performance metrics\n",
        "            statistical_analysis: Whether to compute statistical properties\n",
        "\n",
        "        Returns:\n",
        "            Similarity ratio [0,1] or comprehensive FeatureMatchResult with analysis\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If parameters violate mathematical constraints\n",
        "            RuntimeError: If feature detection or matching fails\n",
        "            ResourceAllocationError: If insufficient memory for processing\n",
        "        \"\"\"\n",
        "        # Record method execution start for comprehensive performance monitoring\n",
        "        method_start_time = time.perf_counter()\n",
        "        memory_before = psutil.Process().memory_info().rss\n",
        "\n",
        "        # Validate normalization strategy parameter against supported algorithms\n",
        "        valid_strategies = {\"total_matches\", \"min_keypoints\"}\n",
        "        if normalization_strategy not in valid_strategies:\n",
        "            raise ValueError(f\"Invalid normalization_strategy: {normalization_strategy}, valid: {valid_strategies}\")\n",
        "\n",
        "        # Validate distance threshold for Hamming distance (8-bit binary descriptors)\n",
        "        if not 0 <= distance_threshold <= 256:\n",
        "            raise ValueError(f\"distance_threshold must be in [0,256], got {distance_threshold}\")\n",
        "\n",
        "        # Validate ratio threshold for Lowe's disambiguation test\n",
        "        if not 0.0 < ratio_threshold < 1.0:\n",
        "            raise ValueError(f\"ratio_threshold must be in (0,1), got {ratio_threshold}\")\n",
        "\n",
        "        # Validate resize parameter for computational efficiency\n",
        "        if resize_max_side is not None and resize_max_side <= 0:\n",
        "            raise ValueError(f\"resize_max_side must be positive, got {resize_max_side}\")\n",
        "\n",
        "        # Initialize comprehensive performance tracking with detailed metrics\n",
        "        operation_metrics = {\n",
        "            'method_name': 'feature_match_ratio',\n",
        "            'start_time': datetime.datetime.utcnow().isoformat(),\n",
        "            'parameters': {\n",
        "                'distance_threshold': distance_threshold,\n",
        "                'normalization_strategy': normalization_strategy,\n",
        "                'apply_ratio_test': apply_ratio_test,\n",
        "                'ratio_threshold': ratio_threshold,\n",
        "                'resize_max_side': resize_max_side,\n",
        "                'geometric_verification': geometric_verification\n",
        "            },\n",
        "            'processing_stages': {}\n",
        "        }\n",
        "\n",
        "        # Helper function for comprehensive image loading and preprocessing\n",
        "        def _load_and_prepare_image_for_features(\n",
        "            img_input: Union[str, Path, np.ndarray],\n",
        "            image_label: str\n",
        "        ) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Load image and prepare for ORB feature detection with optimization.\n",
        "\n",
        "            Args:\n",
        "                img_input: Image input in supported format\n",
        "                image_label: Label for error reporting and performance tracking\n",
        "\n",
        "            Returns:\n",
        "                Tuple of (grayscale_array, processing_metadata)\n",
        "            \"\"\"\n",
        "            # Initialize processing metadata for comprehensive analysis\n",
        "            processing_metadata = {\n",
        "                'input_type': type(img_input).__name__,\n",
        "                'transformations_applied': [],\n",
        "                'validation_performed': True,\n",
        "                'optimization_applied': []\n",
        "            }\n",
        "\n",
        "            # Handle path input with enhanced validation\n",
        "            if isinstance(img_input, (str, Path)):\n",
        "                # Validate image path with comprehensive security analysis\n",
        "                validated_path = self._validate_image_path(\n",
        "                    img_input,\n",
        "                    self._allow_symlinks,\n",
        "                    perform_content_validation=True,\n",
        "                    max_file_size_mb=100.0  # Generous limit for feature detection\n",
        "                )\n",
        "\n",
        "                # Load image in BGR color format for OpenCV processing\n",
        "                image_array = cv2.imread(str(validated_path), cv2.IMREAD_COLOR)\n",
        "                if image_array is None:\n",
        "                    raise RuntimeError(f\"OpenCV failed to load {image_label}: {validated_path}\")\n",
        "\n",
        "                # Record original image properties\n",
        "                processing_metadata.update({\n",
        "                    'file_path': str(validated_path),\n",
        "                    'original_size': (image_array.shape[1], image_array.shape[0]),  # (width, height)\n",
        "                    'original_channels': image_array.shape[2] if image_array.ndim == 3 else 1\n",
        "                })\n",
        "                processing_metadata['transformations_applied'].append('file_loading')\n",
        "\n",
        "            # Handle numpy array input with comprehensive validation\n",
        "            elif isinstance(img_input, np.ndarray):\n",
        "                # Validate array dimensions for image data compatibility\n",
        "                if img_input.ndim not in [2, 3]:\n",
        "                    raise ValueError(f\"Invalid {image_label} array dimensions: {img_input.ndim}\")\n",
        "\n",
        "                # Copy input array to prevent mutation of original data\n",
        "                image_array = img_input.copy()\n",
        "\n",
        "                # Record original array properties\n",
        "                processing_metadata.update({\n",
        "                    'original_size': (image_array.shape[1], image_array.shape[0]),\n",
        "                    'original_channels': image_array.shape[2] if image_array.ndim == 3 else 1,\n",
        "                    'array_dtype': str(image_array.dtype),\n",
        "                    'value_range': (float(image_array.min()), float(image_array.max()))\n",
        "                })\n",
        "\n",
        "                # Ensure 3-channel color image for consistent OpenCV processing\n",
        "                if image_array.ndim == 2:\n",
        "                    # Convert grayscale to 3-channel BGR for uniform handling\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)\n",
        "                    processing_metadata['transformations_applied'].append('grayscale_to_BGR')\n",
        "                elif image_array.shape[2] == 4:\n",
        "                    # Remove alpha channel for OpenCV compatibility\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_BGRA2BGR)\n",
        "                    processing_metadata['transformations_applied'].append('BGRA_to_BGR')\n",
        "                elif image_array.shape[2] == 3:\n",
        "                    # Assume RGB and convert to BGR for OpenCV\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
        "                    processing_metadata['transformations_applied'].append('RGB_to_BGR')\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported {image_label} input type: {type(img_input)}\")\n",
        "\n",
        "            # Apply optional resizing with aspect ratio preservation for efficiency\n",
        "            if resize_max_side is not None:\n",
        "                # Get current image dimensions\n",
        "                current_height, current_width = image_array.shape[:2]\n",
        "                current_max_side = max(current_height, current_width)\n",
        "\n",
        "                # Resize only if current size exceeds limit for computational efficiency\n",
        "                if current_max_side > resize_max_side:\n",
        "                    # Compute scale factor preserving aspect ratio\n",
        "                    scale_factor = resize_max_side / current_max_side\n",
        "                    new_width = int(current_width * scale_factor)\n",
        "                    new_height = int(current_height * scale_factor)\n",
        "\n",
        "                    # Apply high-quality resizing using area interpolation for downscaling\n",
        "                    image_array = cv2.resize(\n",
        "                        image_array,\n",
        "                        (new_width, new_height),\n",
        "                        interpolation=cv2.INTER_AREA\n",
        "                    )\n",
        "\n",
        "                    # Record resizing transformation\n",
        "                    processing_metadata.update({\n",
        "                        'resized_size': (new_width, new_height),\n",
        "                        'scale_factor': scale_factor\n",
        "                    })\n",
        "                    processing_metadata['transformations_applied'].append('aspect_preserving_resize')\n",
        "                    processing_metadata['optimization_applied'].append('computational_downscaling')\n",
        "\n",
        "            # Convert to grayscale for ORB feature detection (algorithm requirement)\n",
        "            if image_array.ndim == 3:\n",
        "                grayscale_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n",
        "                processing_metadata['transformations_applied'].append('BGR_to_grayscale')\n",
        "            else:\n",
        "                grayscale_array = image_array\n",
        "\n",
        "            # Validate final grayscale array properties\n",
        "            if grayscale_array.ndim != 2:\n",
        "                raise RuntimeError(f\"Failed to produce grayscale array for {image_label}\")\n",
        "\n",
        "            # Record final processing results\n",
        "            processing_metadata.update({\n",
        "                'final_size': (grayscale_array.shape[1], grayscale_array.shape[0]),\n",
        "                'final_dtype': str(grayscale_array.dtype)\n",
        "            })\n",
        "\n",
        "            return grayscale_array, processing_metadata\n",
        "\n",
        "        # Load and prepare both images for feature detection with performance monitoring\n",
        "        preprocessing_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Process first image with comprehensive preprocessing\n",
        "            prepared_image1, metadata1 = _load_and_prepare_image_for_features(image1, \"image1\")\n",
        "            # Process second image with comprehensive preprocessing\n",
        "            prepared_image2, metadata2 = _load_and_prepare_image_for_features(image2, \"image2\")\n",
        "\n",
        "            # Record preprocessing performance metrics\n",
        "            preprocessing_time = time.perf_counter() - preprocessing_start\n",
        "            operation_metrics['processing_stages']['preprocessing'] = {\n",
        "                'duration_seconds': preprocessing_time,\n",
        "                'image1_metadata': metadata1,\n",
        "                'image2_metadata': metadata2\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error context for preprocessing failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'preprocessing',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise RuntimeError(f\"Image preprocessing failed: {e}\") from e\n",
        "\n",
        "        # Detect keypoints and compute ORB descriptors with performance monitoring\n",
        "        feature_detection_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Image 1: Detect FAST corners and compute BRIEF descriptors\n",
        "            keypoints1, descriptors1 = self.orb_detector.detectAndCompute(prepared_image1, None)\n",
        "            # Image 2: Detect FAST corners and compute BRIEF descriptors\n",
        "            keypoints2, descriptors2 = self.orb_detector.detectAndCompute(prepared_image2, None)\n",
        "\n",
        "            # Record feature detection performance and results\n",
        "            feature_detection_time = time.perf_counter() - feature_detection_start\n",
        "            operation_metrics['processing_stages']['feature_detection'] = {\n",
        "                'duration_seconds': feature_detection_time,\n",
        "                'keypoints_image1': len(keypoints1) if keypoints1 else 0,\n",
        "                'keypoints_image2': len(keypoints2) if keypoints2 else 0,\n",
        "                'descriptors_image1_shape': descriptors1.shape if descriptors1 is not None else None,\n",
        "                'descriptors_image2_shape': descriptors2.shape if descriptors2 is not None else None\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for feature detection failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'feature_detection',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise RuntimeError(f\"ORB feature detection failed: {e}\") from e\n",
        "\n",
        "        # Handle cases where no descriptors are detected with detailed analysis\n",
        "        if descriptors1 is None or descriptors2 is None or len(descriptors1) == 0 or len(descriptors2) == 0:\n",
        "            # Create comprehensive result for no-feature case with diagnostic information\n",
        "            no_features_result = FeatureMatchResult(\n",
        "                similarity_ratio=0.0,\n",
        "                total_matches=0,\n",
        "                good_matches=0,\n",
        "                keypoints_image1=len(keypoints1) if keypoints1 else 0,\n",
        "                keypoints_image2=len(keypoints2) if keypoints2 else 0,\n",
        "                normalization_strategy=normalization_strategy,\n",
        "                confidence_level=0.0,\n",
        "                geometric_verification_passed=None,\n",
        "                matching_time_seconds=0.0,\n",
        "                keypoint_detection_time_seconds=feature_detection_time,\n",
        "                distance_threshold_used=distance_threshold,\n",
        "                lowe_ratio_threshold=ratio_threshold if apply_ratio_test else None\n",
        "            )\n",
        "\n",
        "            # Record no-features case in performance metrics\n",
        "            operation_metrics['result_type'] = 'no_features_detected'\n",
        "            operation_metrics['total_execution_time'] = time.perf_counter() - method_start_time\n",
        "\n",
        "            # Store performance metrics if monitoring enabled\n",
        "            if performance_monitoring:\n",
        "                self._record_method_performance('feature_match_ratio', operation_metrics)\n",
        "\n",
        "            return no_features_result if return_detailed_result else 0.0\n",
        "\n",
        "        # Perform descriptor matching using configured algorithm with performance monitoring\n",
        "        matching_start = time.perf_counter()\n",
        "        good_matches = []\n",
        "        total_matches = 0\n",
        "        match_distances = []\n",
        "\n",
        "        try:\n",
        "            if apply_ratio_test:\n",
        "                # Apply Lowe's ratio test using k-nearest neighbor matching\n",
        "                # Mathematical foundation: accept match if d₁/d₂ < threshold for disambiguation\n",
        "                knn_matches = self.bf_matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
        "\n",
        "                # Process KNN matches with ratio test validation\n",
        "                for match_pair in knn_matches:\n",
        "                    if len(match_pair) == 2:\n",
        "                        best_match, second_match = match_pair\n",
        "                        # Apply Lowe's ratio test criterion: d₁/d₂ < τ\n",
        "                        if best_match.distance < ratio_threshold * second_match.distance:\n",
        "                            good_matches.append(best_match)\n",
        "                            match_distances.append(best_match.distance)\n",
        "                        total_matches += 1\n",
        "                    elif len(match_pair) == 1:\n",
        "                        # Only one match found - accept it (unambiguous case)\n",
        "                        good_matches.append(match_pair[0])\n",
        "                        match_distances.append(match_pair[0].distance)\n",
        "                        total_matches += 1\n",
        "            else:\n",
        "                # Use simple distance threshold matching with cross-check validation\n",
        "                matches = self.bf_matcher.match(descriptors1, descriptors2)\n",
        "                total_matches = len(matches)\n",
        "\n",
        "                # Filter matches by Hamming distance threshold\n",
        "                for match in matches:\n",
        "                    if match.distance <= distance_threshold:\n",
        "                        good_matches.append(match)\n",
        "                    match_distances.append(match.distance)\n",
        "\n",
        "            # Record matching performance metrics\n",
        "            matching_time = time.perf_counter() - matching_start\n",
        "            operation_metrics['processing_stages']['matching'] = {\n",
        "                'duration_seconds': matching_time,\n",
        "                'total_matches': total_matches,\n",
        "                'good_matches': len(good_matches),\n",
        "                'ratio_test_applied': apply_ratio_test,\n",
        "                'average_distance': float(np.mean(match_distances)) if match_distances else 0.0,\n",
        "                'min_distance': float(np.min(match_distances)) if match_distances else 0.0,\n",
        "                'max_distance': float(np.max(match_distances)) if match_distances else 0.0\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for matching failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'matching',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise RuntimeError(f\"Descriptor matching failed: {e}\") from e\n",
        "\n",
        "        # Compute similarity ratio using specified normalization strategy\n",
        "        if total_matches == 0:\n",
        "            similarity_ratio = 0.0\n",
        "        else:\n",
        "            if normalization_strategy == \"total_matches\":\n",
        "                # Normalize by total number of attempted matches\n",
        "                similarity_ratio = float(len(good_matches)) / float(total_matches)\n",
        "            else:  # \"min_keypoints\"\n",
        "                # Normalize by minimum number of detected keypoints\n",
        "                min_keypoints = min(len(keypoints1), len(keypoints2))\n",
        "                if min_keypoints == 0:\n",
        "                    similarity_ratio = 0.0\n",
        "                else:\n",
        "                    # Clamp ratio to [0,1] range for min_keypoints strategy\n",
        "                    similarity_ratio = min(float(len(good_matches)) / float(min_keypoints), 1.0)\n",
        "\n",
        "        # Perform geometric verification using RANSAC if requested and sufficient matches\n",
        "        geometric_verification_passed = None\n",
        "        homography_inlier_count = None\n",
        "        homography_inlier_ratio = None\n",
        "\n",
        "        if geometric_verification and len(good_matches) >= 4:  # Minimum for homography estimation\n",
        "            geometric_verification_start = time.perf_counter()\n",
        "\n",
        "            try:\n",
        "                # Extract point correspondences from good matches\n",
        "                src_points = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "                dst_points = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "\n",
        "                # Estimate homography using RANSAC with robust parameters\n",
        "                homography_matrix, inlier_mask = cv2.findHomography(\n",
        "                    src_points,\n",
        "                    dst_points,\n",
        "                    cv2.RANSAC,\n",
        "                    ransacReprojectionThreshold=3.0,  # Reprojection error threshold\n",
        "                    maxIters=2000,  # Maximum RANSAC iterations\n",
        "                    confidence=0.99  # Confidence level for RANSAC\n",
        "                )\n",
        "\n",
        "                # Analyze geometric verification results\n",
        "                if homography_matrix is not None and inlier_mask is not None:\n",
        "                    # Count inliers from RANSAC consensus set\n",
        "                    homography_inlier_count = int(np.sum(inlier_mask))\n",
        "                    homography_inlier_ratio = float(homography_inlier_count) / float(len(good_matches))\n",
        "\n",
        "                    # Determine if geometric verification passed based on inlier ratio\n",
        "                    geometric_verification_passed = homography_inlier_ratio >= 0.3  # 30% inlier threshold\n",
        "                else:\n",
        "                    # Homography estimation failed\n",
        "                    homography_inlier_count = 0\n",
        "                    homography_inlier_ratio = 0.0\n",
        "                    geometric_verification_passed = False\n",
        "\n",
        "                # Record geometric verification performance\n",
        "                geometric_verification_time = time.perf_counter() - geometric_verification_start\n",
        "                operation_metrics['processing_stages']['geometric_verification'] = {\n",
        "                    'duration_seconds': geometric_verification_time,\n",
        "                    'homography_found': homography_matrix is not None,\n",
        "                    'inlier_count': homography_inlier_count,\n",
        "                    'inlier_ratio': homography_inlier_ratio,\n",
        "                    'verification_passed': geometric_verification_passed\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle geometric verification failures gracefully\n",
        "                geometric_verification_passed = False\n",
        "                operation_metrics['processing_stages']['geometric_verification'] = {\n",
        "                    'error': str(e),\n",
        "                    'verification_passed': False\n",
        "                }\n",
        "\n",
        "        # Compute statistical properties of match distances if analysis requested\n",
        "        match_distance_statistics = None\n",
        "        if statistical_analysis and match_distances:\n",
        "            try:\n",
        "                # Compute comprehensive statistical properties of Hamming distances\n",
        "                distances_array = np.array(match_distances)\n",
        "\n",
        "                match_distance_statistics = StatisticalProperties(\n",
        "                    sample_size=len(distances_array),\n",
        "                    mean=float(np.mean(distances_array)),\n",
        "                    median=float(np.median(distances_array)),\n",
        "                    variance=float(np.var(distances_array)),\n",
        "                    standard_deviation=float(np.std(distances_array)),\n",
        "                    standard_error=float(np.std(distances_array) / np.sqrt(len(distances_array))),\n",
        "                    skewness=float(stats.skew(distances_array)),\n",
        "                    kurtosis=float(stats.kurtosis(distances_array)),\n",
        "                    confidence_level=0.95\n",
        "                )\n",
        "\n",
        "                # Compute confidence interval for mean distance\n",
        "                if len(distances_array) > 1:\n",
        "                    ci_lower, ci_upper = match_distance_statistics.compute_confidence_interval(\n",
        "                        confidence_level=0.95,\n",
        "                        distribution=\"t\"\n",
        "                    )\n",
        "                    match_distance_statistics.confidence_interval_lower = ci_lower\n",
        "                    match_distance_statistics.confidence_interval_upper = ci_upper\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle statistical analysis failures gracefully\n",
        "                operation_metrics['statistical_analysis_error'] = str(e)\n",
        "\n",
        "        # Compute confidence level based on match statistics and geometric consistency\n",
        "        confidence_level = None\n",
        "        if len(good_matches) > 0:\n",
        "            # Base confidence on number of good matches and geometric consistency\n",
        "            match_confidence = min(float(len(good_matches)) / 50.0, 1.0)  # Normalize to [0,1]\n",
        "\n",
        "            # Adjust confidence based on geometric verification if performed\n",
        "            if geometric_verification_passed is True:\n",
        "                geometric_confidence = homography_inlier_ratio if homography_inlier_ratio else 0.5\n",
        "                confidence_level = (match_confidence + geometric_confidence) / 2.0\n",
        "            elif geometric_verification_passed is False:\n",
        "                # Reduce confidence if geometric verification failed\n",
        "                confidence_level = match_confidence * 0.5\n",
        "            else:\n",
        "                # No geometric verification performed\n",
        "                confidence_level = match_confidence\n",
        "        else:\n",
        "            confidence_level = 0.0\n",
        "\n",
        "        # Record comprehensive performance metrics\n",
        "        total_execution_time = time.perf_counter() - method_start_time\n",
        "        memory_after = psutil.Process().memory_info().rss\n",
        "        memory_delta = memory_after - memory_before\n",
        "\n",
        "        operation_metrics.update({\n",
        "            'total_execution_time_seconds': total_execution_time,\n",
        "            'memory_delta_bytes': memory_delta,\n",
        "            'similarity_ratio': similarity_ratio,\n",
        "            'confidence_level': confidence_level,\n",
        "            'result_type': 'successful_matching'\n",
        "        })\n",
        "\n",
        "        # Store performance metrics for analysis if monitoring enabled\n",
        "        if performance_monitoring:\n",
        "            self._record_method_performance('feature_match_ratio', operation_metrics)\n",
        "\n",
        "        # Create comprehensive result structure with all computed metrics\n",
        "        detailed_result = FeatureMatchResult(\n",
        "            similarity_ratio=similarity_ratio,\n",
        "            total_matches=total_matches,\n",
        "            good_matches=len(good_matches),\n",
        "            keypoints_image1=len(keypoints1),\n",
        "            keypoints_image2=len(keypoints2),\n",
        "            normalization_strategy=normalization_strategy,\n",
        "            confidence_level=confidence_level,\n",
        "            match_distance_statistics=match_distance_statistics,\n",
        "            geometric_verification_passed=geometric_verification_passed,\n",
        "            homography_inlier_count=homography_inlier_count,\n",
        "            homography_inlier_ratio=homography_inlier_ratio,\n",
        "            matching_time_seconds=operation_metrics['processing_stages']['matching']['duration_seconds'],\n",
        "            keypoint_detection_time_seconds=operation_metrics['processing_stages']['feature_detection']['duration_seconds'],\n",
        "            distance_threshold_used=distance_threshold,\n",
        "            lowe_ratio_threshold=ratio_threshold if apply_ratio_test else None\n",
        "        )\n",
        "\n",
        "        # Return appropriate result format based on request\n",
        "        if return_detailed_result:\n",
        "            return detailed_result\n",
        "        else:\n",
        "            return similarity_ratio\n",
        "\n",
        "    def _record_method_performance(self, method_name: str, metrics: Dict[str, Any]) -> None:\n",
        "        \"\"\"\n",
        "        Record method performance metrics for analysis and optimization.\n",
        "\n",
        "        Args:\n",
        "            method_name: Name of method being monitored\n",
        "            metrics: Performance metrics dictionary\n",
        "        \"\"\"\n",
        "        if hasattr(self, '_method_performance_history') and hasattr(self, '_performance_lock'):\n",
        "            with self._performance_lock:\n",
        "                self._method_performance_history[method_name].append(metrics)\n",
        "                # Limit history size to prevent memory growth\n",
        "                if len(self._method_performance_history[method_name]) > 100:\n",
        "                    self._method_performance_history[method_name] = \\\n",
        "                        self._method_performance_history[method_name][-50:]\n",
        "\n",
        "    def histogram_correlation(\n",
        "        self,\n",
        "        image1: Union[str, Path, np.ndarray],\n",
        "        image2: Union[str, Path, np.ndarray],\n",
        "        bins: Union[int, Tuple[int, int]] = (50, 60),\n",
        "        metric: Literal[\"correlation\", \"chi-square\", \"intersection\", \"bhattacharyya\"] = \"correlation\",\n",
        "        mask1: Optional[np.ndarray] = None,\n",
        "        mask2: Optional[np.ndarray] = None,\n",
        "        preserve_aspect: bool = True,\n",
        "        resize_size: Tuple[int, int] = (256, 256),\n",
        "        on_zero_histogram: Literal[\"error\", \"zero\", \"nan\"] = \"error\",\n",
        "        color_space: Literal[\"HSV\", \"RGB\", \"LAB\"] = \"HSV\",\n",
        "        statistical_analysis: bool = True,\n",
        "        performance_monitoring: bool = True,\n",
        "        adaptive_binning: bool = False,\n",
        "        entropy_analysis: bool = True\n",
        "    ) -> Union[float, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Compute histogram correlation with comprehensive statistical analysis and optimization.\n",
        "\n",
        "        Implements rigorous color histogram comparison with mathematical validation,\n",
        "        adaptive binning strategies, entropy analysis, and enterprise-grade performance\n",
        "        monitoring for quantitative image similarity assessment.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        1. Color Space Transformation: I' = transform(I, color_space)\n",
        "        2. Histogram Computation: H_c(i) = Σₓᵧ 𝟙{bin(I'_c(x,y)) = i} for channel c\n",
        "        3. Normalization: Ĥ_c = H_c / Σᵢ H_c[i] (L1 normalization)\n",
        "        4. Statistical Metrics:\n",
        "          - Pearson Correlation: ρ = Σᵢ(h₁[i]-μ₁)(h₂[i]-μ₂) / (σ₁σ₂)\n",
        "          - Chi-Square Distance: χ² = Σᵢ (h₁[i]-h₂[i])² / (h₁[i]+h₂[i])\n",
        "          - Intersection: I = Σᵢ min(h₁[i], h₂[i])\n",
        "          - Bhattacharyya: d_B = -ln(Σᵢ √(h₁[i]h₂[i]))\n",
        "        5. Entropy Analysis: H(X) = -Σᵢ p(i)log₂(p(i)) for information content\n",
        "\n",
        "        Args:\n",
        "            image1: First image as path or numpy array\n",
        "            image2: Second image as path or numpy array\n",
        "            bins: Number of bins per channel (int) or tuple (H_bins, S_bins) for HSV\n",
        "            metric: Statistical distance/similarity metric for comparison\n",
        "            mask1: Optional binary mask for image1 region of interest\n",
        "            mask2: Optional binary mask for image2 region of interest\n",
        "            preserve_aspect: Whether to maintain aspect ratio during resize\n",
        "            resize_size: Target dimensions (width, height) for preprocessing\n",
        "            on_zero_histogram: Behavior when zero histogram encountered\n",
        "            color_space: Color space for histogram computation\n",
        "            statistical_analysis: Whether to perform comprehensive statistical analysis\n",
        "            performance_monitoring: Whether to track detailed performance metrics\n",
        "            adaptive_binning: Whether to use adaptive binning based on image characteristics\n",
        "            entropy_analysis: Whether to compute information-theoretic measures\n",
        "\n",
        "        Returns:\n",
        "            Similarity score/distance or comprehensive analysis dictionary\n",
        "\n",
        "        Raises:\n",
        "            HistogramError: On computation failures or invalid parameters\n",
        "            ValueError: On invalid parameter combinations\n",
        "            ResourceAllocationError: If insufficient memory for processing\n",
        "        \"\"\"\n",
        "        # Record method execution start for comprehensive performance monitoring\n",
        "        method_start_time = time.perf_counter()\n",
        "        memory_before = psutil.Process().memory_info().rss\n",
        "\n",
        "        # Validate metric selection against supported statistical algorithms\n",
        "        supported_metrics = {\"correlation\", \"chi-square\", \"intersection\", \"bhattacharyya\"}\n",
        "        if metric not in supported_metrics:\n",
        "            raise HistogramError(f\"Unsupported metric: {metric}. Supported: {supported_metrics}\")\n",
        "\n",
        "        # Map metric names to OpenCV constants with mathematical interpretation\n",
        "        metric_constants = {\n",
        "            \"correlation\": cv2.HISTCMP_CORREL,      # Pearson correlation: higher = more similar\n",
        "            \"chi-square\": cv2.HISTCMP_CHISQR,      # Chi-square distance: lower = more similar\n",
        "            \"intersection\": cv2.HISTCMP_INTERSECT, # Histogram intersection: higher = more similar\n",
        "            \"bhattacharyya\": cv2.HISTCMP_BHATTACHARYYA  # Bhattacharyya distance: lower = more similar\n",
        "        }\n",
        "\n",
        "        # Validate and normalize bins parameter with mathematical constraints\n",
        "        if isinstance(bins, int):\n",
        "            if bins <= 0 or bins > 256:\n",
        "                raise ValueError(f\"bins must be in range [1,256], got {bins}\")\n",
        "            # Use same number of bins for all channels\n",
        "            hist_bins = [bins] * 3\n",
        "            bins_description = f\"uniform_{bins}\"\n",
        "        elif isinstance(bins, (tuple, list)) and len(bins) == 2:\n",
        "            if any(b <= 0 or b > 256 for b in bins):\n",
        "                raise ValueError(f\"All bin counts must be in [1,256], got {bins}\")\n",
        "            # Use specified bins for H and S channels, default for third channel\n",
        "            hist_bins = [bins[0], bins[1], 50]\n",
        "            bins_description = f\"custom_{bins[0]}x{bins[1]}\"\n",
        "        else:\n",
        "            raise ValueError(f\"bins must be int or 2-tuple, got {type(bins)}\")\n",
        "\n",
        "        # Validate color space selection with supported transformations\n",
        "        supported_color_spaces = {\"HSV\", \"RGB\", \"LAB\"}\n",
        "        if color_space not in supported_color_spaces:\n",
        "            raise ValueError(f\"Unsupported color_space: {color_space}\")\n",
        "\n",
        "        # Initialize comprehensive performance tracking with detailed metrics\n",
        "        operation_metrics = {\n",
        "            'method_name': 'histogram_correlation',\n",
        "            'start_time': datetime.datetime.utcnow().isoformat(),\n",
        "            'parameters': {\n",
        "                'bins': bins,\n",
        "                'metric': metric,\n",
        "                'color_space': color_space,\n",
        "                'preserve_aspect': preserve_aspect,\n",
        "                'resize_size': resize_size,\n",
        "                'adaptive_binning': adaptive_binning,\n",
        "                'entropy_analysis': entropy_analysis\n",
        "            },\n",
        "            'processing_stages': {}\n",
        "        }\n",
        "\n",
        "        # Helper function for comprehensive image loading and preprocessing\n",
        "        def _load_and_prepare_image_for_histogram(\n",
        "            img_input: Union[str, Path, np.ndarray],\n",
        "            mask: Optional[np.ndarray],\n",
        "            image_label: str\n",
        "        ) -> Tuple[np.ndarray, Optional[np.ndarray], Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Load image and prepare for histogram computation with optimization.\n",
        "\n",
        "            Args:\n",
        "                img_input: Image input in supported format\n",
        "                mask: Optional mask for region of interest\n",
        "                image_label: Label for error reporting and performance tracking\n",
        "\n",
        "            Returns:\n",
        "                Tuple of (processed_array, processed_mask, processing_metadata)\n",
        "            \"\"\"\n",
        "            # Initialize processing metadata for comprehensive analysis\n",
        "            processing_metadata = {\n",
        "                'input_type': type(img_input).__name__,\n",
        "                'transformations_applied': [],\n",
        "                'optimization_applied': [],\n",
        "                'validation_performed': True\n",
        "            }\n",
        "\n",
        "            # Handle path-based image loading with enhanced validation\n",
        "            if isinstance(img_input, (str, Path)):\n",
        "                # Validate image path with comprehensive security analysis\n",
        "                validated_path = self._validate_image_path(\n",
        "                    img_input,\n",
        "                    self._allow_symlinks,\n",
        "                    perform_content_validation=True,\n",
        "                    max_file_size_mb=200.0  # Generous limit for histogram analysis\n",
        "                )\n",
        "\n",
        "                # Load image using OpenCV in BGR format for consistent processing\n",
        "                image_array = cv2.imread(str(validated_path), cv2.IMREAD_COLOR)\n",
        "                if image_array is None:\n",
        "                    raise HistogramError(f\"Failed to load {image_label}: {validated_path}\")\n",
        "\n",
        "                # Record original image properties for metadata\n",
        "                processing_metadata.update({\n",
        "                    'file_path': str(validated_path),\n",
        "                    'original_size': (image_array.shape[1], image_array.shape[0]),\n",
        "                    'original_channels': image_array.shape[2] if image_array.ndim == 3 else 1\n",
        "                })\n",
        "                processing_metadata['transformations_applied'].append('file_loading')\n",
        "\n",
        "            # Handle numpy array input with comprehensive validation\n",
        "            elif isinstance(img_input, np.ndarray):\n",
        "                # Validate array dimensions for image data compatibility\n",
        "                if img_input.ndim not in [2, 3]:\n",
        "                    raise HistogramError(f\"Invalid {image_label} array dimensions: {img_input.ndim}\")\n",
        "\n",
        "                # Copy input array to prevent mutation of original data\n",
        "                image_array = img_input.copy()\n",
        "\n",
        "                # Record original array properties\n",
        "                processing_metadata.update({\n",
        "                    'original_size': (image_array.shape[1], image_array.shape[0]),\n",
        "                    'original_channels': image_array.shape[2] if image_array.ndim == 3 else 1,\n",
        "                    'array_dtype': str(image_array.dtype),\n",
        "                    'value_range': (float(image_array.min()), float(image_array.max()))\n",
        "                })\n",
        "\n",
        "                # Ensure 3-channel color image for consistent histogram processing\n",
        "                if image_array.ndim == 2:\n",
        "                    # Convert grayscale to 3-channel BGR\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)\n",
        "                    processing_metadata['transformations_applied'].append('grayscale_to_BGR')\n",
        "                elif image_array.shape[2] == 4:\n",
        "                    # Remove alpha channel for OpenCV compatibility\n",
        "                    image_array = cv2.cvtColor(image_array, cv2.COLOR_BGRA2BGR)\n",
        "                    processing_metadata['transformations_applied'].append('BGRA_to_BGR')\n",
        "                elif image_array.shape[2] != 3:\n",
        "                    raise HistogramError(f\"Unsupported channel count: {image_array.shape[2]}\")\n",
        "\n",
        "            else:\n",
        "                raise HistogramError(f\"Unsupported {image_label} type: {type(img_input)}\")\n",
        "\n",
        "            # Apply resizing with aspect ratio preservation for computational efficiency\n",
        "            def _resize_image_with_optimization(img: np.ndarray) -> np.ndarray:\n",
        "                \"\"\"Resize image according to configuration with optimization.\"\"\"\n",
        "                current_height, current_width = img.shape[:2]\n",
        "                target_width, target_height = resize_size\n",
        "\n",
        "                if preserve_aspect:\n",
        "                    # Compute scaling factor preserving aspect ratio\n",
        "                    scale_factor = min(\n",
        "                        target_width / current_width,\n",
        "                        target_height / current_height\n",
        "                    )\n",
        "                    # Compute new dimensions maintaining aspect ratio\n",
        "                    new_width = int(current_width * scale_factor)\n",
        "                    new_height = int(current_height * scale_factor)\n",
        "\n",
        "                    # Apply high-quality resizing using area interpolation for downscaling\n",
        "                    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "                    processing_metadata['optimization_applied'].append('aspect_preserving_resize')\n",
        "                else:\n",
        "                    # Direct resize to target dimensions\n",
        "                    resized_img = cv2.resize(img, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
        "                    processing_metadata['optimization_applied'].append('direct_resize')\n",
        "\n",
        "                # Record resizing transformation\n",
        "                processing_metadata.update({\n",
        "                    'resized_size': (resized_img.shape[1], resized_img.shape[0]),\n",
        "                    'resize_applied': True\n",
        "                })\n",
        "                processing_metadata['transformations_applied'].append('image_resizing')\n",
        "\n",
        "                return resized_img\n",
        "\n",
        "            # Apply resizing to image\n",
        "            resized_image = _resize_image_with_optimization(image_array)\n",
        "\n",
        "            # Process mask if provided with comprehensive validation\n",
        "            processed_mask = None\n",
        "            if mask is not None:\n",
        "                # Validate mask dimensions match original image\n",
        "                if mask.shape[:2] != image_array.shape[:2]:\n",
        "                    raise HistogramError(\n",
        "                        f\"Mask shape {mask.shape} incompatible with {image_label} shape {image_array.shape}\"\n",
        "                    )\n",
        "\n",
        "                # Resize mask to match processed image with nearest neighbor interpolation\n",
        "                resized_mask = cv2.resize(\n",
        "                    mask.astype(np.uint8),\n",
        "                    resized_image.shape[:2][::-1],  # (width, height) format\n",
        "                    interpolation=cv2.INTER_NEAREST\n",
        "                )\n",
        "\n",
        "                # Ensure binary mask values for histogram computation\n",
        "                processed_mask = (resized_mask > 0).astype(np.uint8) * 255\n",
        "\n",
        "                # Validate mask has valid regions\n",
        "                if np.sum(processed_mask) == 0:\n",
        "                    raise HistogramError(f\"Mask for {image_label} contains no valid regions\")\n",
        "\n",
        "                processing_metadata['transformations_applied'].append('mask_processing')\n",
        "                processing_metadata.update({\n",
        "                    'mask_provided': True,\n",
        "                    'mask_coverage_ratio': float(np.sum(processed_mask > 0)) / float(processed_mask.size)\n",
        "                })\n",
        "            else:\n",
        "                processing_metadata['mask_provided'] = False\n",
        "\n",
        "            return resized_image, processed_mask, processing_metadata\n",
        "\n",
        "        # Load and prepare both images with comprehensive preprocessing\n",
        "        preprocessing_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Process first image with comprehensive preprocessing\n",
        "            prepared_image1, processed_mask1, metadata1 = _load_and_prepare_image_for_histogram(\n",
        "                image1, mask1, \"image1\"\n",
        "            )\n",
        "            # Process second image with comprehensive preprocessing\n",
        "            prepared_image2, processed_mask2, metadata2 = _load_and_prepare_image_for_histogram(\n",
        "                image2, mask2, \"image2\"\n",
        "            )\n",
        "\n",
        "            # Record preprocessing performance metrics\n",
        "            preprocessing_time = time.perf_counter() - preprocessing_start\n",
        "            operation_metrics['processing_stages']['preprocessing'] = {\n",
        "                'duration_seconds': preprocessing_time,\n",
        "                'image1_metadata': metadata1,\n",
        "                'image2_metadata': metadata2\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error context for preprocessing failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'preprocessing',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise HistogramError(f\"Image preprocessing failed: {e}\") from e\n",
        "\n",
        "        # Apply color space transformation based on configuration\n",
        "        color_conversion_start = time.perf_counter()\n",
        "\n",
        "        def _convert_color_space_with_validation(img: np.ndarray, label: str) -> np.ndarray:\n",
        "            \"\"\"Convert image to specified color space with validation.\"\"\"\n",
        "            try:\n",
        "                if color_space == \"HSV\":\n",
        "                    # Convert BGR to HSV for perceptually uniform hue representation\n",
        "                    # HSV ranges: H[0,179], S[0,255], V[0,255] in OpenCV\n",
        "                    converted_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "                elif color_space == \"RGB\":\n",
        "                    # Convert BGR to RGB for standard RGB analysis\n",
        "                    converted_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                elif color_space == \"LAB                # Convert BGR to CIELAB for perceptually uniform color space\n",
        "                    # LAB ranges: L[0,100], A[-127,127], B[-127,127] but OpenCV uses [0,255]\n",
        "                    converted_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "                else:\n",
        "                    raise HistogramError(f\"Unsupported color space: {color_space}\")\n",
        "\n",
        "                # Validate conversion success\n",
        "                if converted_img is None or converted_img.shape != img.shape:\n",
        "                    raise HistogramError(f\"Color space conversion failed for {label}\")\n",
        "\n",
        "                return converted_img\n",
        "\n",
        "            except cv2.error as e:\n",
        "                raise HistogramError(f\"OpenCV color conversion failed for {label}: {e}\") from e\n",
        "\n",
        "        try:\n",
        "            # Transform both images to target color space with validation\n",
        "            color_image1 = _convert_color_space_with_validation(prepared_image1, \"image1\")\n",
        "            color_image2 = _convert_color_space_with_validation(prepared_image2, \"image2\")\n",
        "\n",
        "            # Record color conversion performance\n",
        "            color_conversion_time = time.perf_counter() - color_conversion_start\n",
        "            operation_metrics['processing_stages']['color_conversion'] = {\n",
        "                'duration_seconds': color_conversion_time,\n",
        "                'color_space': color_space,\n",
        "                'conversion_successful': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for color conversion failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'color_conversion',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise HistogramError(f\"Color space conversion failed: {e}\") from e\n",
        "\n",
        "        # Define histogram computation parameters based on color space\n",
        "        if color_space == \"HSV\":\n",
        "            # HSV ranges: H[0,179], S[0,255], V[0,255] in OpenCV\n",
        "            hist_ranges = [0, 180, 0, 256, 0, 256]\n",
        "            channels = [0, 1]  # Use H and S channels for robustness to illumination\n",
        "            channel_names = [\"Hue\", \"Saturation\"]\n",
        "        elif color_space in [\"RGB\", \"LAB\"]:\n",
        "            # RGB/LAB ranges: [0,255] for all channels in OpenCV representation\n",
        "            hist_ranges = [0, 256, 0, 256, 0, 256]\n",
        "            channels = [0, 1, 2]  # Use all three channels\n",
        "            channel_names = [\"Channel0\", \"Channel1\", \"Channel2\"]\n",
        "\n",
        "        # Apply adaptive binning if requested based on image characteristics\n",
        "        if adaptive_binning:\n",
        "            adaptive_binning_start = time.perf_counter()\n",
        "\n",
        "            # Analyze image characteristics for optimal binning\n",
        "            def _compute_adaptive_bins(img: np.ndarray) -> List[int]:\n",
        "                \"\"\"Compute adaptive bin counts based on image characteristics.\"\"\"\n",
        "                adaptive_bins = []\n",
        "\n",
        "                for channel_idx in channels:\n",
        "                    # Extract channel data\n",
        "                    channel_data = img[:, :, channel_idx].flatten()\n",
        "\n",
        "                    # Compute channel statistics for adaptive binning\n",
        "                    unique_values = len(np.unique(channel_data))\n",
        "                    data_range = np.ptp(channel_data)  # Peak-to-peak range\n",
        "\n",
        "                    # Use Freedman-Diaconis rule for optimal bin width\n",
        "                    q75, q25 = np.percentile(channel_data, [75, 25])\n",
        "                    iqr = q75 - q25\n",
        "\n",
        "                    if iqr > 0:\n",
        "                        # Freedman-Diaconis bin width: 2 * IQR / n^(1/3)\n",
        "                        bin_width = 2 * iqr / (len(channel_data) ** (1/3))\n",
        "                        optimal_bins = max(int(data_range / bin_width), 10)\n",
        "                    else:\n",
        "                        # Fallback for constant channels\n",
        "                        optimal_bins = min(unique_values, 32)\n",
        "\n",
        "                    # Clamp to reasonable range\n",
        "                    adaptive_bins.append(min(max(optimal_bins, 8), 128))\n",
        "\n",
        "                return adaptive_bins\n",
        "\n",
        "            # Compute adaptive bins for both images and take average\n",
        "            adaptive_bins1 = _compute_adaptive_bins(color_image1)\n",
        "            adaptive_bins2 = _compute_adaptive_bins(color_image2)\n",
        "\n",
        "            # Use average of adaptive bins for consistency\n",
        "            hist_bins = [int((b1 + b2) / 2) for b1, b2 in zip(adaptive_bins1, adaptive_bins2)]\n",
        "\n",
        "            # Record adaptive binning performance\n",
        "            adaptive_binning_time = time.perf_counter() - adaptive_binning_start\n",
        "            operation_metrics['processing_stages']['adaptive_binning'] = {\n",
        "                'duration_seconds': adaptive_binning_time,\n",
        "                'original_bins': bins,\n",
        "                'adaptive_bins': hist_bins,\n",
        "                'bins_image1': adaptive_bins1,\n",
        "                'bins_image2': adaptive_bins2\n",
        "            }\n",
        "\n",
        "        # Compute normalized histograms for both images with comprehensive error handling\n",
        "        histogram_computation_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Image 1: Compute multi-dimensional histogram\n",
        "            histogram1 = cv2.calcHist(\n",
        "                [color_image1],                    # Source image list\n",
        "                channels,                          # Channel indices for computation\n",
        "                processed_mask1,                   # Mask (None = full image)\n",
        "                hist_bins[:len(channels)],         # Bins per channel\n",
        "                hist_ranges                        # Value ranges for each channel\n",
        "            )\n",
        "\n",
        "            # Image 2: Compute multi-dimensional histogram\n",
        "            histogram2 = cv2.calcHist(\n",
        "                [color_image2],                    # Source image list\n",
        "                channels,                          # Channel indices for computation\n",
        "                processed_mask2,                   # Mask (None = full image)\n",
        "                hist_bins[:len(channels)],         # Bins per channel\n",
        "                hist_ranges                        # Value ranges for each channel\n",
        "            )\n",
        "\n",
        "            # Validate histogram computation success\n",
        "            if histogram1 is None or histogram2 is None:\n",
        "                raise HistogramError(\"Histogram computation returned None\")\n",
        "\n",
        "            if histogram1.size == 0 or histogram2.size == 0:\n",
        "                raise HistogramError(\"Histogram computation produced empty histograms\")\n",
        "\n",
        "            # Record histogram computation performance\n",
        "            histogram_computation_time = time.perf_counter() - histogram_computation_start\n",
        "            operation_metrics['processing_stages']['histogram_computation'] = {\n",
        "                'duration_seconds': histogram_computation_time,\n",
        "                'histogram1_shape': histogram1.shape,\n",
        "                'histogram2_shape': histogram2.shape,\n",
        "                'total_bins': int(np.prod(hist_bins[:len(channels)])),\n",
        "                'channels_used': len(channels)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for histogram computation failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'histogram_computation',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise HistogramError(f\"Histogram computation failed: {e}\") from e\n",
        "\n",
        "        # Handle zero histogram cases according to policy with detailed analysis\n",
        "        hist1_sum = float(np.sum(histogram1))\n",
        "        hist2_sum = float(np.sum(histogram2))\n",
        "\n",
        "        # Record histogram statistics for analysis\n",
        "        histogram_stats = {\n",
        "            'histogram1_sum': hist1_sum,\n",
        "            'histogram2_sum': hist2_sum,\n",
        "            'histogram1_nonzero_bins': int(np.count_nonzero(histogram1)),\n",
        "            'histogram2_nonzero_bins': int(np.count_nonzero(histogram2)),\n",
        "            'histogram1_max': float(np.max(histogram1)),\n",
        "            'histogram2_max': float(np.max(histogram2))\n",
        "        }\n",
        "\n",
        "        if hist1_sum == 0 or hist2_sum == 0:\n",
        "            # Handle zero histogram cases with comprehensive error context\n",
        "            zero_histogram_context = {\n",
        "                'histogram1_zero': hist1_sum == 0,\n",
        "                'histogram2_zero': hist2_sum == 0,\n",
        "                'mask1_coverage': float(np.sum(processed_mask1 > 0)) / float(processed_mask1.size) if processed_mask1 is not None else 1.0,\n",
        "                'mask2_coverage': float(np.sum(processed_mask2 > 0)) / float(processed_mask2.size) if processed_mask2 is not None else 1.0\n",
        "            }\n",
        "\n",
        "            if on_zero_histogram == \"error\":\n",
        "                raise HistogramError(\n",
        "                    \"Zero histogram detected - no data to compare\",\n",
        "                    algorithm_context=zero_histogram_context,\n",
        "                    forensic_metadata=ForensicMetadata(\n",
        "                        operation_name=\"histogram_zero_detection\",\n",
        "                        algorithm_parameters=histogram_stats\n",
        "                    )\n",
        "                )\n",
        "            elif on_zero_histogram == \"zero\":\n",
        "                return 0.0 if not statistical_analysis else {\n",
        "                    'similarity_score': 0.0,\n",
        "                    'zero_histogram_detected': True,\n",
        "                    'zero_histogram_context': zero_histogram_context\n",
        "                }\n",
        "            elif on_zero_histogram == \"nan\":\n",
        "                return float('nan') if not statistical_analysis else {\n",
        "                    'similarity_score': float('nan'),\n",
        "                    'zero_histogram_detected': True,\n",
        "                    'zero_histogram_context': zero_histogram_context\n",
        "                }\n",
        "\n",
        "        # Apply L1 normalization to histograms for statistical comparison\n",
        "        # Mathematical formula: Ĥ[i] = H[i] / Σⱼ H[j]\n",
        "        normalization_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Normalize histogram1 to probability distribution\n",
        "            cv2.normalize(histogram1, histogram1, alpha=1.0, beta=0.0, norm_type=cv2.NORM_L1)\n",
        "            # Normalize histogram2 to probability distribution\n",
        "            cv2.normalize(histogram2, histogram2, alpha=1.0, beta=0.0, norm_type=cv2.NORM_L1)\n",
        "\n",
        "            # Validate normalization success\n",
        "            norm1_sum = float(np.sum(histogram1))\n",
        "            norm2_sum = float(np.sum(histogram2))\n",
        "\n",
        "            if not (0.99 <= norm1_sum <= 1.01) or not (0.99 <= norm2_sum <= 1.01):\n",
        "                raise HistogramError(f\"Normalization failed: sums = {norm1_sum}, {norm2_sum}\")\n",
        "\n",
        "            # Record normalization performance\n",
        "            normalization_time = time.perf_counter() - normalization_start\n",
        "            operation_metrics['processing_stages']['normalization'] = {\n",
        "                'duration_seconds': normalization_time,\n",
        "                'normalized_sum1': norm1_sum,\n",
        "                'normalized_sum2': norm2_sum,\n",
        "                'normalization_successful': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for normalization failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'normalization',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise HistogramError(f\"Histogram normalization failed: {e}\") from e\n",
        "\n",
        "        # Compute entropy analysis if requested for information-theoretic measures\n",
        "        entropy_metrics = {}\n",
        "        if entropy_analysis:\n",
        "            entropy_computation_start = time.perf_counter()\n",
        "\n",
        "            try:\n",
        "                # Compute Shannon entropy for both histograms\n",
        "                # Mathematical formula: H(X) = -Σᵢ p(i)log₂(p(i))\n",
        "                def _compute_shannon_entropy(hist: np.ndarray) -> float:\n",
        "                    \"\"\"Compute Shannon entropy of normalized histogram.\"\"\"\n",
        "                    # Flatten histogram and remove zero entries\n",
        "                    prob_dist = hist.flatten()\n",
        "                    prob_dist = prob_dist[prob_dist > 0]\n",
        "\n",
        "                    if len(prob_dist) == 0:\n",
        "                        return 0.0\n",
        "\n",
        "                    # Compute Shannon entropy using base-2 logarithm\n",
        "                    entropy = -np.sum(prob_dist * np.log2(prob_dist))\n",
        "                    return float(entropy)\n",
        "\n",
        "                # Compute entropies for both histograms\n",
        "                entropy1 = _compute_shannon_entropy(histogram1)\n",
        "                entropy2 = _compute_shannon_entropy(histogram2)\n",
        "\n",
        "                # Compute joint entropy for mutual information analysis\n",
        "                joint_hist = np.outer(histogram1.flatten(), histogram2.flatten())\n",
        "                joint_hist = joint_hist / np.sum(joint_hist)  # Normalize joint distribution\n",
        "                joint_entropy = _compute_shannon_entropy(joint_hist)\n",
        "\n",
        "                # Compute mutual information: I(X,Y) = H(X) + H(Y) - H(X,Y)\n",
        "                mutual_information = entropy1 + entropy2 - joint_entropy\n",
        "\n",
        "                # Compute normalized mutual information for scale invariance\n",
        "                if entropy1 > 0 and entropy2 > 0:\n",
        "                    normalized_mutual_info = mutual_information / np.sqrt(entropy1 * entropy2)\n",
        "                else:\n",
        "                    normalized_mutual_info = 0.0\n",
        "\n",
        "                # Store entropy analysis results\n",
        "                entropy_metrics = {\n",
        "                    'entropy_histogram1': entropy1,\n",
        "                    'entropy_histogram2': entropy2,\n",
        "                    'joint_entropy': joint_entropy,\n",
        "                    'mutual_information': mutual_information,\n",
        "                    'normalized_mutual_information': normalized_mutual_info,\n",
        "                    'entropy_difference': abs(entropy1 - entropy2),\n",
        "                    'entropy_ratio': entropy2 / entropy1 if entropy1 > 0 else float('inf')\n",
        "                }\n",
        "\n",
        "                # Record entropy computation performance\n",
        "                entropy_computation_time = time.perf_counter() - entropy_computation_start\n",
        "                operation_metrics['processing_stages']['entropy_analysis'] = {\n",
        "                    'duration_seconds': entropy_computation_time,\n",
        "                    'entropy_metrics': entropy_metrics\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle entropy computation failures gracefully\n",
        "                entropy_metrics = {'error': str(e)}\n",
        "                operation_metrics['processing_stages']['entropy_analysis'] = {\n",
        "                    'error': str(e)\n",
        "                }\n",
        "\n",
        "        # Compute similarity/distance using specified metric with comprehensive validation\n",
        "        comparison_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Apply OpenCV histogram comparison with selected metric\n",
        "            comparison_result = cv2.compareHist(\n",
        "                histogram1,\n",
        "                histogram2,\n",
        "                metric_constants[metric]\n",
        "            )\n",
        "\n",
        "            # Validate comparison result is finite and within expected range\n",
        "            if not np.isfinite(comparison_result):\n",
        "                raise HistogramError(f\"Histogram comparison produced invalid result: {comparison_result}\")\n",
        "\n",
        "            # Apply metric-specific validation\n",
        "            if metric == \"correlation\":\n",
        "                # Correlation should be in [-1, 1] range\n",
        "                if not -1.0 <= comparison_result <= 1.0:\n",
        "                    raise HistogramError(f\"Correlation result {comparison_result} outside [-1,1] range\")\n",
        "            elif metric == \"intersection\":\n",
        "                # Intersection should be in [0, 1] range for normalized histograms\n",
        "                if not 0.0 <= comparison_result <= 1.0:\n",
        "                    raise HistogramError(f\"Intersection result {comparison_result} outside [0,1] range\")\n",
        "\n",
        "            # Record comparison performance\n",
        "            comparison_time = time.perf_counter() - comparison_start\n",
        "            operation_metrics['processing_stages']['comparison'] = {\n",
        "                'duration_seconds': comparison_time,\n",
        "                'metric_used': metric,\n",
        "                'comparison_result': float(comparison_result),\n",
        "                'result_valid': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for comparison failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'comparison',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise HistogramError(f\"Histogram comparison failed for {metric}: {e}\") from e\n",
        "\n",
        "        # Compute statistical properties and confidence intervals if analysis requested\n",
        "        statistical_properties = None\n",
        "        if statistical_analysis:\n",
        "            statistical_analysis_start = time.perf_counter()\n",
        "\n",
        "            try:\n",
        "                # Compute histogram difference for statistical analysis\n",
        "                hist_diff = histogram1 - histogram2\n",
        "                hist_diff_flat = hist_diff.flatten()\n",
        "\n",
        "                # Compute comprehensive statistical properties of histogram differences\n",
        "                statistical_properties = StatisticalProperties(\n",
        "                    sample_size=len(hist_diff_flat),\n",
        "                    mean=float(np.mean(hist_diff_flat)),\n",
        "                    median=float(np.median(hist_diff_flat)),\n",
        "                    variance=float(np.var(hist_diff_flat)),\n",
        "                    standard_deviation=float(np.std(hist_diff_flat)),\n",
        "                    standard_error=float(np.std(hist_diff_flat) / np.sqrt(len(hist_diff_flat))),\n",
        "                    skewness=float(stats.skew(hist_diff_flat)),\n",
        "                    kurtosis=float(stats.kurtosis(hist_diff_flat)),\n",
        "                    confidence_level=0.95,\n",
        "                    distribution_type=\"histogram_difference\"\n",
        "                )\n",
        "\n",
        "                # Compute confidence interval for the comparison metric\n",
        "                if len(hist_diff_flat) > 1:\n",
        "                    # Use bootstrap method for robust confidence interval estimation\n",
        "                    n_bootstrap = 1000\n",
        "                    bootstrap_results = []\n",
        "\n",
        "                    for _ in range(n_bootstrap):\n",
        "                        # Resample histogram bins with replacement\n",
        "                        bootstrap_indices = np.random.choice(len(hist_diff_flat), size=len(hist_diff_flat), replace=True)\n",
        "                        bootstrap_diff = hist_diff_flat[bootstrap_indices]\n",
        "\n",
        "                        # Reconstruct histograms from bootstrap sample\n",
        "                        bootstrap_hist1 = histogram1.flatten()[bootstrap_indices].reshape(histogram1.shape)\n",
        "                        bootstrap_hist2 = histogram2.flatten()[bootstrap_indices].reshape(histogram2.shape)\n",
        "\n",
        "                        # Compute metric for bootstrap sample\n",
        "                        try:\n",
        "                            bootstrap_result = cv2.compareHist(bootstrap_hist1, bootstrap_hist2, metric_constants[metric])\n",
        "                            if np.isfinite(bootstrap_result):\n",
        "                                bootstrap_results.append(bootstrap_result)\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                    # Compute confidence interval from bootstrap distribution\n",
        "                    if len(bootstrap_results) > 10:\n",
        "                        ci_lower = float(np.percentile(bootstrap_results, 2.5))\n",
        "                        ci_upper = float(np.percentile(bootstrap_results, 97.5))\n",
        "                        statistical_properties.confidence_interval_lower = ci_lower\n",
        "                        statistical_properties.confidence_interval_upper = ci_upper\n",
        "\n",
        "                # Record statistical analysis performance\n",
        "                statistical_analysis_time = time.perf_counter() - statistical_analysis_start\n",
        "                operation_metrics['processing_stages']['statistical_analysis'] = {\n",
        "                    'duration_seconds': statistical_analysis_time,\n",
        "                    'bootstrap_samples': len(bootstrap_results) if 'bootstrap_results' in locals() else 0,\n",
        "                    'confidence_interval_computed': statistical_properties.confidence_interval_lower is not None\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle statistical analysis failures gracefully\n",
        "                operation_metrics['processing_stages']['statistical_analysis'] = {\n",
        "                    'error': str(e)\n",
        "                }\n",
        "\n",
        "        # Record comprehensive performance metrics\n",
        "        total_execution_time = time.perf_counter() - method_start_time\n",
        "        memory_after = psutil.Process().memory_info().rss\n",
        "        memory_delta = memory_after - memory_before\n",
        "\n",
        "        operation_metrics.update({\n",
        "            'total_execution_time_seconds': total_execution_time,\n",
        "            'memory_delta_bytes': memory_delta,\n",
        "            'similarity_score': float(comparison_result),\n",
        "            'histogram_statistics': histogram_stats,\n",
        "            'result_type': 'successful_comparison'\n",
        "        })\n",
        "\n",
        "        # Store performance metrics for analysis if monitoring enabled\n",
        "        if performance_monitoring:\n",
        "            self._record_method_performance('histogram_correlation', operation_metrics)\n",
        "\n",
        "        # Return comprehensive result based on analysis level requested\n",
        "        if statistical_analysis:\n",
        "            # Return comprehensive analysis with all computed metrics\n",
        "            comprehensive_result = {\n",
        "                'similarity_score': float(comparison_result),\n",
        "                'metric_used': metric,\n",
        "                'color_space': color_space,\n",
        "                'bins_used': hist_bins[:len(channels)],\n",
        "                'channels_analyzed': channel_names,\n",
        "                'statistical_properties': statistical_properties,\n",
        "                'entropy_metrics': entropy_metrics,\n",
        "                'histogram_statistics': histogram_stats,\n",
        "                'performance_metrics': operation_metrics,\n",
        "                'computation_successful': True\n",
        "            }\n",
        "            return comprehensive_result\n",
        "        else:\n",
        "            # Return simple numerical result\n",
        "            return float(comparison_result)\n",
        "\n",
        "    def clip_embedding_similarity(\n",
        "        self,\n",
        "        image1: Union[str, Path, Image.Image, torch.Tensor, np.ndarray],\n",
        "        image2: Union[str, Path, Image.Image, torch.Tensor, np.ndarray],\n",
        "        use_mixed_precision: bool = False,\n",
        "        batch_processing: bool = False,\n",
        "        statistical_analysis: bool = True,\n",
        "        performance_monitoring: bool = True,\n",
        "        embedding_analysis: bool = True,\n",
        "        device_optimization: bool = True\n",
        "    ) -> Union[float, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Compute CLIP embedding similarity with comprehensive analysis and optimization.\n",
        "\n",
        "        Implements vision transformer-based semantic similarity with rigorous mathematical\n",
        "        validation, embedding space analysis, performance optimization, and enterprise-grade\n",
        "        monitoring for quantitative image similarity assessment.\n",
        "\n",
        "        Mathematical Foundation:\n",
        "        1. Vision Transformer Encoding:\n",
        "          - Patch embedding: x_patch = Flatten(x_img) ∈ ℝ^(P²·C)\n",
        "          - Position encoding: z₀ = [x_class; E·x_patch] + E_pos\n",
        "          - Multi-head attention: Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
        "          - Layer normalization: LN(x) = γ(x-μ)/σ + β\n",
        "\n",
        "        2. Embedding Space Properties:\n",
        "          - Joint embedding: f_v: Images → ℝ^d, f_t: Text → ℝ^d\n",
        "          - L2 normalization: ê = e / ||e||₂ where ||e||₂ = √(Σᵢ eᵢ²)\n",
        "          - Cosine similarity: sim(ê₁,ê₂) = ê₁·ê₂ = Σᵢ ê₁[i]ê₂[i]\n",
        "\n",
        "        3. Statistical Properties:\n",
        "          - Embedding norm: ||e||₂ ≈ 1 after normalization\n",
        "          - Angular distance: d_θ = arccos(sim(ê₁,ê₂)) ∈ [0,π]\n",
        "          - Euclidean distance: d_E = ||ê₁ - ê₂||₂ = √(2(1 - sim(ê₁,ê₂)))\n",
        "\n",
        "        Args:\n",
        "            image1: First image in supported format\n",
        "            image2: Second image in supported format\n",
        "            use_mixed_precision: Whether to use automatic mixed precision for efficiency\n",
        "            batch_processing: Whether to process images in batch for optimization\n",
        "            statistical_analysis: Whether to perform comprehensive embedding analysis\n",
        "            performance_monitoring: Whether to track detailed performance metrics\n",
        "            embedding_analysis: Whether to analyze embedding space properties\n",
        "            device_optimization: Whether to apply device-specific optimizations\n",
        "\n",
        "        Returns:\n",
        "            Cosine similarity [-1,1] or comprehensive analysis dictionary\n",
        "\n",
        "        Raises:\n",
        "            ModelInferenceError: If CLIP inference fails with detailed context\n",
        "            ValueError: If inputs have incompatible formats or shapes\n",
        "            ResourceAllocationError: If insufficient GPU/CPU resources\n",
        "        \"\"\"\n",
        "        # Record method execution start for comprehensive performance monitoring\n",
        "        method_start_time = time.perf_counter()\n",
        "        memory_before = psutil.Process().memory_info().rss\n",
        "        gpu_memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "        # Ensure CLIP model is loaded using thread-safe lazy initialization\n",
        "        self._load_clip_with_comprehensive_monitoring()\n",
        "\n",
        "        # Initialize comprehensive performance tracking with detailed metrics\n",
        "        operation_metrics = {\n",
        "            'method_name': 'clip_embedding_similarity',\n",
        "            'start_time': datetime.datetime.utcnow().isoformat(),\n",
        "            'parameters': {\n",
        "                'use_mixed_precision': use_mixed_precision,\n",
        "                'batch_processing': batch_processing,\n",
        "                'statistical_analysis': statistical_analysis,\n",
        "                'embedding_analysis': embedding_analysis,\n",
        "                'device_optimization': device_optimization\n",
        "            },\n",
        "            'processing_stages': {},\n",
        "            'device_info': {\n",
        "                'target_device': self.device,\n",
        "                'cuda_available': torch.cuda.is_available(),\n",
        "                'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Helper function for comprehensive input preprocessing with validation\n",
        "        def _prepare_input_tensor_with_validation(\n",
        "            img_input: Union[str, Path, Image.Image, torch.Tensor, np.ndarray],\n",
        "            input_label: str\n",
        "        ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Convert various input types to preprocessed CLIP tensor with comprehensive validation.\n",
        "\n",
        "            Args:\n",
        "                img_input: Image input in supported format\n",
        "                input_label: Label for error reporting and performance tracking\n",
        "\n",
        "            Returns:\n",
        "                Tuple of (preprocessed_tensor, processing_metadata)\n",
        "            \"\"\"\n",
        "            # Initialize processing metadata for comprehensive analysis\n",
        "            processing_metadata = {\n",
        "                'input_type': type(img_input).__name__,\n",
        "                'transformations_applied': [],\n",
        "                'validation_performed': True,\n",
        "                'tensor_properties': {}\n",
        "            }\n",
        "\n",
        "            # Handle pre-computed torch.Tensor input with comprehensive validation\n",
        "            if isinstance(img_input, torch.Tensor):\n",
        "                # Clone tensor to avoid mutation of original data\n",
        "                input_tensor = img_input.clone().detach()\n",
        "\n",
        "                # Validate tensor shape for CLIP input requirements\n",
        "                if input_tensor.ndim == 3:\n",
        "                    # Add batch dimension: (C,H,W) -> (1,C,H,W)\n",
        "                    input_tensor = input_tensor.unsqueeze(0)\n",
        "                    processing_metadata['transformations_applied'].append('batch_dimension_added')\n",
        "                elif input_tensor.ndim == 4:\n",
        "                    # Already has batch dimension: (B,C,H,W)\n",
        "                    processing_metadata['transformations_applied'].append('batch_dimension_present')\n",
        "                else:\n",
        "                    raise ValueError(f\"Invalid {input_label} tensor dimensions: {input_tensor.ndim}, expected 3 or 4\")\n",
        "\n",
        "                # Validate channel count for RGB images\n",
        "                if input_tensor.shape[1] != 3:\n",
        "                    raise ValueError(f\"Expected 3 channels for {input_label}, got {input_tensor.shape[1]}\")\n",
        "\n",
        "                # Validate tensor value range for CLIP preprocessing\n",
        "                tensor_min, tensor_max = input_tensor.min().item(), input_tensor.max().item()\n",
        "                if not (-3.0 <= tensor_min <= 3.0 and -3.0 <= tensor_max <= 3.0):\n",
        "                    # Assume unnormalized tensor and apply standard normalization\n",
        "                    input_tensor = input_tensor.float() / 255.0 if tensor_max > 1.0 else input_tensor.float()\n",
        "                    processing_metadata['transformations_applied'].append('value_normalization')\n",
        "\n",
        "                # Record tensor properties\n",
        "                processing_metadata['tensor_properties'] = {\n",
        "                    'shape': list(input_tensor.shape),\n",
        "                    'dtype': str(input_tensor.dtype),\n",
        "                    'device': str(input_tensor.device),\n",
        "                    'value_range': (tensor_min, tensor_max)\n",
        "                }\n",
        "\n",
        "            # Handle numpy array input with comprehensive preprocessing\n",
        "            elif isinstance(img_input, np.ndarray):\n",
        "                # Validate array dimensions for image data compatibility\n",
        "                if img_input.ndim not in [2, 3]:\n",
        "                    raise ValueError(f\"Invalid {input_label} array dimensions: {img_input.ndim}\")\n",
        "\n",
        "                # Record original array properties\n",
        "                processing_metadata['tensor_properties'] = {\n",
        "                    'original_shape': list(img_input.shape),\n",
        "                    'original_dtype': str(img_input.dtype),\n",
        "                    'value_range': (float(img_input.min()), float(img_input.max()))\n",
        "                }\n",
        "\n",
        "                # Handle different array formats and convert to PIL for CLIP preprocessing\n",
        "                if img_input.ndim == 2:\n",
        "                    # Grayscale array - convert to RGB PIL Image\n",
        "                    pil_image = Image.fromarray(img_input).convert('RGB')\n",
        "                    processing_metadata['transformations_applied'].append('grayscale_to_RGB')\n",
        "                elif img_input.ndim == 3:\n",
        "                    # Color array - handle different channel orders and formats\n",
        "                    if img_input.shape[2] == 3:\n",
        "                        # Assume RGB format for PIL compatibility\n",
        "                        if img_input.dtype != np.uint8:\n",
        "                            # Convert to uint8 if necessary\n",
        "                            if img_input.max() <= 1.0:\n",
        "                                img_input = (img_input * 255).astype(np.uint8)\n",
        "                            else:\n",
        "                                img_input = img_input.astype(np.uint8)\n",
        "                            processing_metadata['transformations_applied'].append('dtype_conversion')\n",
        "\n",
        "                        pil_image = Image.fromarray(img_input).convert('RGB')\n",
        "                        processing_metadata['transformations_applied'].append('array_to_PIL_RGB')\n",
        "                    elif img_input.shape[2] == 4:\n",
        "                        # RGBA - remove alpha channel and convert\n",
        "                        rgb_array = img_input[:, :, :3]\n",
        "                        if rgb_array.dtype != np.uint8:\n",
        "                            if rgb_array.max() <= 1.0:\n",
        "                                rgb_array = (rgb_array * 255).astype(np.uint8)\n",
        "                            else:\n",
        "                                rgb_array = rgb_array.astype(np.uint8)\n",
        "                        pil_image = Image.fromarray(rgb_array).convert('RGB')\n",
        "                        processing_metadata['transformations_applied'].append('RGBA_to_RGB')\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unsupported {input_label} channel count: {img_input.shape[2]}\")\n",
        "\n",
        "                # Apply CLIP preprocessing pipeline to PIL image\n",
        "                input_tensor = self.clip_preprocess(pil_image)\n",
        "                processing_metadata['transformations_applied'].append('CLIP_preprocessing')\n",
        "\n",
        "                # Add batch dimension for inference\n",
        "                input_tensor = input_tensor.unsqueeze(0)\n",
        "                processing_metadata['transformations_applied'].append('batch_dimension_added')\n",
        "\n",
        "            # Handle PIL Image input with validation and preprocessing\n",
        "            elif isinstance(img_input, Image.Image):\n",
        "                # Record original PIL image properties\n",
        "                processing_metadata['tensor_properties'] = {\n",
        "                    'original_format': img_input.format,\n",
        "                    'original_mode': img_input.mode,\n",
        "                    'original_size': img_input.size\n",
        "                }\n",
        "\n",
        "                # Ensure RGB format for CLIP compatibility\n",
        "                rgb_image = img_input.convert('RGB')\n",
        "                processing_metadata['transformations_applied'].append('PIL_to_RGB')\n",
        "\n",
        "                # Apply CLIP preprocessing (resize, normalize, tensorize)\n",
        "                input_tensor = self.clip_preprocess(rgb_image)\n",
        "                processing_metadata['transformations_applied'].append('CLIP_preprocessing')\n",
        "\n",
        "                # Add batch dimension for model input\n",
        "                input_tensor = input_tensor.unsqueeze(0)\n",
        "                processing_metadata['transformations_applied'].append('batch_dimension_added')\n",
        "\n",
        "            # Handle file path input with comprehensive validation and loading\n",
        "            elif isinstance(img_input, (str, Path)):\n",
        "                # Validate image file accessibility using enhanced validation\n",
        "                validated_path = self._validate_image_path(\n",
        "                    img_input,\n",
        "                    self._allow_symlinks,\n",
        "                    perform_content_validation=True,\n",
        "                    max_file_size_mb=50.0  # Reasonable limit for CLIP processing\n",
        "                )\n",
        "\n",
        "                # Record path information for metadata\n",
        "                processing_metadata['tensor_properties'] = {\n",
        "                    'file_path': str(validated_path),\n",
        "                    'file_size_bytes': validated_path.stat().st_size\n",
        "                }\n",
        "\n",
        "                # Load image using PIL with comprehensive error handling\n",
        "                try:\n",
        "                    pil_image = Image.open(validated_path).convert('RGB')\n",
        "                    processing_metadata['transformations_applied'].append('file_to_PIL_RGB')\n",
        "\n",
        "                    # Record loaded image properties\n",
        "                    processing_metadata['tensor_properties'].update({\n",
        "                        'loaded_format': pil_image.format,\n",
        "                        'loaded_mode': pil_image.mode,\n",
        "                        'loaded_size': pil_image.size\n",
        "                    })\n",
        "\n",
        "                except (IOError, OSError) as e:\n",
        "                    # Enhanced error reporting with file analysis\n",
        "                    raise ModelInferenceError(\n",
        "                        f\"Failed to load {input_label} from {validated_path}: {e}\",\n",
        "                        model_name=\"CLIP\",\n",
        "                        algorithm_context={'file_path': str(validated_path), 'input_label': input_label},\n",
        "                        forensic_metadata=ForensicMetadata(\n",
        "                            operation_name=\"clip_image_loading\",\n",
        "                            algorithm_parameters={'input_label': input_label}\n",
        "                        )\n",
        "                    ) from e\n",
        "\n",
        "                # Apply CLIP preprocessing pipeline\n",
        "                input_tensor = self.clip_preprocess(pil_image)\n",
        "                processing_metadata['transformations_applied'].append('CLIP_preprocessing')\n",
        "\n",
        "                # Add batch dimension\n",
        "                input_tensor = input_tensor.unsqueeze(0)\n",
        "                processing_metadata['transformations_applied'].append('batch_dimension_added')\n",
        "\n",
        "            else:\n",
        "                # Unsupported input type with comprehensive error context\n",
        "                raise ValueError(\n",
        "                    f\"Unsupported {input_label} input type: {type(img_input)}. \"\n",
        "                    f\"Supported types: str, Path, PIL.Image, torch.Tensor, np.ndarray\"\n",
        "                )\n",
        "\n",
        "            # Move tensor to appropriate device for inference with optimization\n",
        "            if device_optimization:\n",
        "                # Apply device-specific optimizations\n",
        "                if self.device.startswith('cuda') and torch.cuda.is_available():\n",
        "                    # Move to GPU with memory optimization\n",
        "                    input_tensor = input_tensor.to(self.device, non_blocking=True)\n",
        "                    processing_metadata['transformations_applied'].append('GPU_transfer_optimized')\n",
        "                else:\n",
        "                    # Move to CPU\n",
        "                    input_tensor = input_tensor.to(self.device)\n",
        "                    processing_metadata['transformations_applied'].append('CPU_transfer')\n",
        "            else:\n",
        "                # Standard device transfer\n",
        "                input_tensor = input_tensor.to(self.device)\n",
        "                processing_metadata['transformations_applied'].append('device_transfer')\n",
        "\n",
        "            # Record final tensor properties\n",
        "            processing_metadata['tensor_properties'].update({\n",
        "                'final_shape': list(input_tensor.shape),\n",
        "                'final_dtype': str(input_tensor.dtype),\n",
        "                'final_device': str(input_tensor.device),\n",
        "                'memory_usage_bytes': input_tensor.numel() * input_tensor.element_size()\n",
        "            })\n",
        "\n",
        "            return input_tensor, processing_metadata\n",
        "\n",
        "        # Prepare both input images as CLIP-compatible tensors with comprehensive validation\n",
        "        preprocessing_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Process first image with detailed metadata collection\n",
        "            tensor1, metadata1 = _prepare_input_tensor_with_validation(image1, \"image1\")\n",
        "            # Process second image with detailed metadata collection\n",
        "            tensor2, metadata2 = _prepare_input_tensor_with_validation(image2, \"image2\")\n",
        "\n",
        "            # Record preprocessing performance metrics\n",
        "            preprocessing_time = time.perf_counter() - preprocessing_start\n",
        "            operation_metrics['processing_stages']['preprocessing'] = {\n",
        "                'duration_seconds': preprocessing_time,\n",
        "                'image1_metadata': metadata1,\n",
        "                'image2_metadata': metadata2,\n",
        "                'total_memory_usage_bytes': metadata1['tensor_properties']['memory_usage_bytes'] +\n",
        "                                          metadata2['tensor_properties']['memory_usage_bytes']\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error context for preprocessing failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'preprocessing',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise ModelInferenceError(f\"CLIP input preparation failed: {e}\") from e\n",
        "\n",
        "        # Perform CLIP inference with comprehensive error handling and optimization\n",
        "        inference_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Configure mixed precision inference if requested and supported\n",
        "            if use_mixed_precision and torch.cuda.is_available():\n",
        "                # Use autocast context for automatic mixed precision\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    with torch.no_grad():  # Disable gradient computation for inference efficiency\n",
        "                        if batch_processing:\n",
        "                            # Process both images in single batch for computational efficiency\n",
        "                            batch_tensor = torch.cat([tensor1, tensor2], dim=0)\n",
        "                            batch_embeddings = self.clip_model.encode_image(batch_tensor)\n",
        "                            embedding1, embedding2 = batch_embeddings[0:1], batch_embeddings[1:2]\n",
        "                            operation_metrics['processing_stages']['inference'] = {\n",
        "                                'batch_processing_used': True,\n",
        "                                'mixed_precision_used': True\n",
        "                            }\n",
        "                        else:\n",
        "                            # Process images individually with mixed precision\n",
        "                            embedding1 = self.clip_model.encode_image(tensor1)\n",
        "                            embedding2 = self.clip_model.encode_image(tensor2)\n",
        "                            operation_metrics['processing_stages']['inference'] = {\n",
        "                                'batch_processing_used': False,\n",
        "                                'mixed_precision_used': True\n",
        "                            }\n",
        "            else:\n",
        "                # Standard precision inference\n",
        "                with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "                    if batch_processing:\n",
        "                        # Batch processing for computational efficiency\n",
        "                        batch_tensor = torch.cat([tensor1, tensor2], dim=0)\n",
        "                        batch_embeddings = self.clip_model.encode_image(batch_tensor)\n",
        "                        embedding1, embedding2 = batch_embeddings[0:1], batch_embeddings[1:2]\n",
        "                        operation_metrics['processing_stages']['inference'] = {\n",
        "                            'batch_processing_used': True,\n",
        "                            'mixed_precision_used': False\n",
        "                        }\n",
        "                    else:\n",
        "                        # Individual image processing\n",
        "                        embedding1 = self.clip_model.encode_image(tensor1)\n",
        "                        embedding2 = self.clip_model.encode_image(tensor2)\n",
        "                        operation_metrics['processing_stages']['inference'] = {\n",
        "                            'batch_processing_used': False,\n",
        "                            'mixed_precision_used': False\n",
        "                        }\n",
        "\n",
        "            # Record inference performance metrics\n",
        "            inference_time = time.perf_counter() - inference_start\n",
        "            operation_metrics['processing_stages']['inference'].update({\n",
        "                'duration_seconds': inference_time,\n",
        "                'embedding1_shape': list(embedding1.shape),\n",
        "                'embedding2_shape': list(embedding2.shape),\n",
        "                'inference_successful': True\n",
        "            })\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError as e:\n",
        "            # Handle GPU memory exhaustion with automatic cleanup and CPU fallback\n",
        "            operation_metrics['processing_stages']['inference'] = {\n",
        "                'error_type': 'OutOfMemoryError',\n",
        "                'error_message': str(e),\n",
        "                'gpu_memory_info': self._get_gpu_memory_info()\n",
        "            }\n",
        "\n",
        "            # Perform aggressive GPU memory cleanup\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            # Attempt CPU fallback if currently using CUDA\n",
        "            if self.device != \"cpu\":\n",
        "                try:\n",
        "                    # Move model and tensors to CPU for fallback processing\n",
        "                    self.clip_model = self.clip_model.cpu()\n",
        "                    tensor1 = tensor1.cpu()\n",
        "                    tensor2 = tensor2.cpu()\n",
        "                    self.device = \"cpu\"\n",
        "\n",
        "                    # Retry inference on CPU\n",
        "                    with torch.no_grad():\n",
        "                        embedding1 = self.clip_model.encode_image(tensor1)\n",
        "                        embedding2 = self.clip_model.encode_image(tensor2)\n",
        "\n",
        "                    # Record successful CPU fallback\n",
        "                    operation_metrics['processing_stages']['inference'].update({\n",
        "                        'cpu_fallback_successful': True,\n",
        "                        'original_device_failed': True\n",
        "                    })\n",
        "\n",
        "                except Exception as fallback_error:\n",
        "                    # CPU fallback also failed\n",
        "                    raise ModelInferenceError(\n",
        "                        f\"CLIP inference failed on both GPU and CPU: GPU OOM: {e}, CPU: {fallback_error}\",\n",
        "                        model_name=\"CLIP\",\n",
        "                        algorithm_context=operation_metrics['processing_stages']['inference']\n",
        "                    ) from e\n",
        "            else:\n",
        "                # Already on CPU - re-raise OOM error\n",
        "                raise ModelInferenceError(\n",
        "                    f\"CPU OOM during CLIP inference: {e}\",\n",
        "                    model_name=\"CLIP\",\n",
        "                    algorithm_context=operation_metrics['processing_stages']['inference']\n",
        "                ) from e\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            # Handle other PyTorch runtime errors with comprehensive context\n",
        "            operation_metrics['processing_stages']['inference'] = {\n",
        "                'error_type': 'RuntimeError',\n",
        "                'error_message': str(e),\n",
        "                'device_info': operation_metrics['device_info']\n",
        "            }\n",
        "            raise ModelInferenceError(f\"CLIP inference runtime error: {e}\") from e\n",
        "\n",
        "        # Validate embedding shapes for compatibility and mathematical consistency\n",
        "        if embedding1.shape != embedding2.shape:\n",
        "            raise ValueError(\n",
        "                f\"Embedding shape mismatch: {embedding1.shape} vs {embedding2.shape}\"\n",
        "            )\n",
        "\n",
        "        # Perform comprehensive embedding analysis if requested\n",
        "        embedding_properties = {}\n",
        "        if embedding_analysis:\n",
        "            embedding_analysis_start = time.perf_counter()\n",
        "\n",
        "            try:\n",
        "                # Analyze embedding properties before normalization\n",
        "                embedding_properties = {\n",
        "                    'embedding_dimension': embedding1.shape[-1],\n",
        "                    'embedding1_norm_before': float(torch.norm(embedding1).item()),\n",
        "                    'embedding2_norm_before': float(torch.norm(embedding2).item()),\n",
        "                    'embedding1_mean': float(torch.mean(embedding1).item()),\n",
        "                    'embedding2_mean': float(torch.mean(embedding2).item()),\n",
        "                    'embedding1_std': float(torch.std(embedding1).item()),\n",
        "                    'embedding2_std': float(torch.std(embedding2).item()),\n",
        "                    'embedding1_min': float(torch.min(embedding1).item()),\n",
        "                    'embedding1_max': float(torch.max(embedding1).item()),\n",
        "                    'embedding2_min': float(torch.min(embedding2).item()),\n",
        "                    'embedding2_max': float(torch.max(embedding2).item())\n",
        "                }\n",
        "\n",
        "                # Compute embedding space distances before normalization\n",
        "                euclidean_distance_raw = float(torch.norm(embedding1 - embedding2).item())\n",
        "                embedding_properties['euclidean_distance_raw'] = euclidean_distance_raw\n",
        "\n",
        "                # Record embedding analysis performance\n",
        "                embedding_analysis_time = time.perf_counter() - embedding_analysis_start\n",
        "                operation_metrics['processing_stages']['embedding_analysis'] = {\n",
        "                    'duration_seconds': embedding_analysis_time,\n",
        "                    'properties_computed': len(embedding_properties)\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle embedding analysis failures gracefully\n",
        "                embedding_properties = {'analysis_error': str(e)}\n",
        "                operation_metrics['processing_stages']['embedding_analysis'] = {\n",
        "                    'error': str(e)\n",
        "                }\n",
        "\n",
        "        # Apply L2 normalization to embeddings for cosine similarity computation\n",
        "        # Mathematical formula: ê = e / ||e||₂ where ||e||₂ = √(Σᵢ eᵢ²)\n",
        "        normalization_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Compute L2 norms for validation\n",
        "            norm1 = torch.norm(embedding1, dim=-1, keepdim=True)\n",
        "            norm2 = torch.norm(embedding2, dim=-1, keepdim=True)\n",
        "\n",
        "            # Validate norms are non-zero for safe normalization\n",
        "            if norm1.item() == 0.0 or norm2.item() == 0.0:\n",
        "                raise ModelInferenceError(\"Zero-norm embedding detected - invalid CLIP output\")\n",
        "\n",
        "            # Apply L2 normalization: ê = e / ||e||₂\n",
        "            normalized_embedding1 = embedding1 / norm1\n",
        "            normalized_embedding2 = embedding2 / norm2\n",
        "\n",
        "            # Validate normalization success\n",
        "            norm1_after = float(torch.norm(normalized_embedding1).item())\n",
        "            norm2_after = float(torch.norm(normalized_embedding2).item())\n",
        "\n",
        "            if not (0.99 <= norm1_after <= 1.01) or not (0.99 <= norm2_after <= 1.01):\n",
        "                raise ModelInferenceError(f\"Normalization failed: norms = {norm1_after}, {norm2_after}\")\n",
        "\n",
        "            # Record normalization performance and results\n",
        "            normalization_time = time.perf_counter() - normalization_start\n",
        "            operation_metrics['processing_stages']['normalization'] = {\n",
        "                'duration_seconds': normalization_time,\n",
        "                'norm1_before': float(norm1.item()),\n",
        "                'norm2_before': float(norm2.item()),\n",
        "                'norm1_after': norm1_after,\n",
        "                'norm2_after': norm2_after,\n",
        "                'normalization_successful': True\n",
        "            }\n",
        "\n",
        "            # Update embedding properties with normalized values\n",
        "            if embedding_analysis:\n",
        "                embedding_properties.update({\n",
        "                    'embedding1_norm_after': norm1_after,\n",
        "                    'embedding2_norm_after': norm2_after,\n",
        "                    'normalization_applied': True\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for normalization failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'normalization',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise ModelInferenceError(f\"Embedding normalization failed: {e}\") from e\n",
        "\n",
        "        # Compute cosine similarity via dot product of normalized embeddings\n",
        "        # Mathematical formula: sim(ê₁,ê₂) = ê₁ · ê₂ = Σᵢ ê₁[i]ê₂[i]\n",
        "        similarity_computation_start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            # Compute cosine similarity using matrix multiplication\n",
        "            cosine_similarity = torch.matmul(normalized_embedding1, normalized_embedding2.T)\n",
        "\n",
        "            # Extract scalar similarity value and convert to Python float\n",
        "            similarity_score = float(cosine_similarity.squeeze().item())\n",
        "\n",
        "            # Validate similarity score is within mathematical bounds [-1, 1]\n",
        "            if not -1.0 <= similarity_score <= 1.0:\n",
        "                raise ModelInferenceError(f\"Invalid cosine similarity {similarity_score}, expected [-1,1]\")\n",
        "\n",
        "            # Record similarity computation performance\n",
        "            similarity_computation_time = time.perf_counter() - similarity_computation_start\n",
        "            operation_metrics['processing_stages']['similarity_computation'] = {\n",
        "                'duration_seconds': similarity_computation_time,\n",
        "                'cosine_similarity': similarity_score,\n",
        "                'similarity_valid': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Enhanced error reporting for similarity computation failures\n",
        "            operation_metrics['error'] = {\n",
        "                'stage': 'similarity_computation',\n",
        "                'error_type': type(e).__name__,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            raise ModelInferenceError(f\"Cosine similarity computation failed: {e}\") from e\n",
        "\n",
        "        # Compute additional similarity metrics and statistical properties if analysis requested\n",
        "        additional_metrics = {}\n",
        "        if statistical_analysis:\n",
        "            statistical_analysis_start = time.perf_counter()\n",
        "\n",
        "            try:\n",
        "                # Compute angular distance: d_θ = arccos(sim(ê₁,ê₂))\n",
        "                angular_distance = float(torch.acos(torch.clamp(cosine_similarity, -1.0, 1.0)).item())\n",
        "\n",
        "                # Compute Euclidean distance in normalized space: d_E = ||ê₁ - ê₂||₂\n",
        "                euclidean_distance_normalized = float(torch.norm(normalized_embedding1 - normalized_embedding2).item())\n",
        "\n",
        "                # Verify mathematical relationship: d_E = √(2(1 - cos_sim))\n",
        "                expected_euclidean = float(torch.sqrt(2 * (1 - cosine_similarity)).item())\n",
        "                euclidean_error = abs(euclidean_distance_normalized - expected_euclidean)\n",
        "\n",
        "                # Compute Manhattan distance in embedding space\n",
        "                manhattan_distance = float(torch.sum(torch.abs(normalized_embedding1 - normalized_embedding2)).item())\n",
        "\n",
        "                # Compute embedding space statistics\n",
        "                embedding_difference = normalized_embedding1 - normalized_embedding2\n",
        "                embedding_difference_flat = embedding_difference.flatten()\n",
        "\n",
        "                # Compute comprehensive statistical properties of embedding differences\n",
        "                additional_metrics = {\n",
        "                    'angular_distance_radians': angular_distance,\n",
        "                    'angular_distance_degrees': float(np.degrees(angular_distance)),\n",
        "                    'euclidean_distance_normalized': euclidean_distance_normalized,\n",
        "                    'euclidean_distance_expected': expected_euclidean,\n",
        "                    'euclidean_computation_error': euclidean_error,\n",
        "                    'manhattan_distance': manhattan_distance,\n",
        "                    'embedding_difference_mean': float(torch.mean(embedding_difference_flat).item()),\n",
        "                    'embedding_difference_std': float(torch.std(embedding_difference_flat).item()),\n",
        "                    'embedding_difference_max': float(torch.max(torch.abs(embedding_difference_flat)).item()),\n",
        "                    'mathematical_consistency_verified': euclidean_error < 1e-6\n",
        "                }\n",
        "\n",
        "                # Compute confidence interval for cosine similarity using Fisher z-transformation\n",
        "                if embedding1.shape[-1] > 10:  # Sufficient dimensionality for statistical analysis\n",
        "                    # Fisher z-transformation: z = 0.5 * ln((1+r)/(1-r))\n",
        "                    z_score = 0.5 * np.log((1 + similarity_score) / (1 - similarity_score))\n",
        "\n",
        "                    # Standard error for correlation coefficient\n",
        "                    n_dims = embedding1.shape[-1]\n",
        "                    se_z = 1.0 / np.sqrt(n_dims - 3)\n",
        "\n",
        "                    # 95% confidence interval in z-space\n",
        "                    z_ci_lower = z_score - 1.96 * se_z\n",
        "                    z_ci_upper = z_score + 1.96 * se_z\n",
        "\n",
        "                    # Transform back to correlation space\n",
        "                    r_ci_lower = (np.exp(2 * z_ci_lower) - 1) / (np.exp(2 * z_ci_lower) + 1)\n",
        "                    r_ci_upper = (np.exp(2 * z_ci_upper) - 1) / (np.exp(2 * z_ci_upper) + 1)\n",
        "\n",
        "                    additional_metrics.update({\n",
        "                        'fisher_z_score': z_score,\n",
        "                        'confidence_interval_lower': float(r_ci_lower),\n",
        "                        'confidence_interval_upper': float(r_ci_upper),\n",
        "                        'confidence_interval_width': float(r_ci_upper - r_ci_lower)\n",
        "                    })\n",
        "\n",
        "                # Record statistical analysis performance\n",
        "                statistical_analysis_time = time.perf_counter() - statistical_analysis_start\n",
        "                operation_metrics['processing_stages']['statistical_analysis'] = {\n",
        "                    'duration_seconds': statistical_analysis_time,\n",
        "                    'metrics_computed': len(additional_metrics),\n",
        "                    'confidence_interval_computed': 'confidence_interval_lower' in additional_metrics\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle statistical analysis failures gracefully\n",
        "                additional_metrics = {'statistical_analysis_error': str(e)}\n",
        "                operation_metrics['processing_stages']['statistical_analysis'] = {\n",
        "                    'error': str(e)\n",
        "                }\n",
        "\n",
        "        # Record comprehensive performance metrics\n",
        "        total_execution_time = time.perf_counter() - method_start_time\n",
        "        memory_after = psutil.Process().memory_info().rss\n",
        "        gpu_memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "        memory_delta = memory_after - memory_before\n",
        "        gpu_memory_delta = gpu_memory_after - gpu_memory_before\n",
        "\n",
        "        operation_metrics.update({\n",
        "            'total_execution_time_seconds': total_execution_time,\n",
        "            'memory_delta_bytes': memory_delta,\n",
        "            'gpu_memory_delta_bytes': gpu_memory_delta,\n",
        "            'cosine_similarity': similarity_score,\n",
        "            'result_type': 'successful_similarity_computation'\n",
        "        })\n",
        "\n",
        "        # Store performance metrics for analysis if monitoring enabled\n",
        "        if performance_monitoring:\n",
        "            self._record_method_performance('clip_embedding_similarity', operation_metrics)\n",
        "\n",
        "        # Return comprehensive result based on analysis level requested\n",
        "        if statistical_analysis or embedding_analysis:\n",
        "            # Return comprehensive analysis with all computed metrics\n",
        "            comprehensive_result = {\n",
        "                'cosine_similarity': similarity_score,\n",
        "                'embedding_properties': embedding_properties,\n",
        "                'additional_metrics': additional_metrics,\n",
        "                'performance_metrics': operation_metrics,\n",
        "                'model_info': {\n",
        "                    'model_name': self._clip_model_name,\n",
        "                    'device_used': self.device,\n",
        "                    'mixed_precision_used': use_mixed_precision,\n",
        "                    'batch_processing_used': batch_processing\n",
        "                },\n",
        "                'computation_successful': True\n",
        "            }\n",
        "            return comprehensive_result\n",
        "        else:\n",
        "            # Return simple cosine similarity score\n",
        "            return similarity_score\n",
        "\n",
        "    def reverse_image_search_google(\n",
        "        self,\n",
        "        image_path: Union[str, Path],\n",
        "        driver_path: Union[str, Path],\n",
        "        timeout: float = 15.0,\n",
        "        headless: bool = False,\n",
        "        max_similar_urls: int = 10,\n",
        "        retry_attempts: int = 3,\n",
        "        performance_monitoring: bool = True,\n",
        "        content_analysis: bool = True,\n",
        "        result_validation: bool = True,\n",
        "        advanced_extraction: bool = True\n",
        "    ) -> ReverseImageSearchResult:\n",
        "        \"\"\"\n",
        "        Perform comprehensive Google reverse image search with enterprise-grade automation.\n",
        "\n",
        "        Implements robust web automation with multiple fallback strategies, comprehensive\n",
        "        result extraction, content analysis, and enterprise-grade error handling for\n",
        "        quantitative image provenance and context determination.\n",
        "\n",
        "        Automation Strategy:\n",
        "        1. Multi-selector fallback hierarchy for UI element location\n",
        "        2. Comprehensive error recovery with exponential backoff\n",
        "        3. Advanced result extraction with content quality assessment\n",
        "        4. Statistical confidence scoring based on result characteristics\n",
        "        5. Geographic and temporal distribution analysis\n",
        "\n",
        "        Args:\n",
        "            image_path: Path to local image file for reverse search\n",
        "            driver_path: Path to ChromeDriver executable\n",
        "            timeout: Maximum wait time for page elements (seconds)\n",
        "            headless: Whether to run browser in headless mode\n",
        "            max_similar_urls: Maximum number of similar image URLs to extract\n",
        "            retry_attempts: Number of retry attempts for failed operations\n",
        "            performance_monitoring: Whether to track detailed performance metrics\n",
        "            content_analysis: Whether to perform content quality analysis\n",
        "            result_validation: Whether to validate extracted results\n",
        "            advanced_extraction: Whether to extract additional metadata\n",
        "\n",
        "        Returns:\n",
        "            ReverseImageSearchResult with comprehensive search findings and analysis\n",
        "\n",
        "        Raises:\n",
        "            LaunchError: If ChromeDriver initialization fails with detailed context\n",
        "            NavigationError: If page navigation fails with network analysis\n",
        "            UploadError: If image upload fails with file analysis\n",
        "            ExtractionError: If result extraction fails with DOM analysis\n",
        "        \"\"\"\n",
        "        # Record method execution start for comprehensive performance monitoring\n",
        "        method_start_time = time.perf_counter()\n",
        "        memory_before = psutil.Process().memory_info().rss\n",
        "\n",
        "        # Validate image file accessibility using comprehensive path validation\n",
        "        validated_image_path = self._validate_image_path(\n",
        "            image_path,\n",
        "            self._allow_symlinks,\n",
        "            perform_content_validation=True,\n",
        "            max_file_size_mb=20.0  # Reasonable limit for web upload\n",
        "        )\n",
        "\n",
        "        # Validate ChromeDriver executable accessibility with comprehensive checks\n",
        "        driver_path_obj = Path(driver_path)\n",
        "        if not driver_path_obj.exists() or not driver_path_obj.is_file():\n",
        "            raise LaunchError(\n",
        "                f\"ChromeDriver not found: {driver_path_obj}\",\n",
        "                driver_path=driver_path_obj,\n",
        "                forensic_metadata=ForensicMetadata(\n",
        "                    operation_name=\"chromedriver_validation\",\n",
        "                    algorithm_parameters={'driver_path': str(driver_path_obj)}\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Verify ChromeDriver has execution permissions\n",
        "        if not os.access(driver_path_obj, os.X_OK):\n",
        "            raise LaunchError(\n",
        "                f\"ChromeDriver not executable: {driver_path_obj}\",\n",
        "                driver_path=driver_path_obj,\n",
        "                driver_version=None,\n",
        "                browser_version=None\n",
        "            )\n",
        "\n",
        "        # Initialize comprehensive performance tracking with detailed metrics\n",
        "        operation_metrics = {\n",
        "            'method_name': 'reverse_image_search_google',\n",
        "            'start_time': datetime.datetime.utcnow().isoformat(),\n",
        "            'parameters': {\n",
        "                'image_path': str(validated_image_path),\n",
        "                'timeout': timeout,\n",
        "                'headless': headless,\n",
        "                'max_similar_urls': max_similar_urls,\n",
        "                'retry_attempts': retry_attempts,\n",
        "                'content_analysis': content_analysis,\n",
        "                'advanced_extraction': advanced_extraction\n",
        "            },\n",
        "            'processing_stages': {},\n",
        "            'automation_events': []\n",
        "        }\n",
        "\n",
        "        # Configure Chrome options for robust automation with enterprise-grade settings\n",
        "        chrome_options = Options()\n",
        "\n",
        "        # Essential options for automation stability and security\n",
        "        chrome_options.add_argument(\"--no-sandbox\")                    # Bypass OS security model for automation\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")         # Overcome limited resource problems\n",
        "        chrome_options.add_argument(\"--disable-gpu\")                   # Disable GPU for stability\n",
        "        chrome_options.add_argument(\"--disable-extensions\")            # Disable extensions for speed and security\n",
        "        chrome_options.add_argument(\"--disable-plugins\")               # Disable plugins for security\n",
        "        chrome_options.add_argument(\"--disable-web-security\")          # Disable web security for automation\n",
        "        chrome_options.add_argument(\"--allow-running-insecure-content\") # Allow mixed content\n",
        "        chrome_options.add_argument(\"--disable-features=VizDisplayCompositor\") # Stability improvement\n",
        "\n",
        "        # Performance optimization options\n",
        "        chrome_options.add_argument(\"--disable-background-timer-throttling\")\n",
        "        chrome_options.add_argument(\"--disable-backgrounding-occluded-windows\")\n",
        "        chrome_options.add_argument(\"--disable-renderer-backgrounding\")\n",
        "\n",
        "        # Window management for consistent DOM rendering\n",
        "        if headless:\n",
        "            chrome_options.add_argument(\"--headless\")                  # Run in headless mode\n",
        "            chrome_options.add_argument(\"--window-size=1920,1080\")     # Set window size for headless\n",
        "            chrome_options.add_argument(\"--disable-logging\")           # Reduce log output\n",
        "        else:\n",
        "            chrome_options.add_argument(\"--start-maximized\")           # Maximize window for visibility\n",
        "\n",
        "        # User agent configuration to avoid bot detection\n",
        "        chrome_options.add_argument(\n",
        "            \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "            \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "        )\n",
        "\n",
        "        # Configure WebDriver service with comprehensive error handling\n",
        "        try:\n",
        "            webdriver_service = Service(str(driver_path_obj))\n",
        "            webdriver_service.start()  # Pre-start service for validation\n",
        "\n",
        "            # Record service initialization success\n",
        "            operation_metrics['processing_stages']['service_initialization'] = {\n",
        "                'driver_path': str(driver_path_obj),\n",
        "                'service_started': True,\n",
        "                'initialization_successful': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            raise LaunchError(\n",
        "                f\"WebDriver service configuration failed: {e}\",\n",
        "                driver_path=driver_path_obj,\n",
        "                forensic_metadata=ForensicMetadata(\n",
        "                    operation_name=\"webdriver_service_initialization\",\n",
        "                    algorithm_parameters={'error_details': str(e)}\n",
        "                )\n",
        "            ) from e\n",
        "\n",
        "        # Initialize WebDriver with comprehensive error handling and retry logic\n",
        "        driver = None\n",
        "        driver_initialization_start = time.perf_counter()\n",
        "\n",
        "        for attempt in range(retry_attempts):\n",
        "            try:\n",
        "                # Attempt Chrome WebDriver initialization\n",
        "                driver = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
        "\n",
        "                # Configure timeouts for robust automation\n",
        "                driver.implicitly_wait(timeout)\n",
        "                driver.set_page_load_timeout(timeout * 2)\n",
        "                driver.set_script_timeout(timeout)\n",
        "\n",
        "                # Validate driver functionality with basic test\n",
        "                driver.get(\"data:text/html,<html><body>Test</body></html>\")\n",
        "                if \"Test\" not in driver.page_source:\n",
        "                    raise LaunchError(\"Driver functionality test failed\")\n",
        "\n",
        "                # Record successful driver initialization\n",
        "                driver_initialization_time = time.perf_counter() - driver_initialization_start\n",
        "                operation_metrics['processing_stages']['driver_initialization'] = {\n",
        "                    'duration_seconds': driver_initialization_time,\n",
        "                    'attempt_number': attempt + 1,\n",
        "                    'initialization_successful': True,\n",
        "                    'browser_version': driver.capabilities.get('browserVersion', 'unknown'),\n",
        "                    'driver_version': driver.capabilities.get('chrome', {}).get('chromedriverVersion', 'unknown')\n",
        "                }\n",
        "                break\n",
        "\n",
        "            except WebDriverException as e:\n",
        "                # Record failed attempt\n",
        "                operation_metrics['automation_events'].append({\n",
        "                    'event_type': 'driver_initialization_failed',\n",
        "                    'attempt': attempt + 1,\n",
        "                    'error': str(e),\n",
        "                    'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                })\n",
        "\n",
        "                # Clean up failed driver instance\n",
        "                if driver is not None:\n",
        "                    try:\n",
        "                        driver.quit()\n",
        "                    except:\n",
        "                        pass\n",
        "                    driver = None\n",
        "\n",
        "                # Retry with exponential backoff if attempts remaining\n",
        "                if attempt < retry_attempts - 1:\n",
        "                    backoff_time = 2 ** attempt\n",
        "                    time.sleep(backoff_time)\n",
        "                    continue\n",
        "                else:\n",
        "                    # All attempts failed\n",
        "                    raise LaunchError(\n",
        "                        f\"ChromeDriver launch failed after {retry_attempts} attempts: {e}\",\n",
        "                        driver_path=driver_path_obj,\n",
        "                        browser_version=None,\n",
        "                        driver_version=None,\n",
        "                        forensic_metadata=ForensicMetadata(\n",
        "                            operation_name=\"chromedriver_launch_failure\",\n",
        "                            algorithm_parameters={\n",
        "                                'total_attempts': retry_attempts,\n",
        "                                'final_error': str(e)\n",
        "                            }\n",
        "                        )\n",
        "                    ) from e\n",
        "\n",
        "        # Main automation workflow with comprehensive error handling\n",
        "        try:\n",
        "            # Navigate to Google Images with retry logic and performance monitoring\n",
        "            navigation_start = time.perf_counter()\n",
        "            navigation_success = False\n",
        "\n",
        "            for attempt in range(retry_attempts):\n",
        "                try:\n",
        "                    # Navigate to Google Images homepage\n",
        "                    driver.get(\"https://images.google.com\")\n",
        "\n",
        "                    # Verify successful navigation by checking page elements\n",
        "                    WebDriverWait(driver, timeout).until(\n",
        "                        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "                    )\n",
        "\n",
        "                    # Validate page loaded correctly\n",
        "                    if \"Google\" in driver.title and \"images\" in driver.current_url.lower():\n",
        "                        navigation_success = True\n",
        "\n",
        "                        # Record successful navigation\n",
        "                        navigation_time = time.perf_counter() - navigation_start\n",
        "                        operation_metrics['processing_stages']['navigation'] = {\n",
        "                            'duration_seconds': navigation_time,\n",
        "                            'attempt_number': attempt + 1,\n",
        "                            'final_url': driver.current_url,\n",
        "                            'page_title': driver.title,\n",
        "                            'navigation_successful': True\n",
        "                        }\n",
        "                        break\n",
        "                    else:\n",
        "                        raise NavigationError(f\"Unexpected page content: {driver.title}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Record navigation failure\n",
        "                    operation_metrics['automation_events'].append({\n",
        "                        'event_type': 'navigation_failed',\n",
        "                        'attempt': attempt + 1,\n",
        "                        'error': str(e),\n",
        "                        'current_url': driver.current_url if driver else 'unknown',\n",
        "                        'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                    })\n",
        "\n",
        "                    if attempt < retry_attempts - 1:\n",
        "                        time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                        continue\n",
        "\n",
        "            if not navigation_success:\n",
        "                raise NavigationError(\n",
        "                    \"Failed to navigate to Google Images after all attempts\",\n",
        "                    target_url=\"https://images.google.com\",\n",
        "                    forensic_metadata=ForensicMetadata(\n",
        "                        operation_name=\"google_images_navigation\",\n",
        "                        algorithm_parameters={'total_attempts': retry_attempts}\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # Implement robust selector strategy with comprehensive fallback hierarchy\n",
        "            camera_button_selectors = [\n",
        "                # Primary selectors with high specificity\n",
        "                \"button[aria-label*='Search by image']\",\n",
        "                \"div[aria-label*='Search by image']\",\n",
        "                \"button[data-ved*='camera']\",\n",
        "                \"button[jsname='LgbsSe']\",\n",
        "\n",
        "                # CSS class fallbacks (may change frequently)\n",
        "                \"div.nDcEnd\",\n",
        "                \".rUDD3b\",\n",
        "                \".Gdd5U\",\n",
        "\n",
        "                # XPath fallbacks for robust element location\n",
        "                \"//button[@aria-label and contains(@aria-label, 'Search by image')]\",\n",
        "                \"//div[@aria-label and contains(@aria-label, 'Search by image')]\",\n",
        "                \"//button[contains(@class, 'camera') or contains(@data-ved, 'camera')]\",\n",
        "                \"//div[@role='button' and contains(text(), 'camera')]\"\n",
        "            ]\n",
        "\n",
        "            # Attempt to locate and click camera button using selector hierarchy\n",
        "            camera_button_clicked = False\n",
        "            camera_interaction_start = time.perf_counter()\n",
        "\n",
        "            for selector_idx, selector in enumerate(camera_button_selectors):\n",
        "                try:\n",
        "                    # Determine selector type and create appropriate locator\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        locator = (By.XPATH, selector)\n",
        "                        selector_type = \"xpath\"\n",
        "                    else:\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "                        selector_type = \"css\"\n",
        "\n",
        "                    # Wait for element to be present and clickable\n",
        "                    camera_button = WebDriverWait(driver, timeout).until(\n",
        "                        EC.element_to_be_clickable(locator)\n",
        "                    )\n",
        "\n",
        "                    # Scroll element into view for reliable interaction\n",
        "                    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", camera_button)\n",
        "                    time.sleep(0.5)  # Brief pause for scroll completion\n",
        "\n",
        "                    # Attempt click with multiple strategies\n",
        "                    click_successful = False\n",
        "                    click_strategies = [\n",
        "                        lambda: camera_button.click(),  # Standard click\n",
        "                        lambda: driver.execute_script(\"arguments[0].click();\", camera_button),  # JavaScript click\n",
        "                        lambda: webdriver.ActionChains(driver).move_to_element(camera_button).click().perform()  # Action chains\n",
        "                    ]\n",
        "\n",
        "                    for strategy_idx, click_strategy in enumerate(click_strategies):\n",
        "                        try:\n",
        "                            click_strategy()\n",
        "                            click_successful = True\n",
        "                            break\n",
        "                        except Exception as click_error:\n",
        "                            if strategy_idx < len(click_strategies) - 1:\n",
        "                                time.sleep(0.2)  # Brief pause before next strategy\n",
        "                                continue\n",
        "                            else:\n",
        "                                raise click_error\n",
        "\n",
        "                    if click_successful:\n",
        "                        camera_button_clicked = True\n",
        "\n",
        "                        # Record successful camera button interaction\n",
        "                        camera_interaction_time = time.perf_counter() - camera_interaction_start\n",
        "                        operation_metrics['processing_stages']['camera_button_interaction'] = {\n",
        "                            'duration_seconds': camera_interaction_time,\n",
        "                            'selector_used': selector,\n",
        "                            'selector_type': selector_type,\n",
        "                            'selector_index': selector_idx,\n",
        "                            'click_successful': True\n",
        "                        }\n",
        "                        break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    # Selector not found - try next in hierarchy\n",
        "                    operation_metrics['automation_events'].append({\n",
        "                        'event_type': 'selector_timeout',\n",
        "                        'selector': selector,\n",
        "                        'selector_index': selector_idx,\n",
        "                        'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Other interaction errors - log and continue\n",
        "                    operation_metrics['automation_events'].append({\n",
        "                        'event_type': 'selector_interaction_failed',\n",
        "                        'selector': selector,\n",
        "                        'selector_index': selector_idx,\n",
        "                        'error': str(e),\n",
        "                        'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "            if not camera_button_clicked:\n",
        "                raise NavigationError(\n",
        "                    \"Failed to locate and click camera button with all selectors\",\n",
        "                    target_url=driver.current_url,\n",
        "                    automation_state={'selectors_tried': len(camera_button_selectors)},\n",
        "                    forensic_metadata=ForensicMetadata(\n",
        "                        operation_name=\"camera_button_interaction\",\n",
        "                        algorithm_parameters={'selectors_attempted': camera_button_selectors}\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # Locate and interact with upload tab using multiple strategies\n",
        "            upload_tab_selectors = [\n",
        "                # Direct text-based selectors\n",
        "                \"//a[contains(text(), 'Upload an image')]\",\n",
        "                \"//div[contains(text(), 'Upload an image')]\",\n",
        "                \"//span[contains(text(), 'Upload')]\",\n",
        "\n",
        "                # Attribute-based selectors\n",
        "                \"a[href*='upload']\",\n",
        "                \"div[data-ved*='upload']\",\n",
        "                \"button[aria-label*='upload']\",\n",
        "\n",
        "                # CSS class fallbacks\n",
        "                \".RZQOVd\",\n",
        "                \".aXBtI\",\n",
        "                \".Gdd5U\"\n",
        "            ]\n",
        "\n",
        "            upload_tab_clicked = False\n",
        "            upload_tab_interaction_start = time.perf_counter()\n",
        "\n",
        "            for selector_idx, selector in enumerate(upload_tab_selectors):\n",
        "                try:\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        locator = (By.XPATH, selector)\n",
        "                        selector_type = \"xpath\"\n",
        "                    else:\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "                        selector_type = \"css\"\n",
        "\n",
        "                    # Wait for upload tab element\n",
        "                    upload_tab = WebDriverWait(driver, timeout).until(\n",
        "                        EC.element_to_be_clickable(locator)\n",
        "                    )\n",
        "\n",
        "                    # Scroll into view and click\n",
        "                    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", upload_tab)\n",
        "                    time.sleep(0.3)\n",
        "\n",
        "                    # Attempt click with fallback strategies\n",
        "                    try:\n",
        "                        upload_tab.click()\n",
        "                    except:\n",
        "                        driver.execute_script(\"arguments[0].click();\", upload_tab)\n",
        "\n",
        "                    upload_tab_clicked = True\n",
        "\n",
        "                    # Record successful upload tab interaction\n",
        "                    upload_tab_interaction_time = time.perf_counter() - upload_tab_interaction_start\n",
        "                    operation_metrics['processing_stages']['upload_tab_interaction'] = {\n",
        "                        'duration_seconds': upload_tab_interaction_time,\n",
        "                        'selector_used': selector,\n",
        "                        'selector_type': selector_type,\n",
        "                        'selector_index': selector_idx,\n",
        "                        'interaction_successful': True\n",
        "                    }\n",
        "                    break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    operation_metrics['automation_events'].append({\n",
        "                        'event_type': 'upload_tab_interaction_failed',\n",
        "                        'selector': selector,\n",
        "                        'error': str(e),\n",
        "                        'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "            if not upload_tab_clicked:\n",
        "                raise NavigationError(\n",
        "                    \"Failed to locate and click upload tab with all selectors\",\n",
        "                    target_url=driver.current_url,\n",
        "                    automation_state={'upload_selectors_tried': len(upload_tab_selectors)}\n",
        "                )\n",
        "\n",
        "            # Locate file input element and perform upload with comprehensive validation\n",
        "            file_input_selectors = [\n",
        "                \"input[name='encoded_image']\",\n",
        "                \"input[type='file']\",\n",
        "                \"input[accept*='image']\",\n",
        "                \"input[class*='file']\",\n",
        "                \".cB9M7\",\n",
        "                \"//input[@type='file']\",\n",
        "                \"//input[@accept and contains(@accept, 'image')]\"\n",
        "            ]\n",
        "\n",
        "            file_uploaded = False\n",
        "            file_upload_start = time.perf_counter()\n",
        "\n",
        "            for selector_idx, selector in enumerate(file_input_selectors):\n",
        "                try:\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        locator = (By.XPATH, selector)\n",
        "                    else:\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "\n",
        "                    # Wait for file input element\n",
        "                    file_input = WebDriverWait(driver, timeout).until(\n",
        "                        EC.presence_of_element_located(locator)\n",
        "                    )\n",
        "\n",
        "                    # Validate file input element properties\n",
        "                    if not file_input.is_enabled():\n",
        "                        continue\n",
        "\n",
        "                    # Upload file by sending keys with absolute path\n",
        "                    absolute_path = str(validated_image_path.resolve())\n",
        "                    file_input.send_keys(absolute_path)\n",
        "\n",
        "                    # Verify upload initiated by checking for page changes\n",
        "                    time.sleep(1.0)  # Allow upload to initiate\n",
        "\n",
        "                    file_uploaded = True\n",
        "\n",
        "                    # Record successful file upload\n",
        "                    file_upload_time = time.perf_counter() - file_upload_start\n",
        "                    operation_metrics['processing_stages']['file_upload'] = {\n",
        "                        'duration_seconds': file_upload_time,\n",
        "                        'selector_used': selector,\n",
        "                        'file_path': absolute_path,\n",
        "                        'file_size_bytes': validated_image_path.stat().st_size,\n",
        "                        'upload_successful': True\n",
        "                    }\n",
        "                    break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    operation_metrics['automation_events'].append({\n",
        "                        'event_type': 'file_upload_failed',\n",
        "                        'selector': selector,\n",
        "                        'error': str(e),\n",
        "                        'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "            if not file_uploaded:\n",
        "                raise UploadError(\n",
        "                    \"Failed to upload image file with all selectors\",\n",
        "                    file_path=validated_image_path,\n",
        "                    file_size=int(validated_image_path.stat().st_size),\n",
        "                    upload_parameters={'selectors_tried': len(file_input_selectors)},\n",
        "                    forensic_metadata=ForensicMetadata(\n",
        "                        operation_name=\"image_file_upload\",\n",
        "                        algorithm_parameters={'file_path': str(validated_image_path)}\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # Wait for results page to load and extract comprehensive data\n",
        "            results_extraction_start = time.perf_counter()\n",
        "\n",
        "            # Wait for page to load search results\n",
        "            try:\n",
        "                WebDriverWait(driver, timeout * 2).until(\n",
        "                    lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
        "                )\n",
        "                time.sleep(2.0)  # Additional wait for dynamic content\n",
        "            except TimeoutException:\n",
        "                pass  # Continue with extraction even if page not fully loaded\n",
        "\n",
        "            # Extract best guess text with comprehensive selector strategy\n",
        "            best_guess_text = \"\"\n",
        "            best_guess_selectors = [\n",
        "                \"div[role='heading']\",\n",
        "                \".fKDtNb\",\n",
        "                \"//div[contains(@class, 'r5a77d')]\",\n",
        "                \"h3\",\n",
        "                \".gLFyf\",\n",
        "                \"//div[@data-ved and contains(text(), ' ')]\",\n",
        "                \".yXK7lf\"\n",
        "            ]\n",
        "\n",
        "            for selector in best_guess_selectors:\n",
        "                try:\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        locator = (By.XPATH, selector)\n",
        "                    else:\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "\n",
        "                    best_guess_element = WebDriverWait(driver, timeout // 2).until(\n",
        "                        EC.presence_of_element_located(locator)\n",
        "                    )\n",
        "\n",
        "                    extracted_text = best_guess_element.text.strip()\n",
        "                    if extracted_text and len(extracted_text) > 3:  # Meaningful text threshold\n",
        "                        best_guess_text = extracted_text\n",
        "                        break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    operation_metrics['automation_events'].append({\n",
        "                        'event_type': 'best_guess_extraction_failed',\n",
        "                        'selector': selector,\n",
        "                        'error': str(e),\n",
        "                        'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "            # Extract similar image URLs with advanced filtering and validation\n",
        "            similar_urls = []\n",
        "            similar_image_selectors = [\n",
        "                \"a[jsname='sTFXNd']\",\n",
        "                \"a[href*='/imgres?']\",\n",
        "                \".rg_l\",\n",
        "                \"//a[contains(@href, 'imgres')]\",\n",
        "                \".isv-r a\",\n",
        "                \".rg_di a\"\n",
        "            ]\n",
        "\n",
        "            for selector in similar_image_selectors:\n",
        "                try:\n",
        "                    if selector.startswith(\"//\"):\n",
        "                        locator = (By.XPATH, selector)\n",
        "                    else:\n",
        "                        locator = (By.CSS_SELECTOR, selector)\n",
        "\n",
        "                    similar_elements = WebDriverWait(driver, timeout // 2).until(\n",
        "                        EC.presence_of_all_elements_located(locator)\n",
        "                    )\n",
        "\n",
        "                    # Extract and validate URLs\n",
        "                    for element in similar_elements[:max_similar_urls * 2]:  # Get extra for filtering\n",
        "                        try:\n",
        "                            href = element.get_attribute(\"href\")\n",
        "                            if href and self._validate_search_result_url(href):\n",
        "                                if href not in similar_urls:  # Avoid duplicates\n",
        "                                    similar_urls.append(href)\n",
        "                                    if len(similar_urls) >= max_similar_urls:\n",
        "                                        break\n",
        "                        except Exception:\n",
        "                            continue\n",
        "\n",
        "                    if similar_urls:  # Stop if URLs found\n",
        "                        break\n",
        "\n",
        "                except TimeoutException:\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    operation_metrics['automation_events'].append({\n",
        "                        'event_type': 'similar_images_extraction_failed',\n",
        "                        'selector': selector,\n",
        "                        'error': str(e),\n",
        "                        'timestamp': datetime.datetime.utcnow().isoformat()\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "            # Extract additional metadata if advanced extraction enabled\n",
        "            additional_metadata = {}\n",
        "            if advanced_extraction:\n",
        "                try:\n",
        "                    # Extract page metadata\n",
        "                    additional_metadata = {\n",
        "                        'page_url': driver.current_url,\n",
        "                        'page_title': driver.title,\n",
        "                        'search_timestamp': datetime.datetime.utcnow().isoformat(),\n",
        "                        'total_page_elements': len(driver.find_elements(By.TAG_NAME, \"*\")),\n",
        "                        'page_load_time': results_extraction_start - method_start_time\n",
        "                    }\n",
        "\n",
        "                    # Extract domain distribution from URLs\n",
        "                    if similar_urls:\n",
        "                        from urllib.parse import urlparse\n",
        "                        domains = []\n",
        "                        for url in similar_urls:\n",
        "                            try:\n",
        "                                domain = urlparse(url).netloc.lower()\n",
        "                                domains.append(domain)\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                        unique_domains = list(set(domains))\n",
        "                        additional_metadata.update({\n",
        "                            'unique_domain_count': len(unique_domains),\n",
        "                            'domain_distribution': {domain: domains.count(domain) for domain in unique_domains},\n",
        "                            'duplicate_url_count': len(similar_urls) - len(set(similar_urls))\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    additional_metadata = {'extraction_error': str(e)}\n",
        "\n",
        "            # Record results extraction performance\n",
        "            results_extraction_time = time.perf_counter() - results_extraction_start\n",
        "            operation_metrics['processing_stages']['results_extraction'] = {\n",
        "                'duration_seconds': results_extraction_time,\n",
        "                'best_guess_extracted': bool(best_guess_text),\n",
        "                'similar_urls_count': len(similar_urls),\n",
        "                'advanced_metadata_extracted': bool(additional_metadata),\n",
        "                'extraction_successful': True\n",
        "            }\n",
        "\n",
        "            # Perform content analysis if requested\n",
        "            content_quality_metrics = {}\n",
        "            if content_analysis and (best_guess_text or similar_urls):\n",
        "                content_analysis_start = time.perf_counter()\n",
        "\n",
        "                try:\n",
        "                    # Analyze best guess text quality\n",
        "                    if best_guess_text:\n",
        "                        content_quality_metrics['best_guess_analysis'] = {\n",
        "                            'text_length': len(best_guess_text),\n",
        "                            'word_count': len(best_guess_text.split()),\n",
        "                            'contains_numbers': any(char.isdigit() for char in best_guess_text),\n",
        "                            'contains_special_chars': any(not char.isalnum() and not char.isspace() for char in best_guess_text),\n",
        "                            'language_detected': self._detect_text_language(best_guess_text),\n",
        "                            'confidence_score': self._compute_text_confidence(best_guess_text)\n",
        "                        }\n",
        "\n",
        "                    # Analyze URL quality and distribution\n",
        "                    if similar_urls:\n",
        "                        url_analysis = {\n",
        "                            'total_urls': len(similar_urls),\n",
        "                            'unique_urls': len(set(similar_urls)),\n",
        "                            'duplicate_ratio': (len(similar_urls) - len(set(similar_urls))) / len(similar_urls),\n",
        "                            'average_url_length': statistics.mean([len(url) for url in similar_urls]),\n",
        "                            'https_ratio': sum(1 for url in similar_urls if url.startswith('https://')) / len(similar_urls),\n",
        "                            'domain_diversity': len(set(urlparse(url).netloc for url in similar_urls if self._is_valid_url(url)))\n",
        "                        }\n",
        "                        content_quality_metrics['url_analysis'] = url_analysis\n",
        "\n",
        "                    # Compute overall content quality score\n",
        "                    quality_factors = []\n",
        "\n",
        "                    if best_guess_text:\n",
        "                        text_quality = min(len(best_guess_text) / 50.0, 1.0)  # Normalize to [0,1]\n",
        "                        quality_factors.append(text_quality * 0.4)\n",
        "\n",
        "                    if similar_urls:\n",
        "                        url_quality = min(len(similar_urls) / max_similar_urls, 1.0)\n",
        "                        quality_factors.append(url_quality * 0.6)\n",
        "\n",
        "                    overall_quality = sum(quality_factors) if quality_factors else 0.0\n",
        "                    content_quality_metrics['overall_quality_score'] = overall_quality\n",
        "\n",
        "                    # Record content analysis performance\n",
        "                    content_analysis_time = time.perf_counter() - content_analysis_start\n",
        "                    operation_metrics['processing_stages']['content_analysis'] = {\n",
        "                        'duration_seconds': content_analysis_time,\n",
        "                        'quality_score': overall_quality,\n",
        "                        'analysis_successful': True\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    content_quality_metrics = {'analysis_error': str(e)}\n",
        "                    operation_metrics['processing_stages']['content_analysis'] = {\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "            # Compute confidence score based on result characteristics\n",
        "            confidence_score = 0.0\n",
        "            if best_guess_text or similar_urls:\n",
        "                confidence_factors = []\n",
        "\n",
        "                # Text-based confidence\n",
        "                if best_guess_text:\n",
        "                    text_conf = min(len(best_guess_text.split()) / 10.0, 1.0)\n",
        "                    confidence_factors.append(text_conf * 0.3)\n",
        "\n",
        "                # URL-based confidence\n",
        "                if similar_urls:\n",
        "                    url_conf = min(len(similar_urls) / max_similar_urls, 1.0)\n",
        "                    confidence_factors.append(url_conf * 0.4)\n",
        "\n",
        "                # Diversity-based confidence\n",
        "                if 'unique_domain_count' in additional_metadata:\n",
        "                    diversity_conf = min(additional_metadata['unique_domain_count'] / 5.0, 1.0)\n",
        "                    confidence_factors.append(diversity_conf * 0.3)\n",
        "\n",
        "                confidence_score = sum(confidence_factors)\n",
        "\n",
        "            # Record comprehensive performance metrics\n",
        "            total_execution_time = time.perf_counter() - method_start_time\n",
        "            memory_after = psutil.Process().memory_info().rss\n",
        "            memory_delta = memory_after - memory_before\n",
        "\n",
        "            operation_metrics.update({\n",
        "                'total_execution_time_seconds': total_execution_time,\n",
        "                'memory_delta_bytes': memory_delta,\n",
        "                'confidence_score': confidence_score,\n",
        "                'result_type': 'successful_search'\n",
        "            })\n",
        "\n",
        "            # Store performance metrics for analysis if monitoring enabled\n",
        "            if performance_monitoring:\n",
        "                self._record_method_performance('reverse_image_search_google', operation_metrics)\n",
        "\n",
        "            # Create comprehensive result structure with all extracted data\n",
        "            search_result = ReverseImageSearchResult(\n",
        "                best_guess=best_guess_text or \"No description found\",\n",
        "                similar_image_urls=similar_urls,\n",
        "                source_page_title=driver.title,\n",
        "                site_authority_score=None,  # Could be computed from domain analysis\n",
        "                snippet_text=None,  # Could be extracted from page content\n",
        "                confidence_score=confidence_score,\n",
        "                search_timestamp=datetime.datetime.utcnow(),\n",
        "                search_duration_seconds=total_execution_time,\n",
        "                duplicate_url_count=additional_metadata.get('duplicate_url_count', 0),\n",
        "                unique_domain_count=additional_metadata.get('unique_domain_count', 0),\n",
        "                geographic_indicators=additional_metadata.get('domain_distribution')\n",
        "            )\n",
        "\n",
        "            # Validate result if requested\n",
        "            if result_validation:\n",
        "                try:\n",
        "                    is_valid, violations = search_result.validate_constraints(strict=False)\n",
        "                    if not is_valid:\n",
        "                        operation_metrics['result_validation'] = {\n",
        "                            'validation_passed': False,\n",
        "                            'violations': violations\n",
        "                        }\n",
        "                    else:\n",
        "                        operation_metrics['result_validation'] = {\n",
        "                            'validation_passed': True\n",
        "                        }\n",
        "                except Exception as e:\n",
        "                    operation_metrics['result_validation'] = {\n",
        "                        'validation_error': str(e)\n",
        "                    }\n",
        "\n",
        "            return search_result\n",
        "\n",
        "        except Exception as e:\n",
        "            # Comprehensive error handling with context preservation\n",
        "            if isinstance(e, (NavigationError, UploadError, ExtractionError)):\n",
        "                raise  # Re-raise domain-specific exceptions\n",
        "            else:\n",
        "                # Wrap unexpected errors in appropriate exception type\n",
        "                raise ExtractionError(\n",
        "                    f\"Reverse image search failed: {e}\",\n",
        "                    extraction_target=\"google_search_results\",\n",
        "                    dom_state={'current_url': driver.current_url if driver else 'unknown'},\n",
        "                    extraction_parameters=operation_metrics['parameters'],\n",
        "                    forensic_metadata=ForensicMetadata(\n",
        "                        operation_name=\"reverse_image_search_failure\",\n",
        "                        algorithm_parameters={'error_type': type(e).__name__}\n",
        "                    )\n",
        "                ) from e\n",
        "\n",
        "        finally:\n",
        "            # Ensure WebDriver cleanup regardless of success or failure\n",
        "            if driver is not None:\n",
        "                try:\n",
        "                    # Capture final state for debugging if needed\n",
        "                    final_url = driver.current_url\n",
        "                    final_title = driver.title\n",
        "\n",
        "                    # Close browser and clean up resources\n",
        "                    driver.quit()\n",
        "\n",
        "                    # Record cleanup completion\n",
        "                    operation_metrics['cleanup'] = {\n",
        "                        'driver_closed': True,\n",
        "                        'final_url': final_url,\n",
        "                        'final_title': final_title\n",
        "                    }\n",
        "\n",
        "                except Exception as cleanup_error:\n",
        "                    # Log cleanup failures but don't raise\n",
        "                    operation_metrics['cleanup'] = {\n",
        "                        'cleanup_error': str(cleanup_error)\n",
        "                    }\n",
        "\n",
        "    def _validate_search_result_url(self, url: str) -> bool:\n",
        "        \"\"\"\n",
        "        Validate search result URL for quality and relevance.\n",
        "\n",
        "        Args:\n",
        "            url: URL to validate\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating URL validity\n",
        "        \"\"\"\n",
        "        if not url or not isinstance(url, str):\n",
        "            return False\n",
        "\n",
        "        # Basic URL structure validation\n",
        "        if not url.startswith(('http://', 'https://')):\n",
        "            return False\n",
        "\n",
        "        # Filter out Google's internal URLs\n",
        "        google_internal_patterns = [\n",
        "            'google.com/search',\n",
        "            'google.com/url',\n",
        "            'googleusercontent.com',\n",
        "            'accounts.google.com'\n",
        "        ]\n",
        "\n",
        "        if any(pattern in url.lower() for pattern in google_internal_patterns):\n",
        "            return False\n",
        "\n",
        "        # Check for reasonable URL length\n",
        "        if len(url) > 2000:  # Extremely long URLs are suspicious\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _detect_text_language(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Detect language of extracted text using simple heuristics.\n",
        "\n",
        "        Args:\n",
        "            text: Text to analyze\n",
        "\n",
        "        Returns:\n",
        "            Detected language code or 'unknown'\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return 'unknown'\n",
        "\n",
        "        # Simple heuristic based on character patterns\n",
        "        # In production, would use proper language detection library\n",
        "        ascii_ratio = sum(1 for char in text if ord(char) < 128) / len(text)\n",
        "\n",
        "        if ascii_ratio > 0.9:\n",
        "            return 'en'  # Likely English\n",
        "        else:\n",
        "            return 'other'  # Non-English or mixed\n",
        "\n",
        "    def _compute_text_confidence(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Compute confidence score for extracted text quality.\n",
        "\n",
        "        Args:\n",
        "            text: Text to analyze\n",
        "\n",
        "        Returns:\n",
        "            Confidence score [0,1]\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return 0.0\n",
        "\n",
        "        # Factors contributing to text confidence\n",
        "        factors = []\n",
        "\n",
        "        # Length factor\n",
        "        length_factor = min(len(text) / 100.0, 1.0)\n",
        "        factors.append(length_factor * 0.3)\n",
        "\n",
        "        # Word count factor\n",
        "        word_count = len(text.split())\n",
        "        word_factor = min(word_count / 20.0, 1.0)\n",
        "        factors.append(word_factor * 0.3)\n",
        "\n",
        "        # Character diversity factor\n",
        "        unique_chars = len(set(text.lower()))\n",
        "        diversity_factor = min(unique_chars / 20.0, 1.0)\n",
        "        factors.append(diversity_factor * 0.2)\n",
        "\n",
        "        # Completeness factor (no truncation indicators)\n",
        "        truncation_indicators = ['...', '…', '[...]', 'more']\n",
        "        completeness_factor = 0.0 if any(indicator in text.lower() for indicator in truncation_indicators) else 1.0\n",
        "        factors.append(completeness_factor * 0.2)\n",
        "\n",
        "        return sum(factors)\n"
      ],
      "metadata": {
        "id": "ZJvWYvQkY4-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage Example\n",
        "\n",
        "def demonstrate_image_provenance_analysis(\n",
        "    detector: ImageSimilarityDetector,\n",
        "    image1_path: Union[str, Path],\n",
        "    image2_path: Union[str, Path],\n",
        "    chromedriver_path: Union[str, Path]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes a comprehensive, multi-modal analysis to compare two images and\n",
        "    establish the provenance of the first image.\n",
        "\n",
        "    This function serves as a production-grade demonstration of the\n",
        "    ImageSimilarityDetector's capabilities, invoking each of its primary\n",
        "    analytical methods in a structured sequence. It captures results from\n",
        "    perceptual hashing, local feature matching, global color analysis,\n",
        "    semantic similarity, and public reverse image search.\n",
        "\n",
        "    The methodology proceeds from low-level structural comparisons to\n",
        "    high-level semantic and contextual analysis, providing a holistic\n",
        "    view of the relationship between the images.\n",
        "\n",
        "    Args:\n",
        "        detector (ImageSimilarityDetector): An initialized instance of the\n",
        "            image similarity detector.\n",
        "        image1_path (Union[str, Path]): The file path to the primary image\n",
        "            to be analyzed and compared. This image will also be used for\n",
        "            the reverse image search.\n",
        "        image2_path (Union[str, Path]): The file path to the secondary image\n",
        "            for comparison.\n",
        "        chromedriver_path (Union[str, Path]): The file path to the\n",
        "            Selenium ChromeDriver executable, required for reverse image search.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the detailed results from each\n",
        "            analysis stage. Each key corresponds to an analysis method, and\n",
        "            the value is either a comprehensive result dictionary/object or\n",
        "            an error message if that stage failed.\n",
        "\n",
        "    Raises:\n",
        "        This function is designed to be robust and captures exceptions from\n",
        "        underlying methods within its results dictionary rather than raising\n",
        "        them, allowing the overall analysis to complete where possible.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to aggregate the results from all analysis methods.\n",
        "    analysis_results: Dict[str, Any] = {}\n",
        "    # Configure logging to provide visibility into the analysis process.\n",
        "    logging.info(f\"Starting comprehensive provenance analysis for '{Path(image1_path).name}' and '{Path(image2_path).name}'.\")\n",
        "\n",
        "    # --- Stage 1: Perceptual Hash Analysis (Structural Duplication) ---\n",
        "    # This stage checks for near-identical copies using a DCT-based perceptual hash.\n",
        "    # It is highly effective for detecting direct replication with minor modifications.\n",
        "    logging.info(\"Executing Stage 1: Perceptual Hash Analysis...\")\n",
        "    try:\n",
        "        # Execute perceptual hash analysis with full statistical reporting for rigor.\n",
        "        # A 16x16 hash (256 bits) is used for higher precision in detecting subtle differences.\n",
        "        p_hash_results = detector.perceptual_hash_difference(\n",
        "            image1_path,\n",
        "            image2_path,\n",
        "            hash_size=16,\n",
        "            normalize=True,\n",
        "            return_similarity=True,\n",
        "            statistical_analysis=True\n",
        "        )\n",
        "        # Store the comprehensive results dictionary.\n",
        "        analysis_results['perceptual_hash'] = p_hash_results\n",
        "        # Log the primary similarity score for quick assessment.\n",
        "        logging.info(f\"  - pHash Similarity Score: {p_hash_results.get('similarity_score', 'N/A'):.4f}\")\n",
        "    except (ImageUnreadableError, ValueError, RuntimeError) as e:\n",
        "        # Gracefully handle and record errors related to image processing or hash computation.\n",
        "        analysis_results['perceptual_hash'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        # Log the failure of this analysis stage.\n",
        "        logging.error(f\"  - Perceptual Hash Analysis failed: {e}\")\n",
        "\n",
        "    # --- Stage 2: Local Feature Matching Analysis (Geometric Consistency) ---\n",
        "    # This stage detects structural copying of components using ORB features and RANSAC.\n",
        "    # It is critical for identifying collage-like compositions or transformed object insertions.\n",
        "    logging.info(\"Executing Stage 2: Local Feature Matching Analysis...\")\n",
        "    try:\n",
        "        # Execute feature matching with geometric verification to ensure spatial coherence.\n",
        "        # Lowe's ratio test is applied for more robust and less ambiguous matches.\n",
        "        feature_match_results = detector.feature_match_ratio(\n",
        "            image1_path,\n",
        "            image2_path,\n",
        "            distance_threshold=64,\n",
        "            normalization_strategy=\"min_keypoints\",\n",
        "            apply_ratio_test=True,\n",
        "            ratio_threshold=0.75,\n",
        "            resize_max_side=1024,\n",
        "            return_detailed_result=True,\n",
        "            geometric_verification=True,\n",
        "            statistical_analysis=True\n",
        "        )\n",
        "        # Store the comprehensive FeatureMatchResult object for detailed inspection.\n",
        "        analysis_results['feature_matching'] = feature_match_results\n",
        "        # Log key metrics: the overall similarity and the geometric inlier ratio.\n",
        "        logging.info(f\"  - Feature Match Similarity Ratio: {feature_match_results.similarity_ratio:.4f}\")\n",
        "        logging.info(f\"  - Geometric Inlier Ratio: {feature_match_results.homography_inlier_ratio or 'N/A'}\")\n",
        "    except (ImageUnreadableError, RuntimeError) as e:\n",
        "        # Gracefully handle and record errors in the feature detection or matching pipeline.\n",
        "        analysis_results['feature_matching'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        # Log the failure of this analysis stage.\n",
        "        logging.error(f\"  - Feature Matching Analysis failed: {e}\")\n",
        "\n",
        "    # --- Stage 3: Global Color Distribution Analysis (Palette Similarity) ---\n",
        "    # This stage compares the overall color palettes, providing a measure of aesthetic similarity.\n",
        "    # It is the weakest signal for direct copying but useful for stylistic analysis.\n",
        "    logging.info(\"Executing Stage 3: Global Color Distribution Analysis...\")\n",
        "    try:\n",
        "        # Execute color histogram correlation in the HSV space to be robust to lighting changes.\n",
        "        # Pearson correlation provides a normalized measure of how similarly the color distributions vary.\n",
        "        histogram_results = detector.histogram_correlation(\n",
        "            image1_path,\n",
        "            image2_path,\n",
        "            metric=\"correlation\",\n",
        "            color_space=\"HSV\",\n",
        "            statistical_analysis=True,\n",
        "            adaptive_binning=True\n",
        "        )\n",
        "        # Store the comprehensive results dictionary.\n",
        "        analysis_results['histogram_correlation'] = histogram_results\n",
        "        # Log the primary correlation score for quick assessment.\n",
        "        logging.info(f\"  - Histogram Correlation: {histogram_results.get('similarity_score', 'N/A'):.4f}\")\n",
        "    except (ImageUnreadableError, HistogramError) as e:\n",
        "        # Gracefully handle and record errors in histogram computation or comparison.\n",
        "        analysis_results['histogram_correlation'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        # Log the failure of this analysis stage.\n",
        "        logging.error(f\"  - Histogram Correlation Analysis failed: {e}\")\n",
        "\n",
        "    # --- Stage 4: Semantic Meaning Analysis (Conceptual Similarity) ---\n",
        "    # This stage uses a deep learning model (CLIP) to measure conceptual similarity.\n",
        "    # It can identify that a photo of a cat and a painting of a cat are related.\n",
        "    logging.info(\"Executing Stage 4: Semantic Meaning Analysis...\")\n",
        "    try:\n",
        "        # Execute semantic similarity analysis with full statistical and embedding analysis.\n",
        "        # This provides the most abstract and powerful form of comparison.\n",
        "        clip_results = detector.clip_embedding_similarity(\n",
        "            image1_path,\n",
        "            image2_path,\n",
        "            statistical_analysis=True,\n",
        "            embedding_analysis=True,\n",
        "            batch_processing=True\n",
        "        )\n",
        "        # Store the comprehensive results dictionary.\n",
        "        analysis_results['semantic_similarity'] = clip_results\n",
        "        # Log the primary cosine similarity score.\n",
        "        logging.info(f\"  - CLIP Cosine Similarity: {clip_results.get('cosine_similarity', 'N/A'):.4f}\")\n",
        "    except (ModelLoadError, ModelInferenceError, ValueError) as e:\n",
        "        # Gracefully handle and record errors related to model loading or inference.\n",
        "        analysis_results['semantic_similarity'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        # Log the failure of this analysis stage.\n",
        "        logging.error(f\"  - Semantic Similarity Analysis failed: {e}\")\n",
        "\n",
        "    # --- Stage 5: Public Provenance and Context Analysis (Web Discovery) ---\n",
        "    # This stage uses web automation to discover if the primary image exists online.\n",
        "    # It is a discovery process, not a direct comparison, to establish public context.\n",
        "    logging.info(\"Executing Stage 5: Public Provenance Analysis...\")\n",
        "    try:\n",
        "        # Execute a reverse image search on the first image.\n",
        "        # This is run in headless mode for suitability in automated server environments.\n",
        "        reverse_search_results = detector.reverse_image_search_google(\n",
        "            image_path=image1_path,\n",
        "            driver_path=chromedriver_path,\n",
        "            headless=True,\n",
        "            advanced_extraction=True,\n",
        "            content_analysis=True\n",
        "        )\n",
        "        # Store the comprehensive ReverseImageSearchResult object.\n",
        "        analysis_results['reverse_image_search'] = reverse_search_results\n",
        "        # Log the primary finding from the search.\n",
        "        logging.info(f\"  - Reverse Search Best Guess: {reverse_search_results.best_guess}\")\n",
        "        logging.info(f\"  - Found {len(reverse_search_results.similar_image_urls)} similar images online.\")\n",
        "    except (LaunchError, NavigationError, UploadError, ExtractionError) as e:\n",
        "        # Gracefully handle and record the various failure modes of web automation.\n",
        "        analysis_results['reverse_image_search'] = {'error': str(e), 'details': traceback.format_exc()}\n",
        "        # Log the failure of this analysis stage.\n",
        "        logging.error(f\"  - Reverse Image Search failed: {e}\")\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected errors during the search to prevent crashing.\n",
        "        analysis_results['reverse_image_search'] = {'error': f\"An unexpected error occurred: {e}\", 'details': traceback.format_exc()}\n",
        "        # Log the unexpected failure.\n",
        "        logging.error(f\"  - Reverse Image Search encountered an unexpected error: {e}\")\n",
        "\n",
        "    # Finalize the analysis process.\n",
        "    logging.info(\"Comprehensive provenance analysis complete.\")\n",
        "    # Return the aggregated results dictionary.\n",
        "    return analysis_results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Setup: Environment and Test Data ---\n",
        "    # Configure a basic logger to direct output to the console for this demonstration.\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # Create a temporary directory for test assets.\n",
        "    test_dir = Path(\"./test_images\")\n",
        "    test_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Programmatically create two test images for a controlled experiment.\n",
        "    # Image 1: A base image with a distinct feature.\n",
        "    # Image 2: A transformed version of Image 1 (rotated, color-shifted, compressed).\n",
        "    try:\n",
        "        # Define image parameters.\n",
        "        img_size = (512, 512)\n",
        "        # Create a base image using NumPy: a dark gray background.\n",
        "        base_image_array = np.full((img_size[1], img_size[0], 3), 64, dtype=np.uint8)\n",
        "        # Add a distinct feature: a bright blue circle in the center.\n",
        "        cv2.circle(base_image_array, (img_size[0]//2, img_size[1]//2), 100, (255, 100, 50), -1)\n",
        "        # Save the first image as a high-quality PNG.\n",
        "        image1_path = test_dir / \"original_image.png\"\n",
        "        cv2.imwrite(str(image1_path), base_image_array)\n",
        "\n",
        "        # Create the second, modified image.\n",
        "        # Start with the base image.\n",
        "        modified_image_array = base_image_array.copy()\n",
        "        # Apply a slight color shift to the entire image.\n",
        "        modified_image_array = cv2.add(modified_image_array, np.array([10, 5, -15], dtype=np.uint8))\n",
        "        # Get the rotation matrix for a 5-degree rotation around the center.\n",
        "        rotation_matrix = cv2.getRotationMatrix2D((img_size[0]//2, img_size[1]//2), 5, 0.95) # Rotate and scale down\n",
        "        # Apply the affine transformation (rotation).\n",
        "        modified_image_array = cv2.warpAffine(modified_image_array, rotation_matrix, img_size)\n",
        "        # Save the second image as a moderately compressed JPEG to introduce artifacts.\n",
        "        image2_path = test_dir / \"modified_image.jpg\"\n",
        "        cv2.imwrite(str(image2_path), modified_image_array, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
        "\n",
        "        logging.info(f\"Test images created: '{image1_path}' and '{image2_path}'\")\n",
        "\n",
        "        # --- Execution: Instantiate Detector and Run Analysis ---\n",
        "        # IMPORTANT: The user must provide a valid path to their local ChromeDriver executable.\n",
        "        # Download from: https://googlechromelabs.github.io/chrome-for-testing/\n",
        "        chromedriver_path = Path(\"./chromedriver\") # Assumes chromedriver is in the current directory.\n",
        "        if not chromedriver_path.exists():\n",
        "            logging.error(\"=\"*80)\n",
        "            logging.error(\"FATAL: ChromeDriver not found at the specified path.\")\n",
        "            logging.error(f\"Please download the correct version for your Chrome browser and place it at: '{chromedriver_path.resolve()}'\")\n",
        "            logging.error(\"Download from: https://googlechromelabs.github.io/chrome-for-testing/\")\n",
        "            logging.error(\"=\"*80)\n",
        "            # Exit if the driver is not available, as the reverse search will fail.\n",
        "            sys.exit(1)\n",
        "\n",
        "        # Instantiate the main detector class with enterprise-grade settings.\n",
        "        # This configuration enables performance monitoring and production-level validation.\n",
        "        detector = ImageSimilarityDetector(\n",
        "            resource_constraints=ResourceConstraints.BALANCED,\n",
        "            enable_performance_monitoring=True,\n",
        "            validation_policy=ValidationPolicy.PRODUCTION\n",
        "        )\n",
        "\n",
        "        # Execute the comprehensive analysis function.\n",
        "        full_results = demonstrate_image_provenance_analysis(\n",
        "            detector,\n",
        "            image1_path,\n",
        "            image2_path,\n",
        "            chromedriver_path\n",
        "        )\n",
        "\n",
        "        # --- Output: Display Results ---\n",
        "        # Define a custom serializer to handle complex objects like dataclasses and NumPy arrays.\n",
        "        def result_serializer(obj):\n",
        "            if isinstance(obj, (Path, np.ndarray)):\n",
        "                return str(obj)\n",
        "            if hasattr(obj, 'to_dict'):\n",
        "                return obj.to_dict()\n",
        "            if isinstance(obj, Exception):\n",
        "                return f\"{type(obj).__name__}: {str(obj)}\"\n",
        "            return obj.__dict__ if hasattr(obj, '__dict__') else str(obj)\n",
        "\n",
        "        # Print the aggregated results in a clean, human-readable JSON format.\n",
        "        print(\"\\n\" + \"=\"*40 + \" ANALYSIS RESULTS \" + \"=\"*40)\n",
        "        print(json.dumps(full_results, default=result_serializer, indent=2))\n",
        "        print(\"=\"*100)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any top-level exceptions during the demonstration setup or execution.\n",
        "        logging.fatal(f\"The demonstration script encountered a fatal error: {e}\")\n",
        "        logging.fatal(traceback.format_exc())\n",
        "\n"
      ],
      "metadata": {
        "id": "N3vx7j7JRyFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}